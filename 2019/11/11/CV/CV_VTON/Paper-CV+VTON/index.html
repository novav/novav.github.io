<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"novav.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="[TOC] VTON code 1711.08447 [VITON] An Image-based Virtual Try-on Network.pdf VITON：基于映像的虚拟试穿网络CVPR 2018论文“ VITON：基于图像的虚拟试穿网络”的代码和数据集 人像提取本文中使用的人物表示由2D姿态估计器和人工解析器提取：  实时多人姿势估计 自我监督的结构敏感学习  数据集由于版权问题，该数">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper-CV+VTON">
<meta property="og:url" content="https://novav.github.io/2019/11/11/CV/CV_VTON/Paper-CV+VTON/index.html">
<meta property="og:site_name" content="Simon Shi的小站">
<meta property="og:description" content="[TOC] VTON code 1711.08447 [VITON] An Image-based Virtual Try-on Network.pdf VITON：基于映像的虚拟试穿网络CVPR 2018论文“ VITON：基于图像的虚拟试穿网络”的代码和数据集 人像提取本文中使用的人物表示由2D姿态估计器和人工解析器提取：  实时多人姿势估计 自我监督的结构敏感学习  数据集由于版权问题，该数">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://novav.github.io/2019/11/11/CV/CV_VTON/Paper-CV+VTON/1573543133654.png">
<meta property="og:image" content="https://novav.github.io/2019/11/11/CV/CV_VTON/Paper-CV+VTON/20190525153625478.png">
<meta property="og:image" content="https://novav.github.io/2019/11/11/CV/CV_VTON/Paper-CV+VTON/20190525163224192.png">
<meta property="article:published_time" content="2019-11-11T15:40:25.000Z">
<meta property="article:modified_time" content="2025-08-06T08:16:38.333Z">
<meta property="article:author" content="Simon Shi">
<meta property="article:tag" content="VTON">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://novav.github.io/2019/11/11/CV/CV_VTON/Paper-CV+VTON/1573543133654.png">

<link rel="canonical" href="https://novav.github.io/2019/11/11/CV/CV_VTON/Paper-CV+VTON/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Paper-CV+VTON | Simon Shi的小站</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Simon Shi的小站</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">人工智能，机器学习， 强化学习，大模型，自动驾驶</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/11/11/CV/CV_VTON/Paper-CV+VTON/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper-CV+VTON
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-11 15:40:25" itemprop="dateCreated datePublished" datetime="2019-11-11T15:40:25+00:00">2019-11-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/" itemprop="url" rel="index"><span itemprop="name">CV_Apply</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/VTON/" itemprop="url" rel="index"><span itemprop="name">VTON</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>[TOC]</p>
<p><a target="_blank" rel="noopener" href="https://github.com/xthan/VITON">VTON code</a></p>
<p>1711.08447 [VITON] An Image-based Virtual Try-on Network.pdf</p>
<h2 id="VITON：基于映像的虚拟试穿网络"><a href="#VITON：基于映像的虚拟试穿网络" class="headerlink" title="VITON：基于映像的虚拟试穿网络"></a>VITON：基于映像的虚拟试穿网络</h2><p>CVPR 2018论文“ VITON：基于图像的虚拟试穿网络”的代码和数据集</p>
<h3 id="人像提取"><a href="#人像提取" class="headerlink" title="人像提取"></a>人像提取</h3><p>本文中使用的人物表示由2D姿态估计器和人工解析器提取：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation">实时多人姿势估计</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Engineering-Course/LIP_SSL">自我监督的结构敏感学习</a></li>
</ul>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>由于版权问题，该数据集不再公开可用。对于已经下载数据集的用户，请注意，使用或分发数据集是非法的！</p>
<p>This dataset is crawled from women’s tops on <a target="_blank" rel="noopener" href="https://www.zalando.co.uk/womens-clothing-tops/">Zalando</a>. </p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h4><p>在<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1qFU4KmvnEr4CwEFXQZS_6Ebw5dPJAE21?usp=sharing">Google云端硬盘</a>上下载经过预训练的模型。将它们放在<code>model/</code>文件夹下。</p>
<p>运行<code>test_stage1.sh</code>以进行推断。结果在中<code>results/stage1/images/</code>。<code>results/stage1/index.html</code>可视化结果。</p>
<h4 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h4><p>运行matlab脚本<code>shape_context_warp.m</code>以提取TPS转换控制点。</p>
<p>然后<code>test_stage2.sh</code>进行优化并生成最终结果，该结果位于中<code>results/stage2/images/</code>。<code>results/stage2/index.html</code>可视化结果。</p>
<h3 id="培养"><a href="#培养" class="headerlink" title="培养"></a>培养</h3><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><p>往里走<code>prepare_data</code>。</p>
<p>首先运行<code>extract_tps.m</code>。这将需要一些时间，您可以尝试并行运行它，也可以直接通过Google云端硬盘下载预先计算的TPS控制点，然后将其放入<code>data/tps/</code>。</p>
<p>然后运行<code>./preprocess_viton.sh</code>，生成的TF记录将位于中<code>prepare_data/tfrecord</code>。</p>
<h4 id="第一阶段-1"><a href="#第一阶段-1" class="headerlink" title="第一阶段"></a>第一阶段</h4><p>跑 <code>train_stage1.sh</code></p>
<h4 id="第二阶段-1"><a href="#第二阶段-1" class="headerlink" title="第二阶段"></a>第二阶段</h4><p>跑 <code>train_stage2.sh</code></p>
<h3 id="引文"><a href="#引文" class="headerlink" title="引文"></a>引文</h3><p>如果此代码或数据集有助于您的研究，请引用我们的论文：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@inproceedings&#123;han2017viton,</span><br><span class="line">  title = &#123;VITON: An Image-based Virtual Try-on Network&#125;,</span><br><span class="line">  author = &#123;Han, Xintong and Wu, Zuxuan and Wu, Zhe and Yu, Ruichi and Davis, Larry S&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  year  = &#123;2018&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://github.com/xthan/VITON">https://github.com/xthan/VITON</a></p>
<h2 id="Paper-Viton-札记"><a href="#Paper-Viton-札记" class="headerlink" title="Paper Viton[札记]"></a>Paper Viton[札记]</h2><p>VITON用一个由粗到细的框架解决了这个问题，并期望通过TPS变形来捕捉布料的变形。</p>
<p>An overview of VITON.  </p>
<p><img src="/2019/11/11/CV/CV_VTON/Paper-CV+VTON/1573543133654.png" alt="1573543133654"></p>
<h2 id="VITON-Paper"><a href="#VITON-Paper" class="headerlink" title="VITON Paper:"></a>VITON Paper:</h2><p>VITON 的目标在于，对给定的参考图像（模特）I和目标衣服c，生成合成图像I霸；I霸中c被自然地“穿”到参考图像I中模特对应的区域上，而模特的姿势和其他身体部位特征被保留。最直接的方法是用这样的训练集：同一个模特（姿势和人体特征相同）穿着不同衣服的多张图片和对应的这些衣服的产品图。但是这种数据集是很难大规模得到的。<br>在实际虚拟试衣场景中，在测试时只有参考图像和目标产品的图像是可以得到的。因此，我们把这种设定同样运用到训练集中来，所以输入的参考图像I中模特穿的衣服就是目标衣服c，这样的数据集是易得的（就像某宝上卖衣服，不仅给产品图还要给卖家秀)。那么现在的重点就是，给定c和模特人体信息，怎么训练一个网络-&gt;不仅可以生成合成图像，更重要的是要在测试中能够泛化，用任意所需的服装项目合成感知上令人信服的图像。</p>
<h3 id="3-Person-Representation-人体特征表示"><a href="#3-Person-Representation-人体特征表示" class="headerlink" title="3. Person Representation 人体特征表示"></a>3. Person Representation 人体特征表示</h3><h5 id="3-1-Pose-heatmap-姿势热图"><a href="#3-1-Pose-heatmap-姿势热图" class="headerlink" title="3.1 Pose heatmap 姿势热图"></a>3.1 Pose heatmap 姿势热图</h5><p>人体姿势的多样性导致了衣服不同程度的变形，因此我们使用最先进的<strong>姿势估计器</strong>明确地<strong>建模姿势信息</strong>。（用的是CVPR2017的人体姿势估计Realtime Multi-Person Pose Estimation）人体姿势估计包含了18个关键点，为了使Represent表示的各个部分可以空间堆叠，每个关键点被转换成热图heatmap，在关键点附近有一个11×11的邻居填充了1, 在其他地方填充0，然后把这些热图堆叠成一个18通道的姿势热图。</p>
<ul>
<li>Realtime Multi-Person Pose Estimation –》姿态信息， 18个关键点</li>
</ul>
<h5 id="3-2-Human-body-representation-人体身体部位表示"><a href="#3-2-Human-body-representation-人体身体部位表示" class="headerlink" title="3.2 Human body representation 人体身体部位表示"></a>3.2 Human body representation 人体身体部位表示</h5><p>衣服的外形很大程度上取决于人的形状（高矮胖瘦），因此如何将目标衣服进行变形取决于不同的身体部位（如手臂，躯干）和身体形态。一个先进的人体解析的方法（LIP-SSL，个人感觉LIP-SSP比较难跑，caffe环境配置复杂，建议跑它的另一个版本JPPNet，不过要是想得到.mat要自己改一下输出）输出人体分割图（.mat格式，不同分割区域标记了不同编号）。然后我们把这个分割图转换成一个单通道的二进制掩模，其中1代表身体部分，0表示其他部位。这个二进制掩模呗下采样成一个低分辨率的图（16*12），当身体形状和目标衣服混淆时，避免伪影。</p>
<ul>
<li>LIP-SSL 人体解析算法，生成人体分割图.mat文件</li>
<li>JPPNET(option)</li>
</ul>
<h5 id="3-3-Face-and-hair-segment-脸部、头发的分割"><a href="#3-3-Face-and-hair-segment-脸部、头发的分割" class="headerlink" title="3.3 Face and hair segment 脸部、头发的分割"></a>3.3 Face and hair segment 脸部、头发的分割</h5><p>为了维持人本身的特点，我们在人体表示中融入了身体特征，如脸、肤色、头发之类。我们用Human body representation里一样的人体解析器分离出人脸和头发部分的RGB图，作为新合成图像的身份信息。</p>
<ul>
<li>LIP-SLL 分离人脸和头发部分的RGB图</li>
</ul>
<p>Pose + Body + Face &#x3D;&#x3D; (18 + 1 +  3)</p>
<p>最后，把以上得到的三个部分转换到统一分辨率，然后堆叠形成与衣物无关的人体表示P（256，192，22），示意图如下。P包含了丰富的人体信息，convNet就是基于这些信息学习它们之间的关系，比以往的person Presentation都精细。<br><img src="/2019/11/11/CV/CV_VTON/Paper-CV+VTON/20190525153625478.png" alt="在这里插入图片描述"></p>
<h3 id="Multi-task-Encoder-Decoder-Generator-Gc"><a href="#Multi-task-Encoder-Decoder-Generator-Gc" class="headerlink" title="Multi-task Encoder-Decoder Generator -&gt; Gc"></a>Multi-task Encoder-Decoder Generator -&gt; Gc</h3><p>输入：Person Representation P和目标衣服Target Clothing C<br>        输出：粗合成I’和对应区域掩模M<br>        Gc：这里用到的编码解码框架是一种包含连接跳跃skip connections的U-net网络结构。<br>设Gc为编码解码器估计的方程，它将P和C组合作为输入并输出四通道（I‘和M）结果。我们希望得到一个生成器，使得粗合成图I’尽可能接近参考图I，掩模M尽可能接近从参考图I模特上预测得到的伪ground truth掩模M0。一个简单的方法就是用L1损失，使当输出目标是M0这样的二进制掩模时，它会产生不错的结果。但是如果输出要是RGB图像的话，L1 Loss就会使输出图像变模糊。我们还引入了a perceptual loss感知损失。下式是感知损失的和以及LI Loss加和的结果。<br><img src="/2019/11/11/CV/CV_VTON/Paper-CV+VTON/20190525163224192.png" alt="在这里插入图片描述"><br>网络用ImageNet数据集训练的VGG19权重初始化。超参数lamda_i控制第i层的贡献。第一个Stage的Gc没有学习细节（L1 Loss导致），所以得到的是粗合成图。</p>
<h3 id="Refinement-Network-GR"><a href="#Refinement-Network-GR" class="headerlink" title="Refinement Network -&gt; GR"></a>Refinement Network -&gt; GR</h3><p>是为了从目标衣服产品图中学习对应模糊图像区域的细节并恢复。</p>
<h4 id="Warped-clothing-item-衣服变形"><a href="#Warped-clothing-item-衣服变形" class="headerlink" title="Warped clothing item 衣服变形"></a>Warped clothing item 衣服变形</h4><p>为了运用目标衣服产品图中的纹理细节，要先对衣服进行变形。</p>
<p>a thin plate spline (TPS) transformation</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41971682/article/details/90549389"> copy from </a></p>
<h2 id="VITON-realtime"><a href="#VITON-realtime" class="headerlink" title="VITON_realtime"></a>VITON_realtime</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>The dataset can be downloaded on <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1-RIcmjQKTqsf3PZsoHT4hivNngx_3386?usp=sharing">Google Drive</a>.</p>
<p><strong>This dataset is crawled from women’s tops on</strong> <a target="_blank" rel="noopener" href="https://www.zalando.co.uk/womens-clothing-tops/">Zalando</a>. These images can be downloaded on Google Drive. The results of pose estimation and human parsing are also included. Note that number of the images&#x2F;poses&#x2F;segmentation maps are more than that reported in the paper, since the ones with bad pose estimations (too few keypoints are detected) or parsing results (parsed upper clothes regions only occupy a small portion of the image).</p>
<p>Put all folder and labels in the <code>data</code> folder:</p>
<p><code>data/women_top</code>: reference images (image name is ID_0.jpg) and clothing images (image name is ID_1.jpg). For example, the clothing image on reference image 000001_0.jpg is 000001_1.jpg. The resolution of these images is 1100x762.</p>
<p><code>data/pose.pkl</code>: a pickle file containing a dictionary of the pose keypoints of each reference image. Please refer to <a target="_blank" rel="noopener" href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation/blob/master/testing/python/demo.ipynb">this demo</a> for how to parse the stored results, and <a target="_blank" rel="noopener" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md">OpenPose output</a> to understand the output format. (字典文件保存：（pose keypoints， image）pose keypoints of each reference image)</p>
<p>包含每个参考图像的位姿关键点字典的pickle文件。请参考<a target="_blank" rel="noopener" href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation/blob/master/testing/python/demo.ipynb">这个演示</a>如何解析存储的结果，和<a target="_blank" rel="noopener" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md">OpenPose输出</a>来理解输出格式。</p>
<p><code>data/pose/</code>: folder containing the pose keypoints of each reference image.</p>
<p><code>data/segment</code>: folder containing the segmentation map of each reference image. In a segmentation map, label 5 corresponds to the regions of tops (used as pseudo ground truth of clothing region). label 1 (hat), 2 (hair), 4 (sunglasses), and 13 (face) are merged as the face&#x2F;hair representation. All other non-background regions are merged for extracting the body representation. The colormap for visualization can be downloaded <a target="_blank" rel="noopener" href="https://github.com/Engineering-Course/LIP_SSL/blob/master/human_colormap.mat">here</a>. Due to padding operations of the parser, these segmentation maps are 641x641, you need to crop them based on the aspect ratio of the original reference images.</p>
<p><code>data/tps/</code>: TPS control points between product image and its corresponding reference image.</p>
<p><code>data/viton_train_images.txt</code>: training image list.</p>
<p><code>data/viton_train_pairs.txt</code>: 14,221 reference image and clothing training image pairs.</p>
<p><code>data/viton_test_pairs.txt</code>: 2,032 reference image and target clothing testing image pairs. Note that these pairs are only used for the evaluation in our paper, one can choose any reference image and target clothing to generate the virtual try-on results.</p>
<p>问题1：（人体解析）segment的数据来源</p>
<p>​    LIP-SSL 人体解析，分离人脸，发等特征</p>
<p>问题2：（姿态评估）pose.pkl 文件格式，如何生成</p>
<p>​    see ##3.1. Person Representation</p>
<p>​    reference images的pose keypoints</p>
<p>​    Realtime_Multi-Person_Pose_Estimation 模型得到的人体姿态评估 18通道</p>
<p>问题3：（人体分割）pose结果的格式</p>
<p>​    LIP_SSL得到的人体Shape .mat 文件</p>
<p>问题4：pose keypoints文件的来源</p>
<p>​    见问题3</p>
<p>问题5：tps的数据格式</p>
<h3 id="替代技术选型："><a href="#替代技术选型：" class="headerlink" title="替代技术选型："></a>替代技术选型：</h3><p>Human Parse:      <a target="_blank" rel="noopener" href="https://github.com/llltttppp/SS-NAN">SS-NAN</a></p>
<p>Pose Estimator:   <a target="_blank" rel="noopener" href="https://github.com/ildoonet/tf-pose-estimation">TF-pose-estimation</a></p>
<h3 id="RUN-（How-to-run-）"><a href="#RUN-（How-to-run-）" class="headerlink" title="RUN （How to run?）"></a>RUN （How to run?）</h3><ol>
<li>Download related models</li>
</ol>
<ul>
<li><p>Download pretrained SS-NAN model <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1nvMMl0P">here</a>. Put AttResnet101FCN_lip_0023.h5 under SS-NAN&#x2F; folder.</p>
</li>
<li><p>Model of tf-pose-estimation is already in the repo since it could use mobile-net.</p>
</li>
<li><p>Download pretrained VITON models on <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1qFU4KmvnEr4CwEFXQZS_6Ebw5dPJAE21">Google Drive</a>. Put them under model&#x2F; folder.</p>
</li>
</ul>
<ol start="2">
<li><p>For remote server with GPU support, run the below for API server to deal with pose and segmentation inferrence:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yml</span><br><span class="line">source activate MakeNTU</span><br><span class="line">bash run_API_server.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>For local server, run the below to do VITON inferrence and avoid tensorflow session problem for concurrency:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yml</span><br><span class="line">source activate MakeNTU</span><br><span class="line">export FLASK_APP=VITON_local_server.py</span><br><span class="line">flask run</span><br></pre></td></tr></table></figure>
</li>
<li><p>Change settings in VITON_Demo_post:<br>Set VIDEO_SOURCE to your webcam number or video path.</p>
</li>
<li><p>Finally, run the main app:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export SEG_SERVER=&lt;IP address ofthe remote server, like http://192.168.0.123:8000&gt;</span><br><span class="line">export POSE_SERVER=&lt;IP address ofthe remote server, like http://192.168.0.123:8000&gt;</span><br><span class="line">export VITON_SERVER=&#x27;http://localhost:5000&#x27;</span><br><span class="line">source activate MakeNTU</span><br><span class="line">python VITON_Demo_post.py</span><br></pre></td></tr></table></figure></li>
</ol>
<p>Keyboard controls</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">q: to exit</span><br><span class="line">c: to capture image and do virtual try-on</span><br><span class="line">a/s/d/f: change clothes to try on</span><br></pre></td></tr></table></figure>

<p>Other files are for running all things locally or without concurrency.</p>
<p>One could also run <code>python post_viton.py</code> to run without local VITON server.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/VTON/" rel="tag"># VTON</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/11/06/CV/CV_VTON/Paper-CV+MG-VTON/" rel="prev" title="Paper_CV+ MG-VTON">
      <i class="fa fa-chevron-left"></i> Paper_CV+ MG-VTON
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/11/11/CV/CV_VTON/Paper-CV+CP-VTON/" rel="next" title="Paper-CV+CP-VTON">
      Paper-CV+CP-VTON <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#VITON%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%98%A0%E5%83%8F%E7%9A%84%E8%99%9A%E6%8B%9F%E8%AF%95%E7%A9%BF%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">VITON：基于映像的虚拟试穿网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%BA%E5%83%8F%E6%8F%90%E5%8F%96"><span class="nav-number">1.1.</span> <span class="nav-text">人像提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.2.</span> <span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95"><span class="nav-number">1.3.</span> <span class="nav-text">测试</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5"><span class="nav-number">1.3.1.</span> <span class="nav-text">第一阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5"><span class="nav-number">1.3.2.</span> <span class="nav-text">第二阶段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%B9%E5%85%BB"><span class="nav-number">1.4.</span> <span class="nav-text">培养</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="nav-number">1.4.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5-1"><span class="nav-number">1.4.2.</span> <span class="nav-text">第一阶段</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5-1"><span class="nav-number">1.4.3.</span> <span class="nav-text">第二阶段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E6%96%87"><span class="nav-number">1.5.</span> <span class="nav-text">引文</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Paper-Viton-%E6%9C%AD%E8%AE%B0"><span class="nav-number">2.</span> <span class="nav-text">Paper Viton[札记]</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VITON-Paper"><span class="nav-number">3.</span> <span class="nav-text">VITON Paper:</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Person-Representation-%E4%BA%BA%E4%BD%93%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA"><span class="nav-number">3.1.</span> <span class="nav-text">3. Person Representation 人体特征表示</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-Pose-heatmap-%E5%A7%BF%E5%8A%BF%E7%83%AD%E5%9B%BE"><span class="nav-number">3.1.0.1.</span> <span class="nav-text">3.1 Pose heatmap 姿势热图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-Human-body-representation-%E4%BA%BA%E4%BD%93%E8%BA%AB%E4%BD%93%E9%83%A8%E4%BD%8D%E8%A1%A8%E7%A4%BA"><span class="nav-number">3.1.0.2.</span> <span class="nav-text">3.2 Human body representation 人体身体部位表示</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-3-Face-and-hair-segment-%E8%84%B8%E9%83%A8%E3%80%81%E5%A4%B4%E5%8F%91%E7%9A%84%E5%88%86%E5%89%B2"><span class="nav-number">3.1.0.3.</span> <span class="nav-text">3.3 Face and hair segment 脸部、头发的分割</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-task-Encoder-Decoder-Generator-Gc"><span class="nav-number">3.2.</span> <span class="nav-text">Multi-task Encoder-Decoder Generator -&gt; Gc</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Refinement-Network-GR"><span class="nav-number">3.3.</span> <span class="nav-text">Refinement Network -&gt; GR</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Warped-clothing-item-%E8%A1%A3%E6%9C%8D%E5%8F%98%E5%BD%A2"><span class="nav-number">3.3.1.</span> <span class="nav-text">Warped clothing item 衣服变形</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VITON-realtime"><span class="nav-number">4.</span> <span class="nav-text">VITON_realtime</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dataset"><span class="nav-number">4.1.</span> <span class="nav-text">Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%BF%E4%BB%A3%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B%EF%BC%9A"><span class="nav-number">4.2.</span> <span class="nav-text">替代技术选型：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RUN-%EF%BC%88How-to-run-%EF%BC%89"><span class="nav-number">4.3.</span> <span class="nav-text">RUN （How to run?）</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Simon Shi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">322</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">142</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">269</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Simon Shi</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>

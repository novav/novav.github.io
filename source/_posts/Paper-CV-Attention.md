---
title: Paper_CV_Attention
date: 2019-08-29 15:12:29
tags:
---

注意力机制可以分为四类：

```

基于输入项的柔性注意力（Item-wise Soft Attention）、

基于输入项的硬性注意力（Item-wise Hard Attention）、

基于位置的柔性注意力（Location-wise Soft Attention）、

基于位置的硬性注意力（Location-wise Hard Attention）。
```



总的来说，一种是**软注意力(soft attention)**，另一种则是**强注意力(hard attention)**。

软注意力的关键点在于，这种注意力更关注区域或者通道，而且**软注意力是确定性的注意力**，学习完成后直接可以通过网络生成，最关键的地方是**软注意力是可微的**，这是一个非常重要的地方。可以微分的注意力就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到注意力的权重。

强注意力与软注意力不同点在于，首先强注意力是更加关注点，也就是图像中的每个点都有可能延伸出注意力，同时强注意力是一个随机的预测过程，更强调动态变化。当然，最关键是强注意力是一个不可微的注意力，训练过程往往是通过**增强学习(reinforcement learning)**来完成的。



从注意力域（attention domain）的角度来分析几种注意力的实现方法。其中主要是三种注意力域，**空间域(spatial domain)**，**通道域(channel domain)**，**混合域(mixed domain)。**

### （1） 空间域

设计思路**：**

Spatial Transformer Networks（STN）模型是15年NIPS上的文章，这篇文章通过注意力机制，将原始图片中的空间信息变换到另一个空间中并保留了关键信息。这篇文章的思想非常巧妙，因为卷积神经网络中的池化层（pooling layer）直接用一些max pooling 或者average pooling 的方法，将图片信息压缩，减少运算量提升准确率。但是这篇文章认为之前pooling的方法太过于暴力，直接将信息合并会导致关键信息无法识别出来，所以提出了一个叫空间转换器（spatial transformer）的模块，将图片中的的空间域信息做对应的**空间变换**，从而能将关键的信息提取出来。

### （2） 通道域

设计思路：

通道域的注意力机制原理很简单，我们可以从基本的信号变换的角度去理解。信号系统分析里面，任何一个信号其实都可以写成正弦波的线性组合，经过时频变换之后，时域上连续的正弦波信号就可以用一个频率信号数值代替了。

在卷积神经网络中，每一张图片初始会由（R，G，B）三通道表示出来，之后经过不同的卷积核之后，每一个通道又会生成新的信号，比如图片特征的每个通道使用64核卷积，就会产生64个新通道的矩阵（H,W,64），H,W分别表示图片特征的高度和宽度。每个通道的特征其实就表示该图片在不同卷积核上的分量，类似于时频变换，而这里面用卷积核的卷积类似于信号做了傅里叶变换，从而能够将这个特征一个通道的信息给分解成64个卷积核上的信号分量。既然每个信号都可以被分解成核函数上的分量，产生的新的64个通道对于关键信息的贡献肯定有多有少，如果我们给每个通道上的信号都增加一个权重，来代表该**通道与关键信息的相关度**的话，这个权重越大，则表示相关度越高，也就是我们越需要去注意的通道了。

可参考 SENet模型结构。

### （3） 混合域

了解前两种注意力域的设计思路后，简单对比一下。首先，空间域的注意力是忽略了通道域中的信息，将每个通道中的图片特征同等处理，这种做法会将空间域变换方法局限在原始图片特征提取阶段，应用在神经网络层其他层的可解释性不强。

而通道域的注意力是对一个通道内的信息直接全局平均池化，而忽略每一个通道内的局部信息，这种做法其实也是比较暴力的行为。所以结合两种思路，就可以设计出混合域的注意力机制模型


<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"novav.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Simon Shi的小站">
<meta property="og:url" content="https://novav.github.io/page/27/index.html">
<meta property="og:site_name" content="Simon Shi的小站">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Simon Shi">
<meta property="article:tag" content="AI,Machine Learning, Deep Learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://novav.github.io/page/27/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Simon Shi的小站</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Simon Shi的小站</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">人工智能，机器学习， 强化学习，大模型，自动驾驶</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2020/01/17/CV_3D/CV-3D-Model-Apply/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/17/CV_3D/CV-3D-Model-Apply/" class="post-title-link" itemprop="url">CV-3D-Model-Apply</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-17 10:02:48" itemprop="dateCreated datePublished" datetime="2020-01-17T10:02:48+00:00">2020-01-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-3D/" itemprop="url" rel="index"><span itemprop="name">CV_3D</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-3D/3D-Body/" itemprop="url" rel="index"><span itemprop="name">3D Body</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="3D-Body-Model-reStructure"><a href="#3D-Body-Model-reStructure" class="headerlink" title="3D Body Model reStructure"></a>3D Body Model reStructure</h1><h3 id="SMPLify"><a href="#SMPLify" class="headerlink" title="SMPLify"></a>SMPLify</h3><p>SMPLR</p>
<p>SMPL-X</p>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/1579242997000.png" alt="1579242997000"></p>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/1579243000955.png" alt="1579243000955"></p>
<h1 id="MIP-virtualhumans"><a href="#MIP-virtualhumans" class="headerlink" title="MIP-virtualhumans"></a>MIP-virtualhumans</h1><p><a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/publications.html">http://virtualhumans.mpi-inf.mpg.de/publications.html</a></p>
<h3 id="CVPR-2020-3D-Shape-Restruction"><a href="#CVPR-2020-3D-Shape-Restruction" class="headerlink" title="[CVPR, 2020] 3D Shape Restruction:"></a>[CVPR, 2020] 3D Shape Restruction:</h3><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/chibane20ifnet-1583942168127.png" style="zoom:25%;">

<p><a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/Chibane.html">Julian Chibane</a>, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/alldieck.html">Thiemo Alldieck</a>, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a><br><strong>Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion</strong><br>in <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020.</p>
<p> BibTeX<a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/papers/chibane20ifnet/chibane20ifnet.pdf"> PDF</a></p>
<h3 id="CVPR-2020-3D-Human-Texture-mir-pix-suf"><a href="#CVPR-2020-3D-Human-Texture-mir-pix-suf" class="headerlink" title="[CVPR,2020] 3D Human Texture (mir-pix-suf)"></a>[CVPR,2020] 3D Human Texture (mir-pix-suf)</h3><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/mir20pix2surf-1583942520186.jpg" style="zoom:25%;">

<p><a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/Mir.html">Aymen Mir</a>, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/alldieck.html">Thiemo Alldieck</a>, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a><br><strong>Learning to Transfer Texture from Clothing Images to 3D Humans</strong><br>in <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020.</p>
<p> BibTeX<a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/papers/mir20pix2surf/mir20pix2surf.pdf"> PDF</a></p>
<h3 id="CVPR-2020-VTailor"><a href="#CVPR-2020-VTailor" class="headerlink" title="[CVPR,2020] VTailor"></a>[CVPR,2020] VTailor</h3><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/patel20vtailor.png" style="zoom:25%;">

<p>Chaitanya Patel, Zhouyingcheng Liao, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a><br><strong>The Virtual Tailor: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style</strong><br>in <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020.</p>
<p> BibTeX<a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/papers/patel20vtailor/vtailor.pdf"> PDF</a></p>
<h3 id="CVPR-2020-Drees3DPeople"><a href="#CVPR-2020-Drees3DPeople" class="headerlink" title="[CVPR,2020] Drees3DPeople"></a>[CVPR,2020] Drees3DPeople</h3><ul>
<li>CAPE model</li>
</ul>
<img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/ma20autoenclother.png" style="zoom:25%;">

<p>Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a>, Siyu Tang, Michael Black<br><strong>Learning to Dress 3D People in Generative Clothing</strong> </p>
<p><strong>学习给3D人物穿上生成性服装</strong></p>
<p>in <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020.</p>
<p> BibTeX<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.13615"> arXiv</a></p>
<ul>
<li>模型穿衣（无纹理效果）</li>
<li></li>
</ul>
<h3 id="CVPR-2020-DeepCap"><a href="#CVPR-2020-DeepCap" class="headerlink" title="[CVPR,2020] DeepCap"></a>[CVPR,2020] DeepCap</h3><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/habermann20deepcap.png" style="zoom:75%;">

<p>Marc Habermann, Weipeng Xu, Michael and Zollhoefer, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a>, Christian Theobalt<br><strong>DeepCap: Monocular Human Performance Capture Using Weak Supervision</strong><br>in <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020.</p>
<p> BibTeX</p>
<h3 id="ICCV-2019-MGN"><a href="#ICCV-2019-MGN" class="headerlink" title="[ICCV,2019] MGN"></a>[ICCV,2019] MGN</h3><ul>
<li>1-8 Image</li>
</ul>
<p>标题：Multi-Garment Net: Learning to Dress 3D People from Images</p>
<p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.06903">https://arxiv.org/abs/1908.06903</a></p>
<p>代码：<a target="_blank" rel="noopener" href="https://github.com/bharat-b7/MultiGarmentNetwork">https://github.com/bharat-b7/MultiGarmentNetwork</a></p>
<ul>
<li>⭐⭐⭐⭐</li>
</ul>
<p>作者：<strong>B. L. Bhatnagar</strong></p>
<p>马克斯普朗克信息学院，萨尔兰信息学院，德国<br>专门研究REAL VIRTUAL HUMANS <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/">http://virtualhumans.mpi-inf.mpg.de/</a></p>
<p>学习从图像中为3D人物穿衣</p>
<p>基于SMPL，提出MGN网络，用于从视频帧中预测体型和服装的方法。</p>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/1579228893790.png" alt="1579228893790"></p>
<p>Garment Registration:</p>
<p>​		Laplacian Mesh Processing 三维网格拉普拉斯处理</p>
<p>We register the scans using multi-mesh registration</p>
<h3 id="ICCV-2019-Tex2Shape"><a href="#ICCV-2019-Tex2Shape" class="headerlink" title="[ICCV,2019] Tex2Shape"></a>[ICCV,2019] Tex2Shape</h3><p>作者：<a target="_blank" rel="noopener" href="https://graphics.tu-bs.de/people/alldieck"><strong>Thiemo Alldieck</strong></a>, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/">Gerard Pons-Moll</a>, <a target="_blank" rel="noopener" href="http://gvv.mpi-inf.mpg.de/">Christian Theobalt</a>, <a target="_blank" rel="noopener" href="https://graphics.tu-bs.de/people/magnor">Marcus Magnor</a>:</p>
<p>论文：<a target="_blank" rel="noopener" href="https://graphics.tu-bs.de/publications/alldieck2019tex2shape">Tex2Shape: Detailed Full Human Body Geometry from a Single Image</a></p>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/thmoa/tex2shape">https://github.com/thmoa/tex2shape</a></p>
<p>Arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.08645">https://arxiv.org/abs/1904.08645</a></p>
<p>单张图片—人体3D建模(无纹理重建)</p>
<ul>
<li>形状回归—-转化—-&gt;图像到图像的对齐翻译问题</li>
</ul>
<p><strong>输入：现成方法得到的可见部分的纹理</strong></p>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/image-20200211230827286.png" alt="image-20200211230827286"></p>
<h3 id="ICCV-2019-AMASS"><a href="#ICCV-2019-AMASS" class="headerlink" title="[ICCV,2019] AMASS"></a>[ICCV,2019] AMASS</h3><p>Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a>, Michael J. Black<br><strong>AMASS: Archive of Motion Capture as Surface Shapes</strong></p>
<p>AMASS 作为曲面形状的运动捕捉存档</p>
<p>in <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2019</p>
<p><img src="http://virtualhumans.mpi-inf.mpg.de/papers/mahmood2019amass/mahmood2019amass.png" alt="AMASS: Archive of Motion Capture as Surface Shapes"></p>
<h3 id="3DV-2019-360tex"><a href="#3DV-2019-360tex" class="headerlink" title="[3DV, 2019] 360tex"></a>[3DV, 2019] 360tex</h3><p><a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/Lazova.html">Verica Lazova</a>, Eldar Insafutdinov, <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a><br><strong>360-Degree Textures of People in Clothing from a Single Image</strong><br>in <em>International Conference on 3D Vision (3DV)</em>, 2019.</p>
<p>源码: <a target="_blank" rel="noopener" href="http://virtualhumans.mpi-inf.mpg.de/360tex/">http://virtualhumans.mpi-inf.mpg.de/360tex/</a></p>
<p>从1张图片生成3D穿衣人物（带纹理）</p>
<p>在本文中，我们从一个单一的图像预测一个人的完整的3D化身。我们使用图像到图像的转换方法来推断SMPL模型的uv空间中的纹理和几何体。在给定输入视图的局部纹理和分割布局图的情况下，我们的模型预测了完整的分割图、完整的纹理图和位移图。预测出的地图可以应用到SMPL模型中，以便自然地推广到新的姿势、形状，甚至新衣服。为了在一个公共的UV空间中学习我们的模型，我们将SMPL模型非刚性地注册到数千个3D扫描中，有效地将纹理和几何图形编码为对应的图像。这将困难的三维推理任务转换为更简单的图像转换任务。对DeepFashion数据集中的人物和图像的渲染扫描结果表明，我们的方法可以从单个图像中重建出可信的3D化身。我们进一步使用我们的模型来数字化地改变姿势、形状、在人与人之间交换衣服和编辑衣服。为了鼓励这方面的研究，我们将提供用于研究目的的源代码[5]。</p>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/image-20200312141027764.png" alt="image-20200312141027764"></p>
<h3 id="CVPR-2019-Octopus"><a href="#CVPR-2019-Octopus" class="headerlink" title="[CVPR, 2019] Octopus"></a>[CVPR, 2019] Octopus</h3><p>Reconstruct 3D Cloth Body –CVPR_2019</p>
<ul>
<li>Single Image</li>
</ul>
<p>《Learning to Reconstruct People in Clothing From a Single RGB Camera》</p>
<p>8张照片生成3D模型</p>
<p>作者：T. Alldieck, M. A. Magnor, <strong>B. L. Bhatnagar</strong>, C. Theobalt and G. Pons-Moll</p>
<p>代码：<a target="_blank" rel="noopener" href="https://github.com/thmoa/octopus">https://github.com/thmoa/octopus</a></p>
<p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.05885">https://arxiv.org/abs/1903.05885</a></p>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/image-20200211215035833.png" alt="image-20200211215035833"></p>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/image-20200325191121616.png" alt="image-20200325191102583"></p>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/image-20200211215109044.png" alt="image-20200211215109044"></p>
<h4 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h4><p> purchased 163 scans from renderpeople.com </p>
<p>purchased 54 from axyzdesign.com. </p>
<p>1826 scans were kindly provided from Twindom (<a target="_blank" rel="noopener" href="https://web.twindom.com/">https://web.twindom.com/</a>).</p>
<p>PGN: 转换数据</p>
<p>网络结构:</p>
<pre><code> Generating 3D faces using convolutional mesh autoencoders. 
</code></pre>
<p>纹理计算参考:</p>
<p>​	Detailed human avatars from monocular video.</p>
<h4 id="steps"><a href="#steps" class="headerlink" title="steps:"></a>steps:</h4><p>​		Rendering</p>
<p>​		segmentation</p>
<h3 id="3DV-2018-8-Detailed-Human-Avatars-from-Monocular-Video"><a href="#3DV-2018-8-Detailed-Human-Avatars-from-Monocular-Video" class="headerlink" title="[3DV, 2018.8] [Detailed Human Avatars from Monocular Video]"></a>[3DV, 2018.8] [Detailed Human Avatars from Monocular Video]</h3><p><a target="_blank" rel="noopener" href="https://virtualhumans.mpi-inf.mpg.de/papers/alldieck2018detailed/alldieck2018detailed.pdf">paper</a></p>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/v2-e25b3e9869a93cf68fc200aad6649ed2_720w.jpg" alt="img"></p>
<p>(a). estimate a medium level body shape based on segmentations (基于语义分割的中等级别的身体形状)</p>
<p>(b). we add details using shape-from-shading.（Body Shape细节的优化）</p>
<p>(c). Finally we compute a texture using a semantic prior (c) [ a novel graph cut optimization strategy]</p>
<p><strong>问题：</strong> 如何从RGB视频中获取细致的人体模型</p>
<p><strong>输入：</strong> 单人的单目视频+轮廓信息+语义分割</p>
<p><strong>输出：</strong> 基于SMPL的更细致的人体模型</p>
<p>和之前的区别是，</p>
<ul>
<li>增加shape-from-shading方法，</li>
<li>对SMPL模型进行了划分，增加其点的数量与面片的数量</li>
<li>贴纹理用了graph cut优化</li>
</ul>
<p>代码:</p>
<p>纹理拼接代码<a target="_blank" rel="noopener" href="https://github.com/thmoa/semantic_human_texture_stitching">semantic human texture stitching</a></p>
<p>基于SMPL的p,$\theta$新增了W(混合皮肤)</p>
<p>skeleton joints J(β)</p>
<p>标准T-Pose </p>
<p>consists of N &#x3D; 110210 vertices and F &#x3D; 220416 faces.</p>
<h3 id="CVPR-2018-3-Video-Based-Reconstruction-of-3D-People-Models"><a href="#CVPR-2018-3-Video-Based-Reconstruction-of-3D-People-Models" class="headerlink" title="[CVPR, 2018.3] [Video Based Reconstruction of 3D People Models]"></a>[CVPR, 2018.3] [Video Based Reconstruction of 3D People Models]</h3><p><a target="_blank" rel="noopener" href="https://virtualhumans.mpi-inf.mpg.de/papers/alldieck2018video/alldieck2018videoshapes.pdf">Paper</a></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/97690143">论文解读</a></p>
</blockquote>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/v2-2e71c623346825e4327038443fb1dd04_720w.jpg" alt="img"> </p>
<p>calculate poses using the SMPL model (a)</p>
<p>unpose silhouette camera rays (unposed silhouettes depicted in red) unpose轮廓相机射线</p>
<p>optimize for the subjects shape in the canonical T-pose (c) 优化的对象形状，以规范T-pose形式 (c)</p>
<p> calculate a texture and generate a personalized blend shape model (d) 计算纹理和生成混合形状模型</p>
<p><strong>问题：</strong> 如何从RGB视频中获取细致的人体模型</p>
<p><strong>输入：</strong> 单人的单目视频+轮廓</p>
<p><strong>输出：</strong> 基于SMPL的细致的人体模型</p>
<p><a target="_blank" rel="noopener" href="https://graphics.tu-bs.de/people-snapshot">https://graphics.tu-bs.de/people-snapshot</a></p>
<ul>
<li>github code</li>
<li>dataset</li>
</ul>
<h3 id="ECCV-2018-VRN-Body"><a href="#ECCV-2018-VRN-Body" class="headerlink" title="[ECCV,2018]VRN-Body"></a>[ECCV,2018]VRN-Body</h3><p>《3D Human Body Reconstruction from a Single Image via Volumetric Regression》</p>
<ul>
<li>⭐⭐⭐</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.03770v1.pdf">https://arxiv.org/pdf/1809.03770v1.pdf</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A.S. Jackson [](https://aaronsplace.co.uk/) 英国诺丁汉大学计算机视觉实验室 研究员 -- 图像深度学习</span><br><span class="line">博士研究重点--人脸深度学习（3D重建，面部对齐， 语义部分分割）</span><br><span class="line">无源码</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://52zju.cn/?p=249">https://52zju.cn/?p=249</a></p>
<ul>
<li>3D Human Body Reconstruction</li>
<li>3D Texture 复原：效果一般，人物恢复模糊。</li>
</ul>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/image-20200211105449069.png" alt="image-20200211105449069"></p>
<h3 id="2017-HMR"><a href="#2017-HMR" class="headerlink" title="[2017] HMR"></a>[2017] HMR</h3><p>End to end recovery of human shape and pose: 201712</p>
<p><img src="/2020/01/17/CV_3D/CV-3D-Model-Apply/20191125115749291.png" alt="在这里插入图片描述"></p>
<h3 id="2019-MTC"><a href="#2019-MTC" class="headerlink" title="[2019] MTC"></a>[2019] MTC</h3><p><a target="_blank" rel="noopener" href="https://new.qq.com/omn/20200908/20200908A09RJD00.html">https://new.qq.com/omn/20200908/20200908A09RJD00.html</a></p>
<h3 id="2020-FrankMocap"><a href="#2020-FrankMocap" class="headerlink" title="[2020] FrankMocap"></a>[2020] FrankMocap</h3><p>科学怪物！3D人体全身运动捕捉系统，港中文联合Facebook出品</p>
<p><a target="_blank" rel="noopener" href="https://new.qq.com/omn/20200908/20200908A09RJD00.html">https://new.qq.com/omn/20200908/20200908A09RJD00.html</a></p>
<p>使用神经网络拟合smplx的人体参数，手部参数，极大的提升了原本SMPLify-x的运算效率</p>
<p><a target="_blank" rel="noopener" href="https://penincillin.github.io/frank_mocap">official web site introduction</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/frankmocap">github </a></p>
<p><img src="https://penincillin.github.io/frank_mocap/pipeline.jpg" alt="img"></p>
<h1 id="基础知识SMPL"><a href="#基础知识SMPL" class="headerlink" title="基础知识SMPL:"></a>基础知识SMPL:</h1><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>SMPL</td>
<td>N &#x3D; 6890 vertices and F &#x3D; 13776 faces.</td>
<td></td>
</tr>
<tr>
<td>video2mesh</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Video Reconstruct</td>
<td></td>
<td>穿裙子解决不了 改变不了smpl model的拓扑结构 拉不过去</td>
</tr>
<tr>
<td>Video Avatars—v1</td>
<td>&lt;Video based reconstruction of 3D people models&gt; 贴图方法:</td>
<td>同一个作者</td>
</tr>
<tr>
<td>VideoAvatars-v2</td>
<td><Detailed human avatars from monocular video.>  consists of N &#x3D; 110210 vertices and F &#x3D; 220416 faces.</Detailed></td>
<td>同一个作者</td>
</tr>
<tr>
<td>Octopus</td>
<td>octopus 有模型 有纹理贴图 用了Detailed Human Avatars from Monocular Video.的贴图方法</td>
<td>2019.4video2mesh延伸论文，同一实验室</td>
</tr>
<tr>
<td>Tex2Shape</td>
<td>N &#x3D; 27554 vertices and F &#x3D; 55104 faces</td>
<td></td>
</tr>
<tr>
<td>MGN</td>
<td>V &#x3D; 27554  VT+V &#x3D; 29193    F &#x3D; 54831</td>
<td></td>
</tr>
</tbody></table>
<h2 id="Video-Avatar重建（同一作者）"><a href="#Video-Avatar重建（同一作者）" class="headerlink" title="Video Avatar重建（同一作者）"></a>Video Avatar重建（同一作者）</h2><p>《1803 Video based reconstruction of 3D people models》</p>
<p>​	- 我们的主要贡献是在一个共同的参考系中，将对应于动态人体轮廓的轮廓锥变换成视觉外壳。</p>
<p>《1808 Detailed Human Avatars from Monocular Video.》 consists of N &#x3D; 110210 vertices and F &#x3D; 220416 faces.</p>
<p>1803：</p>
<ol>
<li><p>pose reconstruction (Sec. 3.2)</p>
</li>
<li><p>consensus shape estimation (Sec. 3.3) </p>
</li>
<li><p>frame refinement and texture map generation (Sec. 3.4).</p>
</li>
</ol>
<p>Our main contribution is step 2), the consensus shape estimation; </p>
<p>step 1) builds on previous work and step 3) to obtain texture and time-varying details is optional</p>
<h1 id="基础知识CV"><a href="#基础知识CV" class="headerlink" title="基础知识CV"></a>基础知识CV</h1><p>Non-rigid surface deformations 非刚性表面变形：</p>
<p>非刚性变形的思想是：皮肤变形，不是由单一骨骼的运动来控制，而是由许多骨头的共同运作的结果来支配。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2020/01/16/Paper/Paper-CV-NetworkRepresentationLearning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/16/Paper/Paper-CV-NetworkRepresentationLearning/" class="post-title-link" itemprop="url">Paper-CV-NetworkRepresentationLearning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-16 12:05:17" itemprop="dateCreated datePublished" datetime="2020-01-16T12:05:17+00:00">2020-01-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:40" itemprop="dateModified" datetime="2025-08-06T08:16:40+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[toc]</p>
<h1 id="NRL-Network-representation-learning"><a href="#NRL-Network-representation-learning" class="headerlink" title="NRL:Network representation learning"></a>NRL:Network representation learning</h1><p>NRL(网络表示学习)</p>
<p>GE(Graph Embedding图嵌入) ：侧重于降维； 更加强调是传统的图表示方法，比如LLE，LE，GF等</p>
<p>NE(Network Embedding网络嵌入)： 侧重是对于后来这种分布式假设的表示方法；比如Deepwalk，node2vec，LINE等。它们不仅关注降维任务，还更加强调要尽可能大的保存网络属性，比如高阶的近邻关系或者网络中的某些特性（如同质性&#x2F;结构等价性等）。</p>
<p>关于NRL&#x2F;GE&#x2F;NE的相关理论和文献:</p>
<p>《A Tutorial on Network Embeddings》. &#x2F;&#x2F;这是今年Deepwalk团队的作者给的综述（教程），写的很好。</p>
<p>《Graph Embedding Techniques, Applications, and Performance: A Survey.》 &#x2F;&#x2F;该文写的比较浅显，但整体上不错，适合初学者，并且提供了GEM工具包。</p>
<p>《A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications》. &#x2F;&#x2F;该文写的比较全面，适合进阶。</p>
<h1 id="A-Tutorial-on-Network-Embeddings"><a href="#A-Tutorial-on-Network-Embeddings" class="headerlink" title="A Tutorial on Network Embeddings"></a>A Tutorial on Network Embeddings</h1><p>paper：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.02590">https://arxiv.org/abs/1808.02590</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/chaoran/p/9720667.html">中文阅读-A Tutorial on Network Embeddings</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40042143/article/details/82940542"></a></p>
<h1 id="DeepWalk"><a href="#DeepWalk" class="headerlink" title="DeepWalk"></a>DeepWalk</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40042143/article/details/82940542">DeepWalk原理理解：DeepWalk: online learning of social representations</a></p>
<p><a target="_blank" rel="noopener" href="https://classes.cs.uoregon.edu/17S/cis607bddl/papers/Perozzi.pdf">DeepWork.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/45167021">【论文笔记】DeepWalk</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/phanein/deepwalk">Github </a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2020/01/10/CV/CV-Human-attr/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/10/CV/CV-Human-attr/" class="post-title-link" itemprop="url">人脸属性分析–性别</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-10 12:00:00" itemprop="dateCreated datePublished" datetime="2020-01-10T12:00:00+00:00">2020-01-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/" itemprop="url" rel="index"><span itemprop="name">CV_Apply</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/Face-Edit/" itemprop="url" rel="index"><span itemprop="name">Face Edit</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="人脸属性分析–性别、年龄和表情识别开源集合"><a href="#人脸属性分析–性别、年龄和表情识别开源集合" class="headerlink" title="人脸属性分析–性别、年龄和表情识别开源集合"></a>人脸属性分析–性别、年龄和表情识别开源集合</h1><p>人脸属性指的是根据给定的人脸判断其性别、年龄和表情等，当前在github上开源了一些相关的<a target="_blank" rel="noopener" href="http://www.seotest.cn/wenzhang/gongzuo/">工作</a>，大部分都是基于tensorflow的，还有一部分是keras，CVPR2015曾有一篇是用caffe做的。</p>
<p>1、CVPR2015 caffe实现</p>
<p><a target="_blank" rel="noopener" href="https://github.com/GilLevi/AgeGenderDeepLearning">https://github.com/GilLevi/AgeGenderDeepLearning</a></p>
<p>2、CVPR2015对应的tensorflow实现</p>
<p><a target="_blank" rel="noopener" href="https://github.com/dpressel/rude-carnie">https://github.com/dpressel/rude-carnie</a></p>
<p>3、DEX: Deep EXpectation 实现</p>
<p><a target="_blank" rel="noopener" href="https://github.com/truongnmt/multi-task-learning">https://github.com/truongnmt/multi-task-learning</a></p>
<p>4、CVPR2017 Age <a target="_blank" rel="noopener" href="http://www.seotest.cn/jishu/34534.html">progress</a>ion&#x2F;<a target="_blank" rel="noopener" href="http://www.seotest.cn/jishu/34304.html">regression</a> by <a target="_blank" rel="noopener" href="http://www.seotest.cn/jishu/33944.html">conditional</a> Adversarial Autoencoder </p>
<p><a target="_blank" rel="noopener" href="https://github.com/ZZUTK/Face-Aging-CAAE">https://github.com/ZZUTK/Face-Aging-CAAE</a></p>
<p>5、使用<a target="_blank" rel="noopener" href="http://www.seotest.cn/jishu/34449.html">inception</a> v1同时预测性别和年龄，受限于使用的dlib检测器，效果<a target="_blank" rel="noopener" href="http://www.seotest.cn/yingxiao/36205.html">并不是</a>很好</p>
<p><a target="_blank" rel="noopener" href="https://github.com/BoyuanJiang/Age-Gender-Estimate-TF">https://github.com/BoyuanJiang/Age-Gender-Estimate-TF</a></p>
<p>6、性别种族识别：gender Accuracy: 0.951493,race Accuracy: 0.87557212</p>
<p><a target="_blank" rel="noopener" href="https://github.com/zZyan/race_gender_[recognition](http://www.seotest.cn/jishu/35507.html)">https://github.com/zZyan/race_gender_[recognition](http://www.seotest.cn/jishu/35507.html)</a></p>
<p>8、性别识别全流程实现：<strong>94% accuracy</strong> </p>
<p><a target="_blank" rel="noopener" href="https://github.com/jocialiang/gender_classifier">https://github.com/jocialiang/gender_classifier</a></p>
<p>9、表情、性别识别（keras）</p>
<p><a target="_blank" rel="noopener" href="https://github.com/oarriaga/face_classification">https://github.com/oarriaga/face_classification</a></p>
<p>注明：两个分开的模型对应表情识别和性别识别</p>
<p>10、性别和种族识别</p>
<p><a target="_blank" rel="noopener" href="https://github.com/wondonghyeon/face-classification">https://github.com/wondonghyeon/face-classification</a></p>
<p>11、年龄识别</p>
<p><a target="_blank" rel="noopener" href="https://github.com/shamangary/SSR-Net">https://github.com/shamangary/SSR-Net</a></p>
<p>12、年龄识别（亚洲人<a target="_blank" rel="noopener" href="http://www.seotest.cn/wenzhang/youhua/">优化</a>）</p>
<p><a target="_blank" rel="noopener" href="https://github.com/b02901145/SSR-Net_megaage-asian">https://github.com/b02901145/SSR-Net_megaage-asian</a></p>
<p>13、年龄和性别识别(Keras )</p>
<p><a target="_blank" rel="noopener" href="https://github.com/yu4u/age-gender-estimation">https://github.com/yu4u/age-gender-estimation</a></p>
<p>14、表情和性别识别：表情66% with fer2013</p>
<p><a target="_blank" rel="noopener" href="https://github.com/isseu/emotion-recognition-neural-networks">https://github.com/isseu/emotion-recognition-neural-networks</a></p>
<p>15、性别和年龄识别(tensorflow )：<strong>91%</strong> accuracy in gender and <strong>55%</strong> in age</p>
<p><a target="_blank" rel="noopener" href="https://github.com/zealerww/gender_age_classification">https://github.com/zealerww/gender_age_classification</a></p>
<p><strong>16</strong>、人脸检测、性别和表情识别、数字化妆、轮廓标识等多功能（tensorflow 、keras）</p>
<p><a target="_blank" rel="noopener" href="https://github.com/vipstone/faceai">https://github.com/vipstone/faceai</a></p>
<p><a target="_blank" rel="noopener" href="http://www.seotest.cn/wenzhang/ai_7841/">http://www.seotest.cn/wenzhang/ai_7841/</a></p>
<p>17、表情识别（tensorflow、keras）：fer2013</p>
<p><a target="_blank" rel="noopener" href="https://github.com/XiuweiHe/EmotionClassifier">https://github.com/XiuweiHe/EmotionClassifier</a></p>
<p>18、表情和种族识别：表情 72% accuracy ，种族95% accuracy</p>
<p><a target="_blank" rel="noopener" href="https://github.com/HectorAnadon/Face-expression-and-ethnic-recognition">https://github.com/HectorAnadon/Face-expression-and-ethnic-recognition</a></p>
<p>19、表情识别（caffe）:66.7% on fer2013 with resnet50</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ybch14/Facial-Expression-Recognition-ResNet">https://github.com/ybch14/Facial-Expression-Recognition-ResNet</a></p>
<p>20、表情识别（keras）</p>
<p><a target="_blank" rel="noopener" href="https://github.com/JostineHo/mememoji">https://github.com/JostineHo/mememoji</a></p>
<p>21、种族识别</p>
<p><a target="_blank" rel="noopener" href="https://github.com/mangorocoro/racedetector">https://github.com/mangorocoro/racedetector</a></p>
<p>22、多任务学习:性别、年龄、表情（tensorflow）</p>
<p><a target="_blank" rel="noopener" href="https://github.com/truongnmt/multi-task-learning">https://github.com/truongnmt/multi-task-learning</a></p>
<p>23、性别识别（tensorflow、keras）:爬虫图片+人脸提取识别</p>
<p><a target="_blank" rel="noopener" href="https://github.com/StevenKe8080/recognition_gender">https://github.com/StevenKe8080/recognition_gender</a></p>
<p>24、年龄识别（tensorflow）</p>
<p><a target="_blank" rel="noopener" href="https://github.com/zonetrooper32/AgeEstimateAdience">https://github.com/zonetrooper32/AgeEstimateAdience</a></p>
<p>25、年龄和性别识别</p>
<p><a target="_blank" rel="noopener" href="https://github.com/OValery16/gender-age-classification">https://github.com/OValery16/gender-age-classification</a></p>
<p><strong>26、表情识别</strong></p>
<p><a target="_blank" rel="noopener" href="https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch">【Fer】GitHub - WuJie1010&#x2F;Facial-Expression-Recognition.Pytorch </a></p>
<p><a target="_blank" rel="noopener" href="https://modelscope.cn/models/damo/cv_vgg19_facial-expression-recognition_fer/summary">modelscope.cn &#x2F; Facial Expression Recognition Summary</a></p>
<p>A CNN based pytorch implementation on facial expression recognition (FER2013 and CK+), achieving 73.112% (state-of-the-art) in FER2013 and 94.64% in CK+ dataset</p>
<h1 id="姿态识别"><a href="#姿态识别" class="headerlink" title="姿态识别"></a>姿态识别</h1><p>详见: <a href="../Paper/Paper-CV-PoseEstimation.md">..&#x2F;Paper&#x2F;Paper-CV-PoseEstimation.md</a></p>
<p>1、多人姿态识别框架——<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2119537?from=15425&areaSource=102001.1&traceId=_4NO_SAwr9tc-Xhx9HNzm">AlphaPose</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/MVIG-SJTU/AlphaPose">https://github.com/MVIG-SJTU/AlphaPose</a></p>
<p>2、</p>
<h3 id="开源表情"><a href="#开源表情" class="headerlink" title="开源表情"></a>开源表情</h3><p><a target="_blank" rel="noopener" href="https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch">GitHub - WuJie1010&#x2F;Facial-Expression-Recognition.Pytorch: A CNN based pytorch implementation on facial expression recognition (FER2013 and CK+), achieving 73.112% (state-of-the-art) in FER2013 and 94.64% in CK+ dataset</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/vipstone/faceai">GitHub - vipstone&#x2F;faceai: 一款入门级的人脸、视频、文字检测以及识别的项目.</a></p>
<p><a target="_blank" rel="noopener" href="https://modelscope.cn/models/damo/cv_vgg19_facial-expression-recognition_fer/summary">阿里# Fer 模型介绍</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2020/01/09/AI/ML/ML-datasets/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/09/AI/ML/ML-datasets/" class="post-title-link" itemprop="url">ML-datasets -- 物体识别</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-09 14:29:58" itemprop="dateCreated datePublished" datetime="2020-01-09T14:29:58+00:00">2020-01-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:37" itemprop="dateModified" datetime="2025-08-06T08:16:37+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Datasets/" itemprop="url" rel="index"><span itemprop="name">Datasets</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/Datasets/" itemprop="url" rel="index"><span itemprop="name">Datasets</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[toc]</p>
<h2 id="开源数据集-物体识别："><a href="#开源数据集-物体识别：" class="headerlink" title="开源数据集-物体识别："></a>开源数据集-物体识别：</h2><h3 id="Cifar10：go-ref"><a href="#Cifar10：go-ref" class="headerlink" title="Cifar10：go: ref:"></a>Cifar10：<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar.html">go</a>: <a target="_blank" rel="noopener" href="https://www.cnblogs.com/Jerry-Dong/p/8109938.html">ref</a>:</h3><p><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz">http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz</a></p>
<p><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar-10-matlab.tar.gz">http://www.cs.toronto.edu/~kriz/cifar-10-matlab.tar.gz</a></p>
<p><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz">http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz</a></p>
<p>该数据集文件包含data_batch1……data_batch5，和test_batch。他们都是由cPickle库产生的序列化后的对象（关于pickle,移步<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/pickle.html%EF%BC%89%E3%80%82">https://docs.python.org/3/library/pickle.html）。</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">unpickle</span>(<span class="params">file</span>):</span><br><span class="line">	<span class="keyword">import</span> pickle</span><br><span class="line">	<span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fo:</span><br><span class="line">	    <span class="built_in">dict</span> = pickle.load(fo, encoding=<span class="string">&#x27;bytes&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span> </span><br></pre></td></tr></table></figure>



<h3 id="Cifar100-go"><a href="#Cifar100-go" class="headerlink" title="Cifar100 go:"></a>Cifar100 <a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar.html">go</a>:</h3><table>
<thead>
<tr>
<th>Version</th>
<th>Size</th>
<th>md5sum</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz">CIFAR-100 python version</a></td>
<td>161 MB</td>
<td>eb9058c3a382ffc7106e4002c42a8d85</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar-100-matlab.tar.gz">CIFAR-100 Matlab version</a></td>
<td>175 MB</td>
<td>6a4bfa1dcd5c9453dda6bb54194911f4</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~kriz/cifar-100-binary.tar.gz">CIFAR-100 binary version (suitable for C programs)</a></td>
<td>161 MB</td>
<td>03b5dce01913d631647c71ecec9e9cb8</td>
</tr>
</tbody></table>
<h3 id="VOC"><a href="#VOC" class="headerlink" title="VOC:"></a>VOC:</h3><h3 id="LSUN-go"><a href="#LSUN-go" class="headerlink" title="LSUN: go"></a>LSUN: <a target="_blank" rel="noopener" href="https://www.yf.io/p/lsun">go</a></h3><p>LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop</p>
<p><img src="https://www.yf.io/p/lsun/img/teaser_web.jpg" alt="img"></p>
<p>国外的PASCAL<br>VOC和ImageNet ILSVRC比赛使用的数据集，数据领域包括卧室、冰箱、教师、厨房、起居室、酒店等多个主题。</p>
<p>推荐度：★★，推荐应用方向：图像识别</p>
<p>介绍和下载地址：<a href="https://link.zhihu.com/?target=http://lsun.cs.princeton.edu">http://lsun.cs.princeton.edu</a></p>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>While there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classification confidence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and find that they achieve substantial performance gains when trained on this dataset.</p>
<h4 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h4><p>Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser and Jianxiong Xiao<br><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1506.03365">LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop</a><br><em>arXiv:1506.03365 [cs.CV], 10 Jun 2015</em></p>
<h4 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h4><p><strong>10 scene categories</strong> for LSUN Scene Classification Challange: <a target="_blank" rel="noopener" href="https://github.com/fyu/lsun">Downloading Code</a></p>
<p><strong>20 object categories</strong>: <a target="_blank" rel="noopener" href="http://dl.yf.io/lsun/objects/">Link List</a>. Images for each category are stored in LMDB format and the database is then zipped. After downloading and decompressing the zip files, please to refer to <a target="_blank" rel="noopener" href="https://github.com/fyu/lsun">LSUN utility code</a> to visualize and export the images. MD5 sum for each zip file is also provided so that you can verify your downloads.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">../</span><br><span class="line">airplane.zip                                       06-Mar-2019 00:14     34G</span><br><span class="line">airplane.zip.md5                                   19-Dec-2019 20:04      47</span><br><span class="line">bicycle.zip                                        06-Mar-2019 00:44    129G</span><br><span class="line">bicycle.zip.md5                                    19-Dec-2019 20:04      46</span><br><span class="line">bird.zip                                           06-Mar-2019 00:57     65G</span><br><span class="line">bird.zip.md5                                       19-Dec-2019 20:04      43</span><br><span class="line">boat.zip                                           06-Mar-2019 01:12     86G</span><br><span class="line">boat.zip.md5                                       19-Dec-2019 20:04      43</span><br><span class="line">bottle.zip                                         06-Mar-2019 01:24     64G</span><br><span class="line">bottle.zip.md5                                     19-Dec-2019 20:04      45</span><br><span class="line">bus.zip                                            06-Mar-2019 01:29     24G</span><br><span class="line">bus.zip.md5                                        19-Dec-2019 20:04      42</span><br><span class="line">car.zip                                            06-Mar-2019 02:05    173G</span><br><span class="line">car.zip.md5                                        19-Dec-2019 20:04      42</span><br><span class="line">cat.zip                                            06-Mar-2019 02:12     42G</span><br><span class="line">cat.zip.md5                                        19-Dec-2019 20:04      42</span><br><span class="line">chair.zip                                          06-Mar-2019 02:31    116G</span><br><span class="line">chair.zip.md5                                      19-Dec-2019 20:04      44</span><br><span class="line">cow.zip                                            06-Mar-2019 02:34     15G</span><br><span class="line">cow.zip.md5                                        19-Dec-2019 20:04      42</span><br><span class="line">dining_table.zip                                   06-Mar-2019 02:50     48G</span><br><span class="line">dining_table.zip.md5                               19-Dec-2019 20:04      51</span><br><span class="line">dog.zip                                            06-Mar-2019 03:14    145G</span><br><span class="line">dog.zip.md5                                        19-Dec-2019 20:04      42</span><br><span class="line">horse.zip                                          06-Mar-2019 03:25     69G</span><br><span class="line">horse.zip.md5                                      19-Dec-2019 20:04      44</span><br><span class="line">motorbike.zip                                      06-Mar-2019 03:32     42G</span><br><span class="line">motorbike.zip.md5                                  19-Dec-2019 20:04      48</span><br><span class="line">person.zip                                         06-Mar-2019 04:47    477G</span><br><span class="line">person.zip.md5                                     19-Dec-2019 20:04      45</span><br><span class="line">potted_plant.zip                                   06-Mar-2019 04:54     43G</span><br><span class="line">potted_plant.zip.md5                               19-Dec-2019 20:04      51</span><br><span class="line">sheep.zip                                          06-Mar-2019 04:57     18G</span><br><span class="line">sheep.zip.md5                                      19-Dec-2019 20:04      44</span><br><span class="line">sofa.zip                                           06-Mar-2019 05:06     56G</span><br><span class="line">sofa.zip.md5                                       19-Dec-2019 20:04      43</span><br><span class="line">train.zip                                          06-Mar-2019 05:13     43G</span><br><span class="line">train.zip.md5                                      19-Dec-2019 20:04      44</span><br><span class="line">tv-monitor.zip                                     06-Mar-2019 05:21     46G</span><br><span class="line">tv-monitor.zip.md5                                 19-Dec-2019 20:04      49</span><br></pre></td></tr></table></figure>

<h4 id="LSUN-Challenge"><a href="#LSUN-Challenge" class="headerlink" title="LSUN Challenge"></a>LSUN Challenge</h4><p>In CVPR 2015 and 2016, a image classification challenge has been hosted in LSUN Challenge workshop to evaluate the progress of large-scale image understanding. More information can be found at the <a target="_blank" rel="noopener" href="http://lsun.cs.princeton.edu/2016/">challenge webpage</a>.</p>
<h3 id="ImageNet数据集"><a href="#ImageNet数据集" class="headerlink" title="ImageNet数据集"></a><strong>ImageNet数据集</strong></h3><p>ImageNet数据集是目前深度学习图像领域应用得非常多的一个领域，该数据集有1000多个图像，涵盖图像分类、定位、检测等应用方向。Imagenet数据集文档详细，有专门的团队维护，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。很多大型科技公司都会参加ImageNet图像识别大赛，包括百度、谷歌、微软等。</p>
<p>推荐度：★★★，推荐应用方向：图像识别</p>
<p>介绍和下载地址：<a href="https://link.zhihu.com/?target=http://www.image-net.org/">http://www.image-net.org/</a></p>
<h3 id="Tiny-Images-Dataset"><a href="#Tiny-Images-Dataset" class="headerlink" title="Tiny Images Dataset"></a><strong>Tiny Images Dataset</strong></h3><p>该数据集由79302017张图像组成，每张图像为32x32彩色图像。 该数据以二进制文件的形式存储，大约有400Gb图像。</p>
<p>推荐度：★★，推荐应用方向：图像识别</p>
<p>介绍和下载地址：<a href="https://link.zhihu.com/?target=http://horatio.cs.nyu.edu/mit/tiny/data/index.html">http://horatio.cs.nyu.edu/mit/tiny/data/index.html</a></p>
<h3 id="CoPhIR"><a href="#CoPhIR" class="headerlink" title="CoPhIR"></a><strong>CoPhIR</strong></h3><p>CoPhIR是从Flickr中采集的大概1.06亿个图像数据集，图像中不仅包含了图表本身的数据，例如位置、标题、GPS、标签、评论等，还可提取出颜色模式、颜色布局、边缘直方图、均匀纹理等数据。</p>
<p>推荐度：★★，推荐应用方向：图像识别</p>
<p>介绍和下载地址：<a href="https://link.zhihu.com/?target=http://cophir.isti.cnr.it/whatis.html">http://cophir.isti.cnr.it/whatis.html</a></p>
<h3 id="Labeled-Faces-in-the-Wild数据集"><a href="#Labeled-Faces-in-the-Wild数据集" class="headerlink" title="Labeled Faces in the Wild数据集"></a><strong>Labeled Faces in the Wild数据集</strong></h3><p>该数据集是用于研究无约束面部识别问题的面部照片数据库。数据集包含从网络收集的13000多张图像。每张脸都贴上了所画的人的名字，图片中的1680人在数据集中有两个或更多不同的照片。</p>
<p>推荐度：★★，推荐应用方向：人脸识别</p>
<p>介绍和下载地址：<a href="https://link.zhihu.com/?target=http://vis-www.cs.umass.edu/lfw/">http://vis-www.cs.umass.edu/lfw/</a></p>
<h3 id="SVHN"><a href="#SVHN" class="headerlink" title="SVHN"></a><strong>SVHN</strong></h3><p>SVHN数据来源于 Google 街景视图中房屋信息，它是一个真实世界的图像数据集，用于开发机器学习和对象识别算法，对数据预处理和格式化的要求最低。它跟MNIST相似，但是包含更多数量级的标签数据（超过60万个数字图像），并且来源更加多样，用来识别自然场景图像中的数字。 </p>
<p>推荐度：★★，推荐应用方向：机器学习、图像识别</p>
<p>介绍和下载地址：<a href="https://link.zhihu.com/?target=http://ufldl.stanford.edu/housenumbers/">http://ufldl.stanford.edu/housenumbers/</a></p>
<h3 id="MS-COCO"><a href="#MS-COCO" class="headerlink" title="MS COCO"></a><strong>MS COCO</strong></h3><p>COCO（Common Objects in Context）是一个新的图像识别、分割和图像语义数据集，由微软赞助，图像中不仅有标注类别、位置信息，还有对图像的语义文本描述。COCO数据集的开源使得近两、三年来图像分割语义理解取得了巨大的进展，也几乎成为了图像语义理解算法性能评价的“标准”数据集。</p>
<p>推荐度：★★★，推荐应用方向：图像识别、图像语义理解</p>
<p>介绍和下载地址：<a href="https://link.zhihu.com/?target=http://mscoco.org/">http://mscoco.org/</a></p>
<h3 id="谷歌YouTube-8M"><a href="#谷歌YouTube-8M" class="headerlink" title="谷歌YouTube-8M"></a><strong>谷歌YouTube-8M</strong></h3><p>YouTube-8M一个大型的多样性标注的视频数据集，目前拥有700万的YouTube视频链接、45万小时视频时长、3.2亿视频&#x2F;音频特征、4716个分类、平均每个视频拥有3个标签。</p>
<p>推荐度：★★★，推荐应用方向：视频理解、表示学习（representation learning）、嘈杂数据建模、转移学习（transfer learning）和视频域适配方法（domain<br>adaptation approaches）</p>
<p>数据集介绍和下载地址：<a href="https://link.zhihu.com/?target=https://research.google.com/youtube8m/">https://research.google.com/youtube8m/</a>。</p>
<h3 id="Udacity开源的车辆行使视频数据集"><a href="#Udacity开源的车辆行使视频数据集" class="headerlink" title="Udacity开源的车辆行使视频数据集"></a><strong>Udacity开源的车辆行使视频数据集</strong></h3><p>数据集大概有223G，主要是有关车辆驾驶的数据，其中除了车辆拍摄的图像以外，还包括车辆本身的属性和参数信息，例如经纬度、制动器、油门、转向度、转速等。这些数据可用于车辆自动驾驶方向的模型训练和学习。</p>
<p>推荐度：★★★，推荐应用方向：自动驾驶</p>
<p>介绍和下载地址：<a href="https://link.zhihu.com/?target=https://github.com/udacity/self-driving-car">https://github.com/udacity/self-driving-car</a></p>
<h3 id="牛津RobotCar视频数据集"><a href="#牛津RobotCar视频数据集" class="headerlink" title="牛津RobotCar视频数据集"></a><strong>牛津RobotCar视频数据集</strong></h3><p>RobotCar数据集包含时间范围超过1年，测试超过100次的相同路线的驾驶数据。数据集采集了天气、交通、行人、建筑和道路施工等不同组合的数据。</p>
<p>推荐度：★★★，推荐应用方向：自动驾驶</p>
<p>介绍和下载地址：<a href="https://link.zhihu.com/?target=http://robotcar-dataset.robots.ox.ac.uk/">http://robotcar-dataset.robots.ox.ac.uk/</a></p>
<h3 id="Udacity开源的自然场景短视频数据集"><a href="#Udacity开源的自然场景短视频数据集" class="headerlink" title="Udacity开源的自然场景短视频数据集"></a><strong>Udacity开源的自然场景短视频数据集</strong></h3><p>数据集大概为9T，由3500万个视频剪辑组成，每个视频为短视频（32帧），大约1秒左右的时长。</p>
<p>推荐度：★★★，推荐应用方向：目标跟踪、视频目标识别</p>
<p>介绍和下载地址：<a href="https://link.zhihu.com/?target=http://web.mit.edu/vondrick/tinyvideo/%23data">http://web.mit.edu/vondrick/tinyvideo/#data</a></p>
<h2 id="3-自然语言数据集"><a href="#3-自然语言数据集" class="headerlink" title="3. 自然语言数据集"></a>3. 自然语言数据集</h2><p>【todo】</p>
<p>ref：</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/63383992">https://www.zhihu.com/question/63383992</a></p>
<p><a target="_blank" rel="noopener" href="http://blog.itpub.net/29829936/viewspace-2219159/">http://blog.itpub.net/29829936/viewspace-2219159/</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2020/01/07/Books/Reading-2020-Yoga/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/07/Books/Reading-2020-Yoga/" class="post-title-link" itemprop="url">Reading_2020_Yoga</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-07 20:18:54" itemprop="dateCreated datePublished" datetime="2020-01-07T20:18:54+00:00">2020-01-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Book/" itemprop="url" rel="index"><span itemprop="name">Book</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="瑜伽动作索引"><a href="#瑜伽动作索引" class="headerlink" title="瑜伽动作索引"></a>瑜伽动作索引</h1><p>[TOC]</p>
<h2 id="站式"><a href="#站式" class="headerlink" title="站式"></a>站式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Tadasana—Mountain Pose（山式）</span><br><span class="line">Variation: Samasthiti—Equal Standing, Prayer Pose（山式变式）</span><br><span class="line">Utkatasana—Chair Pose, Awkward Pose（椅式）</span><br><span class="line">Uttanasana—Standing Forward Bend（站立前屈式）</span><br><span class="line">Utthita Hasta Padangusthasana—Extended Hand Toe Pose（手拉脚单腿直立式）</span><br><span class="line">Variation: With Spine Flexed（脊柱屈曲式）</span><br><span class="line">Vrksasana—Tree Pose（树式）</span><br><span class="line">Variation: With Arms Elevated（抬手树式）</span><br><span class="line">Garudasana—Eagle Pose（鹰式）</span><br><span class="line">Natarajasana—King of the Dancers Pose（舞王式）</span><br><span class="line">Virabhadrasana I—Warrior I（勇士式I）</span><br><span class="line">Variation: With Longer Stance（长站姿式）</span><br><span class="line">Virabhadrasana II—Warrior II（勇士式II）</span><br><span class="line">Virabhadrasana III—Warrior III（勇士式III）</span><br><span class="line">Utthita Parsvakonasana—Extended Side Angle Pose（伸展侧角式）</span><br><span class="line">Parivrtta Baddha Parsvakonasana—Revolved Side Angle Pose（旋转侧角式）</span><br><span class="line">Utthita Trikonasana—Extended Triangle Pose（伸展三角式）</span><br><span class="line">Variation: With Longer Stance（长站姿式）</span><br><span class="line">Parivrtta Trikonasana—Revolved Triangle Pose（旋转三角式）</span><br><span class="line">Parsvottanasana—Intense Side Stretch（加强侧伸展式）</span><br><span class="line">Variation: With Arms in Reverse Namaskar（双手背后合十式）</span><br><span class="line">Variation: With Spine Flexed（脊柱屈曲式）</span><br><span class="line">Prasarita Padottanasana—Wide-Stance Forward Bend（宽站姿前屈式）</span><br><span class="line">Upavesasana—Squat, Sitting-Down Pose（下蹲式）</span><br></pre></td></tr></table></figure>

<h2 id="坐式"><a href="#坐式" class="headerlink" title="坐式"></a>坐式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Dandasana—Staff Pose（手杖式）</span><br><span class="line">Paschimottanasana—West (Back) Stretching（西部（背部）伸展式）</span><br><span class="line">Janu Sirsasana—Head-to-Knee Pose（头到膝式）</span><br><span class="line">Parivrtta Janu Sirsasana—Revolved Head-to-Knee Pose（反转头碰膝式）</span><br><span class="line">Mahamudra—The Great Seal（内女式）</span><br><span class="line">Upavistha Konasana—Seated Wide-Angle Pose（坐式广角姿势）</span><br><span class="line">Baddha Konasana—Bound Angle Pose（蝴蝶式）</span><br><span class="line">Variation: Supta Baddha Konasana—Reclining Bound Angle Pose（卧蝴蝶式）</span><br><span class="line">Kurmasana—Turtle Pose（乌龟式）</span><br><span class="line">Variation: Supta Kurmasana—Reclining Turtle Pose（卧龟式）</span><br><span class="line">Ardha Matsyendrasana—Half Lord of the Fishes Pose（半鱼主式）</span><br><span class="line">Gomukhasana—Cow-Faced Pose（牛面式）</span><br><span class="line">Hanumanasana—Monkey Pose（神猴式）</span><br><span class="line">Navasana—Boat Pose（船式）</span><br></pre></td></tr></table></figure>

<h2 id="跪式"><a href="#跪式" class="headerlink" title="跪式"></a>跪式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Balasana—Child&#x27;s Pose（婴儿式）</span><br><span class="line">Supta Virasana—Reclining Hero Pose（卧英雄式）</span><br><span class="line">Ustrasana—Camel Pose（骆驼式）</span><br><span class="line">Eka Pada Rajakapotasana—One-Legged Royal Pigeon Pose（单脚鸽王式）</span><br><span class="line">Variation: Folded Forward（向前折叠）</span><br><span class="line">Parighasana—Gate-Latch Pose（门闩式）</span><br><span class="line">Simhasana—Lion Pose（狮子式）</span><br></pre></td></tr></table></figure>

<h2 id="仰卧式"><a href="#仰卧式" class="headerlink" title="仰卧式"></a>仰卧式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Savasana—Corpse Pose（挺卧式）</span><br><span class="line">Apanasana—Apana Pose, Wind Release Pose（下行气姿势、风速释放姿势）</span><br><span class="line">Setu Bandhasana—Bridge Pose（桥式）</span><br><span class="line">Variation: Dwi Pada Pitham—Two-Legged Table（双脚支撑式）</span><br><span class="line">Salamba Sarvangasana—Supported Shoulder Stand（受支撑的肩倒立式）</span><br><span class="line">Niralamba Sarvangasana—Unsupported (No-Arm) Shoulder Stand（无支撑（不用手臂）肩倒立式）</span><br><span class="line">Viparita Karani—Inverted Pose（倒立姿势）</span><br><span class="line">Halasana—Plow Pose（犁式）</span><br><span class="line">Karnapidasana—Ear-to-Knee Pose（耳到膝式）</span><br><span class="line">Jathara Parivrtti—Belly Twist（腹部扭转式）</span><br><span class="line">Variation: With Legs Extended（双腿伸直）</span><br><span class="line">Matsyasana—Fish Pose（鱼式）</span><br><span class="line">Variation: With Arms and Legs Lifted（提升手臂和双腿）</span><br><span class="line">Anantasana—Reclining Vishnu Couch Pose（毗湿奴躺沙发式）</span><br></pre></td></tr></table></figure>

<h2 id="俯卧式"><a href="#俯卧式" class="headerlink" title="俯卧式"></a>俯卧式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Bhujangasana—Cobra Pose（蛇式）</span><br><span class="line">Variation: With Knees Flexed（膝盖弯曲）</span><br><span class="line">Dhanurasana—Bow Pose（弓式）</span><br><span class="line">Salabhasana—Locust Pose（蝗虫式）</span><br><span class="line">Viparita Salabhasana—Full Locust Pose（全蝗虫式）</span><br></pre></td></tr></table></figure>

<h2 id="臂架式"><a href="#臂架式" class="headerlink" title="臂架式"></a>臂架式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Adho Mukha Svanasana—Downward-Facing Dog Pose（下犬式）</span><br><span class="line">Urdhva Mukha Svanasana—Upward-Facing Dog Pose（上犬式）</span><br><span class="line">Adho Mukha Vrksasana—Downward-Facing Tree Pose（手倒立式）</span><br><span class="line">Chaturanga Dandasana—Four-Limbed Stick Pose（四肢支撑式）</span><br><span class="line">Bakasana—Crow Pose, Crane Pose（乌鸦式、鹤禅式）</span><br><span class="line">Parsva Bakasana—Side Crow Pose, Side Crane Pose（侧乌鸦式、侧鹤式）</span><br><span class="line">Astavakrasana—Eight-Angle Pose（八角式）</span><br><span class="line">Mayurasana—Peacock Pose（孔雀式）</span><br><span class="line">Pincha Mayurasana—Feathered Peacock Pose（孔雀起舞式）</span><br><span class="line">Salamba Sirsasana—Supported Headstand（支撑头倒立式）</span><br><span class="line">Vrschikasana—Scorpion Pose（蝎子式）</span><br><span class="line">Urdhva Dhanurasana—Upward Bow Pose, Wheel Pose（上弓式、轮式）</span><br><span class="line">Vasisthasana—Side Plank Pose, Sage Vasistha&#x27;s Pose（侧板式、圣人瓦西斯塔式）</span><br><span class="line">Chatus Pada Pitham—Four-Footed Tabletop Pose（四脚桌子式）</span><br><span class="line">Purvottanasana—Upward Plank Pose（上平板式）</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2020/01/07/Paper/Paper-CV-GAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/07/Paper/Paper-CV-GAN/" class="post-title-link" itemprop="url">Paper-CV-GAN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-07 11:03:52" itemprop="dateCreated datePublished" datetime="2020-01-07T11:03:52+00:00">2020-01-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:40" itemprop="dateModified" datetime="2025-08-06T08:16:40+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV/Networks/" itemprop="url" rel="index"><span itemprop="name">Networks</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV/Networks/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>NVlab </p>
<p>[TOC]</p>
<h1 id="VAE生成模型"><a href="#VAE生成模型" class="headerlink" title="VAE生成模型"></a>VAE生成模型</h1><p>Variational autoencoder &#x3D;&#x3D; 变分自编码器</p>
<p>可以输入一个低维空间的Z，映射到高维空间的真实数据。比如，生成不同样的数字，人脸，卡通头像等等。</p>
<img src="/2020/01/07/Paper/Paper-CV-GAN/VAE_12.png" alt="img" style="zoom:73%;">

<p>2）VAE与GAN之间的差异性</p>
<p>既然VAE与GAN都是属于最近很火的生成网络序列，那么他们之间有什么不同呢？</p>
<p>假设，给定一系列猫的照片，我希望你能够对应我随机输入的一个n维向量，生成一张新的猫的照片，你需要怎么去做?对于GAN就是典型的深度学习时代的逻辑，你不是不清楚这个n维向量与猫的图片之间的关系嘛，没关系，我直接拟合出来猫的图片对于n维向量的分布，通过对抗学习的方式获得较好的模型效果，这个方法虽然很暴力，但是却是有效的。(<strong>暴力求解</strong>)</p>
<p>VAE则不同，他通过说我希望生成一张新的猫脸，那么这个<strong>n维向量代表的就是n个决定最终猫脸模样的隐形因素</strong>。对于每个因素，都对应产生一种分布，从这些分布关系中进行采样，那么我就可以通过一个深度网络恢复出最终的猫脸。VAE相比较于GAN它的效果往往会略微模糊一点，但是也不失为一种良好的解决方案。并且相对于GAN的暴力求解，VAE的建模思路无疑要复杂的多，它更能体现理科思维的艺术感。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40955254/article/details/82315224">https://blog.csdn.net/weixin_40955254/article/details/82315224</a>  — Good</p>
<p>VAE是<strong>直接计算</strong>生成图片和原始图片的<strong>均方误差</strong></p>
<p>GAN 对抗来学习</p>
<p><a target="_blank" rel="noopener" href="https://antkillerfarm.github.io/gan%20&%20vae/2019/05/05/VAE_3.html">https://antkillerfarm.github.io/gan%20&amp;%20vae/2019/05/05/VAE_3.html</a></p>
<p><img src="/2020/01/07/Paper/Paper-CV-GAN/VAE_GAN.jpg" alt="img"></p>
<p>不像标准自编码器那样产生实数值向量，VAE的编码器会产生两个向量:一个是均值向量，一个是标准差向量。</p>
<img src="/2020/01/07/Paper/Paper-CV-GAN/798706-20161222110521495-412598726.png" alt="img" style="zoom:27%;">

<p>可以通过编码解码的步骤，直接比较重建图片和原始图片的差异，但是GAN做不到。</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/huangshiyu13/p/6209016.html">https://www.cnblogs.com/huangshiyu13/p/6209016.html</a></p>
<h1 id="GAN生成模型"><a href="#GAN生成模型" class="headerlink" title="GAN生成模型"></a>GAN生成模型</h1><p>Generative Adversarial Nets</p>
<p><img src="/2020/01/07/Paper/Paper-CV-GAN/image-20201012154819284.png" alt="image-20201012154819284"></p>
<p><img src="/2020/01/07/Paper/Paper-CV-GAN/image-20201012154844963.png" alt="image-20201012154844963"></p>
<p>从判别器 D 的角度看，它希望自己能尽可能区分真实样本和虚假样本，因此希望 D(x) 尽可能大，D(G(z)) 尽可能小， 即 V(D,G)尽可能大。从生成器 G 的角度看，它希望自己尽可能骗过 D，也就是希望 D(G(z)) 尽可能大，即 V(D,G) 尽可能小。两个模型相对抗，最后达到全局最优。<br>从数据分布来说，就是开始的噪声noise，在G不断修正后，产生的分布，和目标数据分布达到一致：</p>
<p><img src="/2020/01/07/Paper/Paper-CV-GAN/image-20201012154857364.png" alt="image-20201012154857364"></p>
<h2 id="cGAN（Conditional-GAN）"><a href="#cGAN（Conditional-GAN）" class="headerlink" title="cGAN（Conditional GAN）"></a>cGAN（Conditional GAN）</h2><ul>
<li>MG-VTON 采用了此技术</li>
</ul>
<h2 id="StyleGAN"><a href="#StyleGAN" class="headerlink" title="StyleGAN"></a>StyleGAN</h2><p>A Style-Based Generator Architecture for Generative Adversarial Networks</p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVlabs/stylegan">https://github.com/NVlabs/stylegan</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04948">https://arxiv.org/abs/1812.04948</a></p>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p><img src="/2020/01/07/Paper/Paper-CV-GAN/1578381870941.png" alt="1578381870941"></p>
<h3 id="Custom-Date"><a href="#Custom-Date" class="headerlink" title="Custom Date"></a>Custom Date</h3><ul>
<li><p>准备好数据集</p>
</li>
<li><p>把数据集储存为多重分辨率的Tfrecords</p>
</li>
<li><p>数据集表示为一个目录，里面的每张图像都有多种不同的分辨率，用于高效的streaming。每个分辨率都有一个自己的*.tfrecords文件。数据有标注的话，也是用一个分开的文件来储存的。</p>
</li>
<li><pre><code>1&gt; python dataset_tool.py create_lsun datasets/lsun-bedroom-full ~/lsun/bedroom_lmdb --resolution 256

2&gt; python dataset_tool.py create_lsun_wide datasets/lsun-car -512x384 ~/lsun/car_lmdb --width 512--height 384

3&gt; python dataset_tool.py create_lsun datasets/lsun-cat-full ~/lsun/cat_lmdb --resolution 256

4&gt; python dataset_tool.py create_cifar10 datasets/cifar10 ~/cifar10

# 自定义
5&gt; python dataset_tool.py create_from_images datasets/custom-dataset ~/custom-images
</code></pre>
</li>
</ul>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>官方提供的训练过程分四步：</p>
<blockquote>
<p>1.编辑train.py，通过取消注释或者修改某些行，来指定数据集和训练配置；</p>
<p>2.用train.py来运行训练脚本；</p>
<p>3.结果会写在一个新目录里，叫results&#x2F; - ；</p>
<p>4.训练直至完成，几天时间可能是要的。</p>
</blockquote>
<h2 id="StyleGAN2"><a href="#StyleGAN2" class="headerlink" title="StyleGAN2"></a>StyleGAN2</h2><p>Analyzing and Improving the Image Quality of StyleGAN</p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVlabs/stylegan2">https://github.com/NVlabs/stylegan2</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1912.04958">https://arxiv.org/abs/1912.04958</a></p>
<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH#scrollTo=4_s8h-ilzHQc">StyleGAN2 Google Colab Example</a></p>
<h3 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h3><p><img src="/2020/01/07/Paper/Paper-CV-GAN/1578382642371.png" alt="1578382642371"></p>
<h3 id><a href="#" class="headerlink" title></a></h3><h2 id="数据集-origin"><a href="#数据集-origin" class="headerlink" title="数据集(origin)"></a>数据集(origin)</h2><p><a target="_blank" rel="noopener" href="https://github.com/NVlabs/ffhq-dataset">https://github.com/NVlabs/ffhq-dataset</a></p>
<table>
<thead>
<tr>
<th>Path</th>
<th>Size</th>
<th>Files</th>
<th>Format</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1u2xu7bSrWxrbUxk-dT-UvEJq8IjdmNTP">ffhq-dataset</a></td>
<td>2.56 TB</td>
<td>210,014</td>
<td></td>
<td>Main folder</td>
</tr>
<tr>
<td>&boxvr;&nbsp;   <a target="_blank" rel="noopener" href="https://drive.google.com/open?id=16N0RV4fHI6joBuKbQAoG34V_cQk7vxSA">ffhq-dataset-v2.json</a></td>
<td>255 MB</td>
<td>1</td>
<td>JSON</td>
<td>Metadata including copyright info, URLs, etc.</td>
</tr>
<tr>
<td>├  <a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1tZUcXDBeOibC6jcMCtgRRz67pzrAHeHL">images1024x1024</a></td>
<td>89.1 GB</td>
<td>70,000</td>
<td>PNG</td>
<td>Aligned and cropped images at 1024×1024 (同in-the-wild-image, 大小统一格式化)</td>
</tr>
<tr>
<td>├  <a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1tg-Ur7d4vk1T8Bn0pPpUSQPxlPGBlGfv">thumbnails128x128</a></td>
<td>1.95 GB</td>
<td>70,000</td>
<td>PNG</td>
<td>Thumbnails at 128×128</td>
</tr>
<tr>
<td>├  <a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1ZX7QOy6LZuTLTnsOtQk-kmKq2-69l5hu">in-the-wild-images</a></td>
<td>955 GB</td>
<td>70,000</td>
<td>PNG</td>
<td>Original images from Flickr（Human top）</td>
</tr>
<tr>
<td>├  <a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1LTBpJ0W_WLjqza3zdayligS8Dh1V1gA6">tfrecords</a></td>
<td>273 GB</td>
<td>9</td>
<td>tfrecords</td>
<td>Multi-resolution data for <a target="_blank" rel="noopener" href="https://github.com/NVlabs/stylegan">StyleGAN</a> and <a target="_blank" rel="noopener" href="https://github.com/NVlabs/stylegan2">StyleGAN2</a></td>
</tr>
<tr>
<td>└  <a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1WocxvZ4GEZ1DI8dOz30aSj2zT6pkATYS">zips</a></td>
<td>1.28 TB</td>
<td>4</td>
<td>ZIP</td>
<td>Contents of each folder as a ZIP archive.</td>
</tr>
</tbody></table>
<h4 id="数据集合（baiduYUN）："><a href="#数据集合（baiduYUN）：" class="headerlink" title="数据集合（baiduYUN）："></a>数据集合（baiduYUN）：</h4><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>images1024x1024</td>
<td></td>
<td>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1Tdu2G2E8PLKsLnICUi13EQ">https://pan.baidu.com/s/1Tdu2G2E8PLKsLnICUi13EQ</a><br>提取码：jtjv</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>参考资料： <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37867534/article/details/90404660#ffhqdatasetimages1024x1024_4">ffhq-dataset（images1024x1024）</a></p>
<h2 id="相关Blog"><a href="#相关Blog" class="headerlink" title="相关Blog"></a>相关Blog</h2><h3 id="1、零成本体验StyleGAN2："><a href="#1、零成本体验StyleGAN2：" class="headerlink" title="1、零成本体验StyleGAN2："></a>1、零成本体验StyleGAN2：</h3><p>style gan论文翻译</p>
<p>Colab代码直接使用，细节逼真难以分辨 <a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2019-12-21-3">https://www.jiqizhixin.com/articles/2019-12-21-3</a></p>
<h3 id="2、StyleGAN效果图"><a href="#2、StyleGAN效果图" class="headerlink" title="2、StyleGAN效果图"></a>2、StyleGAN效果图</h3><p>用StyleGAN风格迁移模型生成人脸 <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/c728a7cc1a6b">https://www.jianshu.com/p/c728a7cc1a6b</a></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
<td><img src="/2020/01/07/Paper/Paper-CV-GAN/1578475143653.png" width="400px"></td>
</tr>
<tr>
<td></td>
<td></td>
<td><img src="/2020/01/07/Paper/Paper-CV-GAN/1578475162181.png" width="400px"></td>
</tr>
<tr>
<td></td>
<td></td>
<td><img src="/2020/01/07/Paper/Paper-CV-GAN/1578475175948.png" width="400"></td>
</tr>
</tbody></table>
<h3 id="3、StyleGAN-人脸生成器（网红脸）"><a href="#3、StyleGAN-人脸生成器（网红脸）" class="headerlink" title="3、StyleGAN 人脸生成器（网红脸）"></a>3、StyleGAN 人脸生成器（网红脸）</h3><p><a target="_blank" rel="noopener" href="http://www.gwylab.com/download.html">http://www.gwylab.com/download.html</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/T9i7Pr054YylB3SI0k_88A">https://mp.weixin.qq.com/s/T9i7Pr054YylB3SI0k_88A</a> 超模脸、网红脸、萌娃脸…换头像不重样？我开源了5款人脸生成器</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2020/01/03/CV_3D/CV-3D-BuildModel-CMPL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/03/CV_3D/CV-3D-BuildModel-CMPL/" class="post-title-link" itemprop="url">CV_3D_BuildModel_SMPL</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-03 17:32:43" itemprop="dateCreated datePublished" datetime="2020-01-03T17:32:43+00:00">2020-01-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-3D/" itemprop="url" rel="index"><span itemprop="name">CV_3D</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-3D/3D-Body/" itemprop="url" rel="index"><span itemprop="name">3D Body</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h2 id="人体3D建模"><a href="#人体3D建模" class="headerlink" title="人体3D建模:"></a>人体3D建模:</h2><ul>
<li>均是蒙皮模型，没有纹理</li>
</ul>
<h3 id="SMPL-2015"><a href="#SMPL-2015" class="headerlink" title="SMPL 2015"></a>SMPL 2015</h3><p>《A Skinned Multi-Person Linear Model》</p>
<p>Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J.</p>
<ul>
<li>参数人体建模</li>
</ul>
<h4 id="Official"><a href="#Official" class="headerlink" title="Official"></a>Official</h4><p><a target="_blank" rel="noopener" href="https://smpl.is.tue.mpg.de/">https://smpl.is.tue.mpg.de/</a></p>
<h4 id="SMPL"><a href="#SMPL" class="headerlink" title="SMPL"></a>SMPL</h4><p><a target="_blank" rel="noopener" href="https://github.com/CalciferZh/SMPL">https://github.com/CalciferZh/SMPL</a></p>
<ul>
<li>Numpy, TF and PyTorch implementation of human body SMPL model and infant body SMIL model.</li>
</ul>
<p>参数化的人体3D模型生成工具</p>
<h3 id="SMPLify-2016"><a href="#SMPLify-2016" class="headerlink" title="SMPLify 2016"></a>SMPLify 2016</h3><p>论文：《Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image》</p>
<p>官网：<a target="_blank" rel="noopener" href="http://smplify.is.tue.mpg.de/">http://smplify.is.tue.mpg.de/</a></p>
<p>github: <a target="_blank" rel="noopener" href="https://github.com/vchoutas/smplify-x">https://github.com/vchoutas/smplify-x</a></p>
<p>论文解读: <a target="_blank" rel="noopener" href="https://eros-l.github.io/weekly/2018/11/28/weekly5/">https://eros-l.github.io/weekly/2018/11/28/weekly5/</a></p>
<ul>
<li>根据单张照片生成3D模型</li>
</ul>
<h4 id="项目实践："><a href="#项目实践：" class="headerlink" title="项目实践："></a>项目实践：</h4><p>官网数据：</p>
<p>smplify_code_v2.zip</p>
<p>lsp_results.tar.gz</p>
<p>human_eva_results.tar.gz</p>
<p>h36M_results_wtout_plytar.gz</p>
<p>常见问题：</p>
<p>—–自定义自己的数据集</p>
<ul>
<li>使用经过LSP训练过的关节检测器CPM&#x2F;DeepCut</li>
<li>fit_3d.py 的<code>run_sigle_fit</code></li>
</ul>
<p>Ubuntu下python3环境：</p>
<ol>
<li>下载lsp数据集。</li>
<li>DeepCut 提取lsp数据集2D Pose特征点 （DeepCut 提取的 lsp dataset 的二维特征点）</li>
<li>SMPLify fit_3d.py 转换人体3D Pose的模型截图jpg和pkl格式。</li>
<li>pkl 文件来生成对应的 obj 文件或者其他格式的 mesh（未完成）</li>
</ol>
<p>参考文档：</p>
<p>​    1.<a target="_blank" rel="noopener" href="https://eros-l.github.io/weekly/2018/11/28/weekly5/">学习报告(week 5)</a></p>
<p>​    2.<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_36595565/article/details/88929575">（week 2）【经验教训】如何将CPM和SMPL的输入替换成任意图片并得到人体三维模型</a></p>
<h3 id="SMPL-H（mano）-2017"><a href="#SMPL-H（mano）-2017" class="headerlink" title="SMPL-H（mano） 2017"></a>SMPL-H（mano） 2017</h3><p>官网：<a target="_blank" rel="noopener" href="https://mano.is.tue.mpg.de/">https://mano.is.tue.mpg.de/</a></p>
<p>论文：《Embodied Hands: Modeling and Capturing Hands and Bodies Together》 2017</p>
<p>Javier Romero*, Dimitrios Tzionas* and Michael J Black</p>
<p><em>SIGGRAPH ASIA 2017, BANGKOK, THAILAND</em></p>
<ul>
<li>增加了对手的部分的细节的还原</li>
</ul>
<h3 id="SMPLX-2019"><a href="#SMPLX-2019" class="headerlink" title="SMPLX 2019"></a>SMPLX 2019</h3><p>官网：<a target="_blank" rel="noopener" href="https://smpl-x.is.tue.mpg.de/">https://smpl-x.is.tue.mpg.de/</a></p>
<p>github: <a target="_blank" rel="noopener" href="https://github.com/vchoutas/smplx">https://github.com/vchoutas/smplx</a></p>
<p>《Expressive Body Capture: 3D Hands, Face, and Body from a Single Image》</p>
<p><a target="_blank" rel="noopener" href="https://ps.is.tuebingen.mpg.de/person/gpavlakos">G. Pavlakos*</a>, <a target="_blank" rel="noopener" href="https://ps.is.tuebingen.mpg.de/person/vchoutas">V. Choutas*</a>, <a target="_blank" rel="noopener" href="https://ps.is.tuebingen.mpg.de/person/nghorbani">N. Ghorbani</a>, <a target="_blank" rel="noopener" href="https://ps.is.tuebingen.mpg.de/person/tbolkart">T. Bolkart</a>, <a target="_blank" rel="noopener" href="https://ps.is.tuebingen.mpg.de/person/aosman">A. A. A. Osman</a>, <a target="_blank" rel="noopener" href="https://ps.is.tuebingen.mpg.de/person/dtzionas">D. Tzionas</a> and <a target="_blank" rel="noopener" href="https://ps.is.tuebingen.mpg.de/person/black">M. J. Black</a></p>
<p>CVPR 2019</p>
<ul>
<li><p>增加了对手，脸部，身体的细节的完善</p>
</li>
<li><p>pytorch实现，摈弃了chumpy</p>
</li>
</ul>
<h4 id="SMPLify-X"><a href="#SMPLify-X" class="headerlink" title="SMPLify-X"></a>SMPLify-X</h4><p><a target="_blank" rel="noopener" href="https://github.com/vchoutas/smplify-x">https://github.com/vchoutas/smplify-x</a></p>
<p>从单张图片的3D人体复原</p>
<h3 id="AMASS-dataset"><a href="#AMASS-dataset" class="headerlink" title="AMASS dataset"></a>AMASS dataset</h3><p><a target="_blank" rel="noopener" href="https://amass.is.tuebingen.mpg.de/">https://amass.is.tuebingen.mpg.de/</a></p>
<p>AMASS is a large database of human motion unifying different optical marker-based motion capture datasets by representing them within a common framework and parameterization. AMASS is readily useful for animation, visualization, and generating training data for deep learning.</p>
<p>AMASS是一个大型的人类运动数据库，通过在共同的框架和参数化中表示不同的基于光学标记的运动捕获数据集来统一这些数据。AMASS对于动画，可视化以及生成用于深度学习的训练数据非常有用。</p>
<h2 id="SMPL模型源码-项目介绍"><a href="#SMPL模型源码-项目介绍" class="headerlink" title="SMPL模型源码&#x2F;项目介绍"></a><a target="_blank" rel="noopener" href="https://52zju.cn/?p=471">SMPL模型源码&#x2F;项目介绍</a></h2><p>SMPL源码下载：<a target="_blank" rel="noopener" href="http://smpl.is.tue.mpg.de/downloads">http://smpl.is.tue.mpg.de/downloads</a>   需要注册</p>
<p>本文主要讨论SMPL源码核心代码。需要储备的知识还有：三维重建基础知识，以及<a target="_blank" rel="noopener" href="http://52zju.cn/2018/11/13/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA2-%E6%B8%B2%E6%9F%93%E5%B7%A5%E5%85%B7opendr%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E9%83%A8%E5%88%86%E9%98%85%E8%AF%BB-2/"> chumpy阅读</a>以及<a target="_blank" rel="noopener" href="http://52zju.cn/2018/11/13/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA2-%E6%B8%B2%E6%9F%93%E5%B7%A5%E5%85%B7opendr%E8%AE%BA%E6%96%87%E5%92%8C%E6%BA%90%E7%A0%81%E9%83%A8%E5%88%86%E9%98%85%E8%AF%BB-2/">opendr阅读</a>，<a target="_blank" rel="noopener" href="https://52zju.cn/?p=474">LBS DQS</a>等</p>
<p>SMPL文件主要包括 vert.py serialization.py lbs.py。下面将逐一说说明。</p>
<p>0、关节位置</p>
<p>SMPL模型关节点名称</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">self.j_names = &#123;</span><br><span class="line">0: ‘Pelvis’,</span><br><span class="line">1: ‘L_Hip’,</span><br><span class="line">2: ‘R_Hip’,</span><br><span class="line">3:  ‘Spine1’,</span><br><span class="line">4: ‘L_Knee’,</span><br><span class="line">5:  ‘R_Knee’,</span><br><span class="line">6:  ‘Spine2’,</span><br><span class="line">7:  ‘L_Ankle’,</span><br><span class="line">8:  ‘R_Ankle’,</span><br><span class="line">9:  ‘Spine3’,</span><br><span class="line">10: ‘L_Foot’,</span><br><span class="line">11: ‘R_Foot’,</span><br><span class="line">12: ‘Neck’,</span><br><span class="line">13: ‘L_Collar’,</span><br><span class="line">14: ‘R_Collar’,</span><br><span class="line">15: ‘Head’,</span><br><span class="line">16: ‘L_Shoulder’,</span><br><span class="line">17: ‘R_Shoulder’,</span><br><span class="line">18: ‘L_Elbow’,</span><br><span class="line">19: ‘R_Elbow’,</span><br><span class="line">20: ‘L_Wrist’,</span><br><span class="line">21: ‘R_Wrist’,</span><br><span class="line">22: ‘L_Hand’,</span><br><span class="line">23: ‘R_Hand’, &#125;</span><br></pre></td></tr></table></figure>

<p>关节树图形：</p>
<p><img src="/2020/01/03/CV_3D/CV-3D-BuildModel-CMPL/5e0697acaa6695d3b54f35923fe644df.png" alt="img"></p>
<h3 id="1、posemapper-py"><a href="#1、posemapper-py" class="headerlink" title="1、posemapper.py"></a>1、posemapper.py</h3><p>主要是实现关节角度到姿势混合形状的映射。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> chumpy <span class="keyword">as</span> ch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Rodrigues</span>(ch.Ch):  <span class="comment">#创建函数类</span></span><br><span class="line">    dterms = <span class="string">&#x27;rt&#x27;</span>  <span class="comment">#可微变量旋度</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_r</span>(<span class="params">self</span>):  <span class="comment">#计算旋转矩阵</span></span><br><span class="line">        <span class="keyword">return</span> cv2.Rodrigues(<span class="variable language_">self</span>.rt.r)[<span class="number">0</span>] </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_dr_wrt</span>(<span class="params">self, wrt</span>): <span class="comment"># 计算旋度</span></span><br><span class="line">        <span class="keyword">if</span> wrt <span class="keyword">is</span> <span class="variable language_">self</span>.rt:</span><br><span class="line">            <span class="keyword">return</span> cv2.Rodrigues(<span class="variable language_">self</span>.rt.r)[<span class="number">1</span>].T <span class="comment">#返回旋度的转置</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lrotmin</span>(<span class="params">p</span>): </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(p, np.ndarray): <span class="comment"># 判断p的类型是不是np，基本上都不是，跳过</span></span><br><span class="line">        p = p.ravel()[<span class="number">3</span>:] <span class="comment"># 转化成3*n矩阵</span></span><br><span class="line">        <span class="keyword">return</span> np.concatenate([(cv2.Rodrigues(np.array(pp))[<span class="number">0</span>]-np.eye(<span class="number">3</span>)).ravel() <span class="keyword">for</span> pp <span class="keyword">in</span> p.reshape((-<span class="number">1</span>,<span class="number">3</span>))]).ravel()        </span><br><span class="line">    <span class="keyword">if</span> p.ndim != <span class="number">2</span> <span class="keyword">or</span> p.shape[<span class="number">1</span>] != <span class="number">3</span>: <span class="comment"># 判断姿势的维度，如果不是2维或第二维shape不为3</span></span><br><span class="line">        p = p.reshape((-<span class="number">1</span>,<span class="number">3</span>)) <span class="comment"># 将pose转换成（24,3）的形式</span></span><br><span class="line">    p = p[<span class="number">1</span>:]  <span class="comment"># 去掉root出，提取23个关节处</span></span><br><span class="line">    <span class="keyword">return</span> ch.concatenate([(Rodrigues(pp)-ch.eye(<span class="number">3</span>)).ravel() <span class="keyword">for</span> pp <span class="keyword">in</span> p]).ravel()  <span class="comment"># 将旋转向量转化成旋转矩阵，并拼接在一起</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">posemap</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">if</span> s == <span class="string">&#x27;lrotmin&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> lrotmin</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">&#x27;Unknown posemapping: %s&#x27;</span> % (<span class="built_in">str</span>(s),))</span><br></pre></td></tr></table></figure>

<h3 id="2、LBS-py"><a href="#2、LBS-py" class="headerlink" title="2、LBS.py"></a>2、LBS.py</h3><p>就是实现LBS</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> posemapper <span class="keyword">import</span> posemap</span><br><span class="line"><span class="keyword">import</span> chumpy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算模型的全局刚体变换，包括旋度和平移量。实现论文里的公式4</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">global_rigid_transformation</span>(<span class="params">pose, J, kintree_table, xp</span>):</span><br><span class="line">    results = &#123;&#125;</span><br><span class="line">    <span class="comment"># 看了后面parents 终于知道这个kintree_table是干啥的了：关节树，可以理解为各个关节相互依赖的关系。</span></span><br><span class="line">    pose = pose.reshape((-<span class="number">1</span>,<span class="number">3</span>)) <span class="comment">#静止的姿势参数：shape=(24,3) 可能都是0向量</span></span><br><span class="line">    <span class="comment"># 下面两行主要是变换列表，并剔除[0,0]坐标的异常值（可以认为是root的方向异常）。kintree_table的shape=（2,24）</span></span><br><span class="line">    id_to_col = &#123;kintree_table[<span class="number">1</span>,i] : i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(kintree_table.shape[<span class="number">1</span>])&#125;</span><br><span class="line">    parent = &#123;i : id_to_col[kintree_table[<span class="number">0</span>,i]] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, kintree_table.shape[<span class="number">1</span>])&#125;</span><br><span class="line">    <span class="comment">#如果xp是chumpy类型</span></span><br><span class="line">    <span class="keyword">if</span> xp == chumpy:</span><br><span class="line">        <span class="keyword">from</span> posemapper <span class="keyword">import</span> Rodrigues <span class="comment"># 则引入posemapper的函数 </span></span><br><span class="line">        rodrigues = <span class="keyword">lambda</span> x : Rodrigues(x) <span class="comment"># 创建匿名类，ch对象</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">import</span> cv2</span><br><span class="line">        rodrigues = <span class="keyword">lambda</span> x : cv2.Rodrigues(x)[<span class="number">0</span>]  <span class="comment">#否则的话使用opencv的函数</span></span><br><span class="line">    <span class="comment"># stack纵向拼接，改变的是行数，类似于concat。xp在这里就相当于chumpy类</span></span><br><span class="line">    with_zeros = <span class="keyword">lambda</span> x : xp.vstack((x, xp.array([[<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>]]))) </span><br><span class="line">    <span class="comment"># 先确定根关节的结果：目前还不知道这个result有什么用。</span></span><br><span class="line">    <span class="comment"># hstack横向拼接，改变的是列数。输入的是pose的root节点和关节的root节点，最后输出是主对角线为1的4*4矩阵 </span></span><br><span class="line">    <span class="comment"># pose[0] 表示静止姿势的根关节位置</span></span><br><span class="line">    results[<span class="number">0</span>] = with_zeros(xp.hstack((rodrigues(pose[<span class="number">0</span>,:]), J[<span class="number">0</span>,:].reshape((<span class="number">3</span>,<span class="number">1</span>))))) </span><br><span class="line">    <span class="comment"># 利用关节的依赖关系求得其他关节的result    </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, kintree_table.shape[<span class="number">1</span>]): <span class="comment"># i从1到23，代表每个关节的编号</span></span><br><span class="line">        results[i] = results[parent[i]].dot(with_zeros(xp.hstack((  <span class="comment"># 父关节的轴角乘以关节相对旋度等于当前关节的轴角</span></span><br><span class="line">            rodrigues(pose[i,:]),</span><br><span class="line">            ((J[i,:] - J[parent[i],:]).reshape((<span class="number">3</span>,<span class="number">1</span>)))  <span class="comment"># 计算关节的相对位置</span></span><br><span class="line">            ))))</span><br><span class="line"></span><br><span class="line">    pack = <span class="keyword">lambda</span> x : xp.hstack([np.zeros((<span class="number">4</span>, <span class="number">3</span>)), x.reshape((<span class="number">4</span>,<span class="number">1</span>))])</span><br><span class="line"></span><br><span class="line">    results = [results[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">sorted</span>(results.keys())]</span><br><span class="line">    results_global = results</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">        results2 = [results[i] - (pack(</span><br><span class="line">            results[i].dot(xp.concatenate( ( (J[i,:]), <span class="number">0</span> ) )))</span><br><span class="line">            ) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(results))]</span><br><span class="line">        results = results2</span><br><span class="line">    result = xp.dstack(results)</span><br><span class="line">    <span class="keyword">return</span> result, results_global</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每个顶点的关节影响度的混合 </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">verts_core</span>(<span class="params">pose, v, J, weights, kintree_table, want_Jtr=<span class="literal">False</span>, xp=chumpy</span>):</span><br><span class="line">    A, A_global = global_rigid_transformation(pose, J, kintree_table, xp) <span class="comment">#调用函数，生成全局和局部的旋转矩阵。A.shape=[4,4,24] A_global.shape=[24,4,4]</span></span><br><span class="line">    T = A.dot(weights.T)  <span class="comment"># weight.shape=[6890,24],生成结果的shape=[4,4,6890]</span></span><br><span class="line"></span><br><span class="line">    rest_shape_h = xp.vstack((v.T, np.ones((<span class="number">1</span>, v.shape[<span class="number">0</span>]))))  <span class="comment"># 静止姿势的shape</span></span><br><span class="line"></span><br><span class="line">    v =(T[:,<span class="number">0</span>,:] * rest_shape_h[<span class="number">0</span>, :].reshape((<span class="number">1</span>, -<span class="number">1</span>)) +</span><br><span class="line">        T[:,<span class="number">1</span>,:] * rest_shape_h[<span class="number">1</span>, :].reshape((<span class="number">1</span>, -<span class="number">1</span>)) +</span><br><span class="line">        T[:,<span class="number">2</span>,:] * rest_shape_h[<span class="number">2</span>, :].reshape((<span class="number">1</span>, -<span class="number">1</span>)) +</span><br><span class="line">        T[:,<span class="number">3</span>,:] * rest_shape_h[<span class="number">3</span>, :].reshape((<span class="number">1</span>, -<span class="number">1</span>))).T <span class="comment"># 应用公式2</span></span><br><span class="line"></span><br><span class="line">    v = v[:,:<span class="number">3</span>] </span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> want_Jtr:</span><br><span class="line">        <span class="keyword">return</span> v</span><br><span class="line">    Jtr = xp.vstack([g[:<span class="number">3</span>,<span class="number">3</span>] <span class="keyword">for</span> g <span class="keyword">in</span> A_global])</span><br><span class="line">    <span class="keyword">return</span> (v, Jtr)</span><br></pre></td></tr></table></figure>

<h3 id="3、vert-py"><a href="#3、vert-py" class="headerlink" title="3、vert.py"></a>3、vert.py</h3><p>verts_decorated函数没有被用到就不过多注释了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> chumpy</span><br><span class="line"><span class="keyword">import</span> lbs</span><br><span class="line"><span class="keyword">from</span> posemapper <span class="keyword">import</span> posemap</span><br><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> chumpy.ch <span class="keyword">import</span> MatVecMult</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ischumpy</span>(<span class="params">x</span>): <span class="keyword">return</span> <span class="built_in">hasattr</span>(x, <span class="string">&#x27;dterms&#x27;</span>) <span class="comment">#如果没有dterm属性就返回chumpy</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># verts_decorated函数：从另一个SMPL模型中参数来创建新的SMPL模型实例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">verts_decorated</span>(<span class="params">trans, pose,</span></span><br><span class="line"><span class="params">    v_template, J, weights, kintree_table, bs_style, f,</span></span><br><span class="line"><span class="params">    bs_type=<span class="literal">None</span>, posedirs=<span class="literal">None</span>, betas=<span class="literal">None</span>, shapedirs=<span class="literal">None</span>, want_Jtr=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment">#各个参数的含义 </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> which <span class="keyword">in</span> [trans, pose, v_template, weights, posedirs, betas, shapedirs]:</span><br><span class="line">        <span class="keyword">if</span> which <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">           <span class="keyword">assert</span> ischumpy(which)  <span class="comment">#判断是否是chumpy类型，如果不是则报错</span></span><br><span class="line"></span><br><span class="line">    v = v_template</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> shapedirs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">       <span class="keyword">if</span> betas <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          betas = chumpy.zeros(shapedirs.shape[-<span class="number">1</span>])</span><br><span class="line">       v_shaped = v + shapedirs.dot(betas)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">       v_shaped = v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> posedirs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">       v_posed = v_shaped + posedirs.dot(posemap(bs_type)(pose))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">       v_posed = v_shaped</span><br><span class="line"></span><br><span class="line">    v = v_posed</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> sp.issparse(J):</span><br><span class="line">       regressor = J</span><br><span class="line">       J_tmpx = MatVecMult(regressor, v_shaped[:,<span class="number">0</span>])</span><br><span class="line">       J_tmpy = MatVecMult(regressor, v_shaped[:,<span class="number">1</span>])</span><br><span class="line">       J_tmpz = MatVecMult(regressor, v_shaped[:,<span class="number">2</span>])</span><br><span class="line">       J = chumpy.vstack((J_tmpx, J_tmpy, J_tmpz)).T</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">       <span class="keyword">assert</span>(ischumpy(J))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(bs_style==<span class="string">&#x27;lbs&#x27;</span>)</span><br><span class="line">    result, Jtr = lbs.verts_core(pose, v, J, weights, kintree_table, want_Jtr=<span class="literal">True</span>, xp=chumpy)</span><br><span class="line"></span><br><span class="line">    tr = trans.reshape((<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">    result = result + tr</span><br><span class="line">    Jtr = Jtr + tr</span><br><span class="line"></span><br><span class="line">    result.trans = trans</span><br><span class="line">    result.f = f</span><br><span class="line">    result.pose = pose</span><br><span class="line">    result.v_template = v_template</span><br><span class="line">    result.J = J</span><br><span class="line">    result.weights = weights</span><br><span class="line">    result.kintree_table = kintree_table</span><br><span class="line">    result.bs_style = bs_style</span><br><span class="line">    result.bs_type =bs_type</span><br><span class="line">    <span class="keyword">if</span> posedirs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">       result.posedirs = posedirs</span><br><span class="line">       result.v_posed = v_posed</span><br><span class="line">    <span class="keyword">if</span> shapedirs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">       result.shapedirs = shapedirs</span><br><span class="line">       result.betas = betas</span><br><span class="line">       result.v_shaped = v_shaped</span><br><span class="line">    <span class="keyword">if</span> want_Jtr:</span><br><span class="line">       result.J_transformed = Jtr</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># verts_core ：overloaded function inherited by lbs.verts_core</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">verts_core</span>(<span class="params">pose, v, J, weights, kintree_table, bs_style, want_Jtr=<span class="literal">False</span>, xp=chumpy</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> xp == chumpy:</span><br><span class="line">       <span class="keyword">assert</span>(<span class="built_in">hasattr</span>(pose, <span class="string">&#x27;dterms&#x27;</span>))</span><br><span class="line">       <span class="keyword">assert</span>(<span class="built_in">hasattr</span>(v, <span class="string">&#x27;dterms&#x27;</span>))</span><br><span class="line">       <span class="keyword">assert</span>(<span class="built_in">hasattr</span>(J, <span class="string">&#x27;dterms&#x27;</span>))</span><br><span class="line">       <span class="keyword">assert</span>(<span class="built_in">hasattr</span>(weights, <span class="string">&#x27;dterms&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(bs_style==<span class="string">&#x27;lbs&#x27;</span>)</span><br><span class="line">    result = lbs.verts_core(pose, v, J, weights, kintree_table, want_Jtr, xp)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<h3 id="4、serialization-py"><a href="#4、serialization-py" class="headerlink" title="4、serialization.py"></a>4、serialization.py</h3><p>SMPL模型的序列化函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">save_model</span>(<span class="params">model, fname</span>): <span class="comment"># 保存模型model到文件名fname中</span></span><br><span class="line">    m0 = model</span><br><span class="line">    <span class="comment"># 将模型参数存入字典。各个模型参数的含义：</span></span><br><span class="line">    <span class="comment"># v_template：顶点模板 J：关节坐标，weight：关节权重，kintree_table：关节树，f:相机参数，bs_type：蒙皮方法LBS还是DQBS</span></span><br><span class="line">    trainer_dict = &#123;<span class="string">&#x27;v_template&#x27;</span>: np.asarray(m0.v_template),<span class="string">&#x27;J&#x27;</span>: np.asarray(m0.J),<span class="string">&#x27;weights&#x27;</span>: np.asarray(m0.weights),<span class="string">&#x27;kintree_table&#x27;</span>: m0.kintree_table,<span class="string">&#x27;f&#x27;</span>: m0.f, <span class="string">&#x27;bs_type&#x27;</span>: m0.bs_type, <span class="string">&#x27;posedirs&#x27;</span>: np.asarray(m0.posedirs)&#125;    </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">&#x27;J_regressor&#x27;</span>):</span><br><span class="line">        trainer_dict[<span class="string">&#x27;J_regressor&#x27;</span>] = m0.J_regressor</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">&#x27;J_regressor_prior&#x27;</span>):</span><br><span class="line">        trainer_dict[<span class="string">&#x27;J_regressor_prior&#x27;</span>] = m0.J_regressor_prior</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">&#x27;weights_prior&#x27;</span>):</span><br><span class="line">        trainer_dict[<span class="string">&#x27;weights_prior&#x27;</span>] = m0.weights_prior</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">&#x27;shapedirs&#x27;</span>):</span><br><span class="line">        trainer_dict[<span class="string">&#x27;shapedirs&#x27;</span>] = m0.shapedirs</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">&#x27;vert_sym_idxs&#x27;</span>):</span><br><span class="line">        trainer_dict[<span class="string">&#x27;vert_sym_idxs&#x27;</span>] = m0.vert_sym_idxs</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(model, <span class="string">&#x27;bs_style&#x27;</span>):</span><br><span class="line">        trainer_dict[<span class="string">&#x27;bs_style&#x27;</span>] = model.bs_style</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        trainer_dict[<span class="string">&#x27;bs_style&#x27;</span>] = <span class="string">&#x27;lbs&#x27;</span></span><br><span class="line">    pickle.dump(trainer_dict, <span class="built_in">open</span>(fname, <span class="string">&#x27;w&#x27;</span>), -<span class="number">1</span>) <span class="comment">#写入文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载model时更改参数名称</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">backwards_compatibility_replacements</span>(<span class="params">dd</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># replacements</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;default_v&#x27;</span> <span class="keyword">in</span> dd:</span><br><span class="line">        dd[<span class="string">&#x27;v_template&#x27;</span>] = dd[<span class="string">&#x27;default_v&#x27;</span>]</span><br><span class="line">        <span class="keyword">del</span> dd[<span class="string">&#x27;default_v&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;template_v&#x27;</span> <span class="keyword">in</span> dd:</span><br><span class="line">        dd[<span class="string">&#x27;v_template&#x27;</span>] = dd[<span class="string">&#x27;template_v&#x27;</span>]</span><br><span class="line">        <span class="keyword">del</span> dd[<span class="string">&#x27;template_v&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;joint_regressor&#x27;</span> <span class="keyword">in</span> dd:</span><br><span class="line">        dd[<span class="string">&#x27;J_regressor&#x27;</span>] = dd[<span class="string">&#x27;joint_regressor&#x27;</span>]</span><br><span class="line">        <span class="keyword">del</span> dd[<span class="string">&#x27;joint_regressor&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;blendshapes&#x27;</span> <span class="keyword">in</span> dd:</span><br><span class="line">        dd[<span class="string">&#x27;posedirs&#x27;</span>] = dd[<span class="string">&#x27;blendshapes&#x27;</span>]</span><br><span class="line">        <span class="keyword">del</span> dd[<span class="string">&#x27;blendshapes&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;J&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> dd:</span><br><span class="line">        dd[<span class="string">&#x27;J&#x27;</span>] = dd[<span class="string">&#x27;joints&#x27;</span>]</span><br><span class="line">        <span class="keyword">del</span> dd[<span class="string">&#x27;joints&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># defaults</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;bs_style&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> dd:</span><br><span class="line">        dd[<span class="string">&#x27;bs_style&#x27;</span>] = <span class="string">&#x27;lbs&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ready_arguments</span>(<span class="params">fname_or_dict</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(fname_or_dict, <span class="built_in">dict</span>):</span><br><span class="line">        dd = pickle.load(<span class="built_in">open</span>(fname_or_dict))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dd = fname_or_dict</span><br><span class="line"></span><br><span class="line">    backwards_compatibility_replacements(dd)</span><br><span class="line"></span><br><span class="line">    want_shapemodel = <span class="string">&#x27;shapedirs&#x27;</span> <span class="keyword">in</span> dd</span><br><span class="line">    nposeparms = dd[<span class="string">&#x27;kintree_table&#x27;</span>].shape[<span class="number">1</span>]*<span class="number">3</span> <span class="comment"># 这些不是pose的变量，是关节树</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;trans&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> dd:</span><br><span class="line">        dd[<span class="string">&#x27;trans&#x27;</span>] = np.zeros(<span class="number">3</span>) <span class="comment"># 定义平移量为0 </span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;pose&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> dd:</span><br><span class="line">        dd[<span class="string">&#x27;pose&#x27;</span>] = np.zeros(nposeparms) <span class="comment"># 定义姿势为 0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;shapedirs&#x27;</span> <span class="keyword">in</span> dd <span class="keyword">and</span> <span class="string">&#x27;betas&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> dd:</span><br><span class="line">        dd[<span class="string">&#x27;betas&#x27;</span>] = np.zeros(dd[<span class="string">&#x27;shapedirs&#x27;</span>].shape[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> [<span class="string">&#x27;v_template&#x27;</span>, <span class="string">&#x27;weights&#x27;</span>, <span class="string">&#x27;posedirs&#x27;</span>, <span class="string">&#x27;pose&#x27;</span>, <span class="string">&#x27;trans&#x27;</span>, <span class="string">&#x27;shapedirs&#x27;</span>, <span class="string">&#x27;betas&#x27;</span>, <span class="string">&#x27;J&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> (s <span class="keyword">in</span> dd) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(dd[s], <span class="string">&#x27;dterms&#x27;</span>):</span><br><span class="line">            dd[s] = ch.array(dd[s])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> want_shapemodel: <span class="comment"># 需要形状模型</span></span><br><span class="line">        dd[<span class="string">&#x27;v_shaped&#x27;</span>] = dd[<span class="string">&#x27;shapedirs&#x27;</span>].dot(dd[<span class="string">&#x27;betas&#x27;</span>])+dd[<span class="string">&#x27;v_template&#x27;</span>]</span><br><span class="line">        v_shaped = dd[<span class="string">&#x27;v_shaped&#x27;</span>]</span><br><span class="line">        J_tmpx = MatVecMult(dd[<span class="string">&#x27;J_regressor&#x27;</span>], v_shaped[:,<span class="number">0</span>])        </span><br><span class="line">        J_tmpy = MatVecMult(dd[<span class="string">&#x27;J_regressor&#x27;</span>], v_shaped[:,<span class="number">1</span>])        </span><br><span class="line">        J_tmpz = MatVecMult(dd[<span class="string">&#x27;J_regressor&#x27;</span>], v_shaped[:,<span class="number">2</span>])        </span><br><span class="line">        dd[<span class="string">&#x27;J&#x27;</span>] = ch.vstack((J_tmpx, J_tmpy, J_tmpz)).T    </span><br><span class="line">        dd[<span class="string">&#x27;v_posed&#x27;</span>] = v_shaped + dd[<span class="string">&#x27;posedirs&#x27;</span>].dot(posemap(dd[<span class="string">&#x27;bs_type&#x27;</span>])(dd[<span class="string">&#x27;pose&#x27;</span>]))</span><br><span class="line">    <span class="keyword">else</span>:    </span><br><span class="line">        dd[<span class="string">&#x27;v_posed&#x27;</span>] = dd[<span class="string">&#x27;v_template&#x27;</span>] + dd[<span class="string">&#x27;posedirs&#x27;</span>].dot(posemap(dd[<span class="string">&#x27;bs_type&#x27;</span>])(dd[<span class="string">&#x27;pose&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_model</span>(<span class="params">fname_or_dict</span>):</span><br><span class="line">    dd = ready_arguments(fname_or_dict)</span><br><span class="line"></span><br><span class="line">    args = &#123;</span><br><span class="line">        <span class="string">&#x27;pose&#x27;</span>: dd[<span class="string">&#x27;pose&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;v&#x27;</span>: dd[<span class="string">&#x27;v_posed&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;J&#x27;</span>: dd[<span class="string">&#x27;J&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;weights&#x27;</span>: dd[<span class="string">&#x27;weights&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;kintree_table&#x27;</span>: dd[<span class="string">&#x27;kintree_table&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;xp&#x27;</span>: ch,</span><br><span class="line">        <span class="string">&#x27;want_Jtr&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">&#x27;bs_style&#x27;</span>: dd[<span class="string">&#x27;bs_style&#x27;</span>]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    result, Jtr = verts_core(**args)</span><br><span class="line">    result = result + dd[<span class="string">&#x27;trans&#x27;</span>].reshape((<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">    result.J_transformed = Jtr + dd[<span class="string">&#x27;trans&#x27;</span>].reshape((<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> dd.items():</span><br><span class="line">        <span class="built_in">setattr</span>(result, k, v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<h2 id="环境配置-SMPL"><a href="#环境配置-SMPL" class="headerlink" title="环境配置(SMPL)"></a>环境配置(SMPL)</h2><p><strong>chumpy安装</strong></p>
<p>pip install chumpy (0.86的chumpy已经支持python3 不需要特别修改代码安装)</p>
<p><strong>python2和python3一些库的名字不同：</strong></p>
<p>例如：no module named cPickle</p>
<p>pickle模块，在ython3中为import pickle，python2中为import cPickle as pickle</p>
<p><strong>opendr 安装</strong></p>
<p><a target="_blank" rel="noopener" href="https://codeload.github.com/polmorenoc/opendr/zip/master">https://codeload.github.com/polmorenoc/opendr/zip/master</a></p>
<ul>
<li>1 python setup.py build</li>
<li>2 python setup.py install</li>
</ul>
<p><strong>OPENGL-python 安装</strong></p>
<p><a target="_blank" rel="noopener" href="https://files.pythonhosted.org/packages/d7/8a/5db9096aa6506e405309c400bd0feb41997689cbba30683479c30dba6355/PyOpenGL-3.1.4.tar.gz">https://files.pythonhosted.org/packages/d7/8a/5db9096aa6506e405309c400bd0feb41997689cbba30683479c30dba6355/PyOpenGL-3.1.4.tar.gz</a></p>
<ul>
<li>1- 解压</li>
<li>2- python setup.py build</li>
<li>3- python setup.py install</li>
</ul>
<h2 id="环境配置Ubuntu（SMPL）"><a href="#环境配置Ubuntu（SMPL）" class="headerlink" title="环境配置Ubuntu（SMPL）"></a>环境配置Ubuntu（SMPL）</h2><p><strong>opendr</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install opendr</span><br></pre></td></tr></table></figure>

<p><strong>Mesa 3D Graphics Library</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.mesa3d.org/osmesa.html">https://www.mesa3d.org/osmesa.html</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libosmesa6-dev</span><br><span class="line">pip3 install opendr</span><br></pre></td></tr></table></figure>

<p>PyOpenGL </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install  PyOpenGL PyOpenGL_accelerate</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install Bottleneck</span><br></pre></td></tr></table></figure>

<h2 id="c参考资料："><a href="#c参考资料：" class="headerlink" title="c参考资料："></a>c参考资料：</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_28660035/article/details/81319055">SMPL模型改用python3+numpy计算</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/6305f1da8f2d">SMPL: A Skinned Multi-Person Linear Model</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/RdxmOi75glQBKMhKD-APNg">人体模型介绍 - SMPL</a></p>
<h2 id="3D相关学术人员及Blog"><a href="#3D相关学术人员及Blog" class="headerlink" title="3D相关学术人员及Blog"></a>3D相关学术人员及Blog</h2><p>浙大博士 <a target="_blank" rel="noopener" href="https://52zju.cn/?paged=2">https://52zju.cn/?paged=2</a> 三维重建</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2020/01/03/CV/CV-Algorithm-SIFT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/03/CV/CV-Algorithm-SIFT/" class="post-title-link" itemprop="url">CV_Algorithm_SIFT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-03 15:51:57" itemprop="dateCreated datePublished" datetime="2020-01-03T15:51:57+00:00">2020-01-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV/Algorithm/" itemprop="url" rel="index"><span itemprop="name">Algorithm</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="SIFT"><a href="#SIFT" class="headerlink" title="SIFT"></a>SIFT</h2><p><a target="_blank" rel="noopener" href="https://github.com/alicevision/popsift">https://github.com/alicevision/popsift</a></p>
<p>SIFT即<strong>尺度不变特征变换</strong>，是用于图像处理领域的一种描述。这种描述具有尺度不变性，可在图像中检测出关键点，是一种<strong>局部特征描述子</strong>。</p>
<h3 id="一、SIFT算法特点："><a href="#一、SIFT算法特点：" class="headerlink" title="一、SIFT算法特点："></a>一、SIFT算法特点：</h3><p>1、具有较好的稳定性和不变性，能够适应旋转、尺度缩放、亮度的变化，能在一定程度上不受视角变化、仿射变换、噪声的干扰。</p>
<p>2、区分性好，能够在海量特征数据库中进行快速准确的区分信息进行匹配</p>
<p>3、多量性，就算只有单个物体，也能产生大量特征向量</p>
<p>4、高速性，能够快速的进行特征向量匹配</p>
<p>5、可扩展性，能够与其它形式的特征向量进行联合</p>
<h3 id="二、SIFT算法实质"><a href="#二、SIFT算法实质" class="headerlink" title="二、SIFT算法实质"></a>二、SIFT算法实质</h3><p>在不同的尺度空间上查找关键点，并计算出关键点的方向。<br><img src="/2020/01/03/CV/CV-Algorithm-SIFT/20190316212036359.png" alt="在这里插入图片描述"></p>
<h3 id="三、SIFT算法实现特征匹配主要有以下三个流程："><a href="#三、SIFT算法实现特征匹配主要有以下三个流程：" class="headerlink" title="三、SIFT算法实现特征匹配主要有以下三个流程："></a>三、SIFT算法实现特征匹配主要有以下三个流程：</h3><p>1、提取关键点：关键点是一些十分突出的不会因光照、尺度、旋转等因素而消失的点，比如角点、边缘点、暗区域的亮点以及亮区域的暗点。此步骤是搜索所有尺度空间上的图像位置。通过高斯微分函数来识别潜在的具有尺度和旋转不变的兴趣点。</p>
<p>2、定位关键点并确定特征方向：在每个候选的位置上，通过一个拟合精细的模型来确定位置和尺度。关键点的选择依据于它们的稳定程度。然后基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向。所有后面的对图像数据的操作都相对于关键点的方向、尺度和位置进行变换，从而提供对于这些变换的不变性。</p>
<p>3、通过各关键点的特征向量，进行两两比较找出相互匹配的若干对特征点，建立景物间的对应关系。</p>
<h3 id="四、尺度空间"><a href="#四、尺度空间" class="headerlink" title="四、尺度空间"></a>四、尺度空间</h3><h4 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h4><p>尺度空间即试图在图像领域中模拟人眼观察物体的概念与方法。例如：观察一颗树，关键在于我们想要观察是树叶子还是整棵树：如果是一整棵树(相当于大尺度情况下观察)，那么就应该去除图像的细节部分。如果是树叶(小尺度情况下观察)，那么就该观察局部细节特征。<br>SIFT算法在构建尺度空间时候采取高斯核函数进行滤波，使原始图像保存最多的细节特征，经过高斯滤波后细节特征逐渐减少来模拟大尺度情况下的特征表示。<br>利用高斯核函数进行滤波的主要原因有两个：<br>（1）高斯核函数是唯一的尺度不变核函数。<br>（2）DoG核函数可以近似为LoG函数，这样可以使特征提取更加简单。同时，David. Lowe作者在论文中提出将原始图像进行2倍上采样后滤波能够保留更多的信息便于后续特征提取与匹配。其实尺度空间图像生成就是当前图像与不同尺度核参数σ进行卷积运算后产生的图像。</p>
<h4 id="2、表示"><a href="#2、表示" class="headerlink" title="2、表示"></a>2、表示</h4><p>L(x, y, σ) ,定义为原始图像 I(x, y)与一个可变尺度的2维高斯函数G(x, y, σ) 卷积运算。</p>
<p>*表示卷积运算，(x,y)代表图像的像素位置。是尺度空间因子，值越小表示图像被平滑的越少，相应的尺度也就越小。大尺度对应于图像的概貌特征，小尺度对应于图像的细节特征。</p>
<h3 id="五、高斯金字塔的构建"><a href="#五、高斯金字塔的构建" class="headerlink" title="五、高斯金字塔的构建"></a>五、高斯金字塔的构建</h3><h4 id="1、概念-1"><a href="#1、概念-1" class="headerlink" title="1、概念"></a>1、概念</h4><p>尺度空间在实现时使用高斯金字塔表示，高斯金字塔的构建分为两步：<br>（1）对图像做高斯平滑；<br>（2）对图像做降采样。</p>
<p>图像的金字塔模型是指将原始图像不断降阶采样，得到一系列大小不一的图像，由大到小，从下到上构成的塔状模型。原图像为金子塔的第一层，每次降采样所得到的新图像为金字塔的一层(每层一张图像)，每个金字塔共n层。为了让尺度体现其连续性，高斯金字塔在简单降采样的基础上加上了高斯滤波。如上图所示，将图像金字塔每层的一张图像使用不同参数做高斯模糊，Octave表示一幅图像可产生的图像组数，Interval表示一组图像包括的图像层数。另外，降采样时，高斯金字塔上一组图像的初始图像(底层图像)是由前一组图像的倒数第三张图像隔点采样得到的。</p>
<h3 id="2、表示-1"><a href="#2、表示-1" class="headerlink" title="2、表示"></a>2、表示</h3><p>高斯图像金字塔共o组、s层，则有</p>
<p>σ：尺度空间坐标；s：sub-level层坐标；σ0：初始尺度；S：每组层数（一般为3~5）<br>组内和组间尺度：</p>
<p>i:金字塔组数；n:每一组的层数</p>
<h3 id="六、DOG空间极值检测"><a href="#六、DOG空间极值检测" class="headerlink" title="六、DOG空间极值检测"></a>六、DOG空间极值检测</h3><h4 id="1、DOG函数"><a href="#1、DOG函数" class="headerlink" title="1、DOG函数"></a>1、DOG函数</h4><h4 id="2、DoG高斯差分金字塔"><a href="#2、DoG高斯差分金字塔" class="headerlink" title="2、DoG高斯差分金字塔"></a>2、DoG高斯差分金字塔</h4><p>（1）对应DOG算子，需构建DOG金字塔。<br>可以通过高斯差分图像看出图像上的像素值变化情况。（如果没有变化，也就没有特征。特征必须是变化尽可能多的点。）DOG图像描绘的是目标的轮廓。</p>
<p>（2）DOG局部极值检测<br>特征点是由DOG空间的局部极值点组成的。为了寻找DoG函数的极值点，每一个像素点要和它所有的相邻点比较，看其是否比它的图像域和尺度域的相邻点大或者小。特征点是由DOG空间的局部极值点组成的。为了寻找DoG函数的极值点，每一个像素点要和它所有的相邻点比较，看其是否比它的图像域和尺度域的相邻点大或者小。如下图，中间的检测点和它同尺度的8个相邻点和上下相邻尺度对应的9×2个点共26个点比较，以确保在尺度空间和二维图像空间都检测到极值点。</p>
<p>（2）去除边缘效应<br>在边缘梯度的方向上主曲率值比较大，而沿着边缘方向则主曲率值较小。候选特征点的DoG函数D(x)的主曲率与2×2Hessian矩阵H的特征值成正比。</p>
<p>其中，是候选点邻域对应位置的差分求得的。<br>H的特征值α和β代表x和y方向的梯度</p>
<p>表示矩阵H对角线元素之和，表示矩阵H的行列式。假设是α较大的特征值，而是β较小的特征值，令，则</p>
<p>该值在两特征值相等时达最小。Lowe论文中建议阈值T为1.2，即<br>时保留关键点，反之剔除</p>
<h3 id="七、关键点方向分配"><a href="#七、关键点方向分配" class="headerlink" title="七、关键点方向分配"></a>七、关键点方向分配</h3><p>1、通过尺度不变性求极值点，需要利用图像的局部特征为给每一个关键点分配一个基准方向，使描述子对图像旋转具有不变性。对于在DOG金字塔中检测出的关键点，采集其所在高斯金字塔图像3σ邻域窗口内像素的梯度和方向分布特征。梯度的模值和方向如下：</p>
<p>2、本算法采用梯度直方图统计法，统计以关键点为原点，一定区域内的图像像素点确定关键点方向。在完成关键点的梯度计算后，使用直方图统计邻域内像素的梯度和方向。梯度直方图将0~360度的方向范围分为36个柱，其中每柱10度。如下图所示，直方图的峰值方向代表了关键点的主方向，方向直方图的峰值则代表了该特征点处邻域梯度的方向，以直方图中最大值作为该关键点的主方向。为了增强匹配的鲁棒性，只保留峰值大于主方向峰值80％的方向作为该关键点的辅方向。</p>
<h3 id="八、关键点描述"><a href="#八、关键点描述" class="headerlink" title="八、关键点描述"></a>八、关键点描述</h3><p>对于每一个关键点，都拥有位置、尺度以及方向三个信息。为每个关键点建立一个描述符，用一组向量将这个关键点描述出来，使其不随各种变化而改变，比如光照变化、视角变化等等。这个描述子不但包括关键点，也包含关键点周围对其有贡献的像素点，并且描述符应该有较高的独特性，以便于提高特征点正确匹配的概率。</p>
<p>Lowe实验结果表明：描述子采用4×4×8＝128维向量表征，综合效果最优（不变性与独特性）。</p>
<h3 id="九、关键点匹配"><a href="#九、关键点匹配" class="headerlink" title="九、关键点匹配"></a>九、关键点匹配</h3><p>1、分别对模板图（参考图，reference image）和实时图（观测图，<br>observation image）建立关键点描述子集合。目标的识别是通过两点集内关键点描述子的比对来完成。具有128维的关键点描述子的相似性度量采用欧式距离。</p>
<p>3、匹配可采取穷举法完成，但所花费的时间太多。所以一般采用kd树的数据结构来完成搜索。搜索的内容是以目标图像的关键点为基准，搜索与目标图像的特征点最邻近的原图像特征点和次邻近的原图像特征点。<br>Kd树如下如所示，是个平衡二叉树</p>
<h3 id="十、总结"><a href="#十、总结" class="headerlink" title="十、总结"></a>十、总结</h3><p>SIFT特征具有稳定性和不变性，在图像处理和计算机视觉领域有着很重要的作用。<br>1、DoG尺度空间的极值检测。<br>2、删除不稳定的极值点。<br>3、确定特征点的主方向<br>4、生成特征点的描述子进行关键点匹配。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>SIFT百科 <a target="_blank" rel="noopener" href="https://baike.baidu.com/item/SIFT/1396275?fr=aladdin">https://baike.baidu.com/item/SIFT/1396275?fr=aladdin</a></p>
<p>SIFT算法详解 <a target="_blank" rel="noopener" href="https://blog.csdn.net/zddblog/article/details/7521424">https://blog.csdn.net/zddblog/article/details/7521424</a></p>
<p>SIFT算法系列之尺度空间 <a target="_blank" rel="noopener" href="https://blog.csdn.net/Small_Munich/article/details/79968229">https://blog.csdn.net/Small_Munich/article/details/79968229</a></p>
<p>SIFT特征详解 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/wangguchangqing/p/4853263.html">https://www.cnblogs.com/wangguchangqing/p/4853263.html</a></p>
<p>SIFT算法原理 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37374643/article/details/88606351">https://blog.csdn.net/qq_37374643/article/details/88606351</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2020/01/03/CV_3D/CV-3D-BuildModel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/03/CV_3D/CV-3D-BuildModel/" class="post-title-link" itemprop="url">CV—CV_3D_BuildModel</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-03 11:47:01" itemprop="dateCreated datePublished" datetime="2020-01-03T11:47:01+00:00">2020-01-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-3D/" itemprop="url" rel="index"><span itemprop="name">CV_3D</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-3D/3D-Body/" itemprop="url" rel="index"><span itemprop="name">3D Body</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h2 id="3D建模"><a href="#3D建模" class="headerlink" title="3D建模"></a>3D建模</h2><p>目前动画、游戏中使用的静态三维数字人体主要通过3D网格模型（英文一般叫mesh），贴上材质贴图来实现。</p>
<p>三维数字人体目前主要的方法有：</p>
<p>1.多视角几何重建（不同角度拍摄多张照片）；</p>
<p>2.使用数字化人体表示模型预测网格</p>
<h2 id="多视角几何重建"><a href="#多视角几何重建" class="headerlink" title="多视角几何重建"></a>多视角几何重建</h2><p>生成工具:</p>
<p><a href="https://link.zhihu.com/?target=https://www.agisoft.com/">PhotoScan</a>（商业软件）</p>
<p><a href="https://link.zhihu.com/?target=https://alicevision.org/">MeshRoom</a>（开源软件）</p>
<h2 id="模型方法预测或生成3D网格"><a href="#模型方法预测或生成3D网格" class="headerlink" title="模型方法预测或生成3D网格"></a>模型方法预测或生成3D网格</h2><p>传统的基于骨架的Skinning方法有LBS, DQS, Implicit Skinning等，这些方法可以理解为产生了一个从<strong>骨架姿势（Posture of the skeleton）<strong>到 <strong>角色模型网格（Mesh of the character）<strong>的映射，输入是</strong>Posture</strong>，输出是</strong>Mesh</strong>，怎么产生的就是使用特定的方法，大多是几何物理方法等。</p>
<p>产生骨架姿势，可以用正向动力学（Forward Kinematics）、反向动力学（Inverse Kinematics）。</p>
<p>产生动画有几种思路，比如用网格（Mesh）内部的骨架（Skeleton）驱动，用Mesh外部的Cage驱动，用一些Handle来控制变形（在面部动画中经常使用，比如blend shape），用一些关键帧（Keyframes）插值得到中间的结果等。</p>
<h3 id="数字化人体表示模型"><a href="#数字化人体表示模型" class="headerlink" title="数字化人体表示模型"></a><strong>数字化人体表示模型</strong></h3><h3 id="SMPL"><a href="#SMPL" class="headerlink" title="SMPL"></a>SMPL</h3><p>(A Skinned Multi-Person Linear Model)</p>
<h3 id="SMPL-H"><a href="#SMPL-H" class="headerlink" title="SMPL-H"></a>SMPL-H</h3><h3 id="SMPL-X"><a href="#SMPL-X" class="headerlink" title="SMPL-X"></a>SMPL-X</h3><p>SMPL-X (SMPL expressive)是针对SMPL模型的优化，增加了面部表情、手势姿态、脚的姿态以及人的性别，改进人体重建的3D细节。</p>
<p>Expressive Body Capture: 3D Hands, Face, and Body from a Single Image</p>
<p><a href="https://link.zhihu.com/?target=https://smpl-x.is.tue.mpg.de/">https://smpl-x.is.tue.mpg.de/smpl-x.is.tue.mpg.de</a></p>
<p>有代码，实测可运行：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/vchoutas/smplx">vchoutas&#x2F;smplxgithub.com<img src="/2020/01/03/CV_3D/CV-3D-BuildModel/v2-adeb01c5732699d3e0c2a26d1649b5fc_ipico.jpg" alt="图标"></a></p>
<h3 id="Frank"><a href="#Frank" class="headerlink" title="Frank"></a>Frank</h3><p>Total Capture 2018 论文</p>
<h3 id="LBS（Linear-Blend-Skinning）"><a href="#LBS（Linear-Blend-Skinning）" class="headerlink" title="LBS（Linear Blend Skinning）"></a>LBS（Linear Blend Skinning）</h3><p>中文名称：线性混合蒙皮</p>
<h3 id="DQBS-Dual-Quaternion-Blend-Skinning"><a href="#DQBS-Dual-Quaternion-Blend-Skinning" class="headerlink" title="DQBS(Dual Quaternion Blend Skinning)"></a>DQBS(Dual Quaternion Blend Skinning)</h3><p>中文名称：双四元数蒙皮</p>
<h3 id="2005-SCAPE-Shape-Completion-and-Animation-of-People"><a href="#2005-SCAPE-Shape-Completion-and-Animation-of-People" class="headerlink" title="[2005] SCAPE(Shape Completion and Animation of People)"></a>[2005] SCAPE(Shape Completion and Animation of People)</h3><p>官网：<a target="_blank" rel="noopener" href="https://ai.stanford.edu/~drago/Projects/scape/scape.html">https://ai.stanford.edu/~drago/Projects/scape/scape.html</a></p>
<p><img src="/2020/01/03/CV_3D/CV-3D-BuildModel/1579226914959.png" alt="1579226914959"></p>
<p>我们介绍了一种数据驱动的方法来构建涵盖对象形状和姿势变化的人体形状模型。该方法基于结合了关节变形和非刚性变形的表示。我们学习了一个<em><strong>姿态变形模型</strong></em>，该<em>模型</em>可以根据关节骨骼的姿态导出非刚性表面变形。我们还学习了基于<em>体形</em>的单独的变异模型。当两个模型都没有出现在训练集中时，可以将我们的两个模型结合起来以产生具有不同姿势的逼真的肌肉变形的3D表面模型。我们展示了如何将模型用于<em>完成形状</em>   -在指定目标形状的一组有限标记的情况下，生成完整的表面网格。我们介绍了形状完成在部分视图完成和运动捕捉动画中的应用。尤其是，我们的方法能够仅通过对单个移动物体进行一次静态扫描就可以构造出具有真实肌肉变形的移动物体的高质量动画表面模型。 </p>
<h3 id="Implicit-Skinning"><a href="#Implicit-Skinning" class="headerlink" title="Implicit Skinning"></a>Implicit Skinning</h3><h2 id="神经网络3D建模"><a href="#神经网络3D建模" class="headerlink" title="神经网络3D建模"></a>神经网络3D建模</h2><p>[TOC]</p>
<table>
<thead>
<tr>
<th>3D 人体建模</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>HumanMesh Recovery</td>
<td>CVPR 2018</td>
<td></td>
</tr>
<tr>
<td>HumanMeshNet</td>
<td>2019</td>
<td></td>
</tr>
<tr>
<td>HMR</td>
<td>ICCV2019</td>
<td></td>
</tr>
<tr>
<td>Chained Representation Cycling</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SPIN(SMPL OPtimization IN the loop)</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="Human-Mesh-Recovery-CVPR2018"><a href="#Human-Mesh-Recovery-CVPR2018" class="headerlink" title="Human Mesh Recovery CVPR2018"></a>Human Mesh Recovery CVPR2018</h3><p>End-to-end Recovery of Human Shape and Pose</p>
<p>Human Mesh Recovery（HMR）, 从单张彩色图片中恢复出3D 人体网格（包括人体形状和关节角度）</p>
<p><img src="/2020/01/03/CV_3D/CV-3D-BuildModel/1579243110376.png" alt="1579243110376"></p>
<p><img src="/2020/01/03/CV_3D/CV-3D-BuildModel/1579243115045.png" alt="1579243115045"></p>
<h3 id="HMR-ICCV2019"><a href="#HMR-ICCV2019" class="headerlink" title="HMR-ICCV2019"></a>HMR-ICCV2019</h3><p><strong>标题：</strong><Human mesh recovery from monocular images via a skeleton-disentangled representation></Human></p>
<p><strong>下载：</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.07172v2.pdf">https://arxiv.org/pdf/1908.07172v2.pdf</a> </p>
<p><strong>Code:</strong> <a target="_blank" rel="noopener" href="https://github.com/Arthur151/DSD-SATN">https://github.com/Arthur151/DSD-SATN</a></p>
<p>**简介：**本文描述了一种从单目图像和单目视频中恢复三维人体网格的端到端方法</p>
<h3 id="HumanMeshNet"><a href="#HumanMeshNet" class="headerlink" title="HumanMeshNet"></a>HumanMeshNet</h3><p>2019 Aug</p>
<p>印度 iiit，国际信息技术研究所</p>
<p>单目图像三维人体建模</p>
<p><img src="/2020/01/03/CV_3D/CV-3D-BuildModel/1579228511503.png" alt="1579228511503"></p>
<p><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/HumanMeshNet%3A-Polygonal-Mesh-Recovery-of-Humans-Venkat-Patel/032d3d6a83e8bb7635037814c879c9ec9a6e7ddb">semantic cholar</a></p>
<p><img src="/2020/01/03/CV_3D/CV-3D-BuildModel/1579227847960.png" alt="1579227847960"></p>
<h3 id="Chained-Representation-Cycling"><a href="#Chained-Representation-Cycling" class="headerlink" title="Chained Representation Cycling"></a>Chained Representation Cycling</h3><p>苏黎世理工学院</p>
<p>一种新的图片转换3D模型的方法</p>
<p>2001.01613 [Chained Representation Cycling]Learning to Estimate 3D Human Pose and Shape by Cycling Between Representations.pdf</p>
<p><img src="/2020/01/03/CV_3D/CV-3D-BuildModel/1579228149537.png" alt="1579228149537"></p>
<h3 id="SPIN"><a href="#SPIN" class="headerlink" title="SPIN"></a>SPIN</h3><p>(SMPL OPtimization IN the loop）</p>
<p><strong>标题：</strong>&lt;Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop&gt; ICCV2019</p>
<p><strong>下载地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.12828v1.pdf">https://arxiv.org/pdf/1909.12828v1.pdf</a></strong></p>
<p><strong>Code：<a target="_blank" rel="noopener" href="https://github.com/nkolot/SPIN">https://github.com/nkolot/SPIN</a></strong></p>
<p>**简介：**把optimization and regression做了一个结合：</p>
<p><img src="file:///C:\Users\admin\AppData\Local\Temp\ksohtml88988\wps1.jpg" alt="img"> </p>
<p><strong>技术流程：</strong></p>
<p><img src="file:///C:\Users\admin\AppData\Local\Temp\ksohtml88988\wps2.jpg" alt="img"> </p>
<h3 id="Neal-body-fitting"><a href="#Neal-body-fitting" class="headerlink" title="Neal body fitting"></a>Neal body fitting</h3><p>基于SMPL</p>
<p><a target="_blank" rel="noopener" href="https://github.com/mohomran/neural_body_fitting">https://github.com/mohomran/neural_body_fitting</a></p>
<p><img src="/2020/01/03/CV_3D/CV-3D-BuildModel/1579252060836.png" alt="1579252060836"></p>
<h2 id="知名机构"><a href="#知名机构" class="headerlink" title="知名机构"></a>知名机构</h2><p>Alicevision  <a target="_blank" rel="noopener" href="https://alicevision.org/#contributing">https://alicevision.org/#contributing</a></p>
<p>欧盟Horizon 2020研究与创新计划的资助</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2010年，[IMAGINE](http://imagine.enpc.fr/)研究团队（[巴黎理工学院](http://www.enpc.fr/en)与[法国](http://www.enpc.fr/en)[科学技术中心](http://international.cstb.fr/)联合研究小组）与Mikros Image围绕Pierre Moulon的论文建立了伙伴关系，由学术方面的Renaud Marlet和Pascal Monasse以及Benoit Maujean负责工业方面。2013年，他们发布了一个名为openMVG（“多视图几何”）的开源SfM管道，为创建[哑光绘画](https://en.wikipedia.org/wiki/Matte_painting)的视觉效果提供了更好的解决方案的基础。</span><br><span class="line"></span><br><span class="line">2009年，CTU的CMP研究团队在Tomas Pajdla的指导下开始了Michal Jancosek的博士学位论文。他们在2012年发布了其MVS管道CMPMVS的Windows二进制文件。</span><br><span class="line"></span><br><span class="line">2009年，INPT，INRIA和Duran Duboi启动了法国ANR项目，以基于自然特征和名为CCTag的新标记设计创建基于模型的相机跟踪解决方案。</span><br><span class="line"></span><br><span class="line">2015年，Simula，INTT和Mikros Image共同参与了欧盟[POPART](http://www.popartproject.eu/)项目，以创建Previz系统。2017年，CTU加入了欧盟项目[LADIO中](http://www.ladioproject.eu/)的团队，创建了一个中央枢纽，可对在[现场](http://www.ladioproject.eu/)生成的所有数据进行结构化访问。</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2020/01/02/Paper/Paper-CV-FPN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/02/Paper/Paper-CV-FPN/" class="post-title-link" itemprop="url">Paper_CV_FPN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-02 11:54:17" itemprop="dateCreated datePublished" datetime="2020-01-02T11:54:17+00:00">2020-01-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:40" itemprop="dateModified" datetime="2025-08-06T08:16:40+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV/" itemprop="url" rel="index"><span itemprop="name">CV</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV/Networks/" itemprop="url" rel="index"><span itemprop="name">Networks</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[toc]</p>
<h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h2><p><strong>Arch:</strong></p>
<p><img src="/2020/01/02/Paper/Paper-CV-FPN/1577958895321.png" alt="1577958895321"></p>
<p><img src="/2020/01/02/Paper/Paper-CV-FPN/1577958921261.png" alt="1577958921261"></p>
<p>lateral connection  &amp; path way</p>
<p><img src="/2020/01/02/Paper/Paper-CV-FPN/1577958949126.png" alt="1577958949126"></p>
<p><strong>ResNet的结构图:</strong></p>
<p><img src="/2020/01/02/Paper/Paper-CV-FPN/20170606223119184.jpg" alt="这里写图片描述"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/26/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><span class="page-number current">27</span><a class="page-number" href="/page/28/">28</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="extend next" rel="next" href="/page/28/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Simon Shi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">322</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">142</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">269</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Simon Shi</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>

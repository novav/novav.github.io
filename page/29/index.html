<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"novav.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Simon Shi的小站">
<meta property="og:url" content="https://novav.github.io/page/29/index.html">
<meta property="og:site_name" content="Simon Shi的小站">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Simon Shi">
<meta property="article:tag" content="AI,Machine Learning, Deep Learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://novav.github.io/page/29/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Simon Shi的小站</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Simon Shi的小站</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">人工智能，机器学习， 强化学习，大模型，自动驾驶</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/11/22/Paper/Paper-CV-PoseEstimation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/22/Paper/Paper-CV-PoseEstimation/" class="post-title-link" itemprop="url">Paper-CV-PoseEstimation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-22 16:04:11" itemprop="dateCreated datePublished" datetime="2019-11-22T16:04:11+00:00">2019-11-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:40" itemprop="dateModified" datetime="2025-08-06T08:16:40+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/" itemprop="url" rel="index"><span itemprop="name">CV_Apply</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/PoseEstimation/" itemprop="url" rel="index"><span itemprop="name">PoseEstimation</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CV-PoseEstimation-Method"><a href="#CV-PoseEstimation-Method" class="headerlink" title="CV PoseEstimation Method"></a>CV PoseEstimation Method</h1><p>[TOC]</p>
<h1 id="Human-Pose-Estimation"><a href="#Human-Pose-Estimation" class="headerlink" title="Human Pose Estimation"></a>Human Pose Estimation</h1><table>
<thead>
<tr>
<th></th>
<th>论文&#x2F;原理</th>
<th>S-单人<br>D-多人</th>
</tr>
</thead>
<tbody><tr>
<td>DensePose</td>
<td>实时人体姿态估计</td>
<td></td>
</tr>
<tr>
<td><strong>单人姿态估计</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DeepPose</td>
<td>最早应用CNN的方法,直接回归关节坐标</td>
<td>S</td>
</tr>
<tr>
<td>Flowing ConvNets</td>
<td>热力图-&gt;关节点</td>
<td>S</td>
</tr>
<tr>
<td>CPM</td>
<td>multi-stage和refine heatmap的思想</td>
<td>S</td>
</tr>
<tr>
<td>stacked hourglass</td>
<td>卷积层、解卷积层.</td>
<td>S</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>多人姿态估计</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DeepCut</td>
<td>CVPR2016 《DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation》</td>
<td>S&#x2F;D</td>
</tr>
<tr>
<td>OpenPose</td>
<td>Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</td>
<td>S&#x2F;D</td>
</tr>
<tr>
<td>PAF</td>
<td>Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</td>
<td></td>
</tr>
<tr>
<td>Mask R-CNN</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>人体姿态跟踪</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>3D人体姿态估计</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/v2-3609c3b8bfef3db55da0ec0c18b260d9_hd.jpg" alt="img"></p>
<h1 id="数据集："><a href="#数据集：" class="headerlink" title="数据集："></a>数据集：</h1><p>单人：常见的数据集有<a href="https://link.zhihu.com/?target=http://human-pose.mpi-inf.mpg.de/">MPII</a>, <a href="https://link.zhihu.com/?target=https://sam.johnson.io/research/lsp.html">LSP</a>, <a href="https://link.zhihu.com/?target=https://bensapp.github.io/flic-dataset.html">FLIC</a>, <a href="https://link.zhihu.com/?target=http://sysu-hcp.net/lip/">LIP</a>。MPII目前可以认为是单人姿态估计中最常用的benchmark， 使用的是PCKh的指标（可以认为预测的关键点与GT标注的关键点经过head size normalize后的距离）</p>
<p>多人：主要有<a href="https://link.zhihu.com/?target=http://cocodataset.org/%23keypoints-2019">COCO</a>, 最近有新出一个数据集<a href="https://link.zhihu.com/?target=https://github.com/Jeff-sjtu/CrowdPose">CrowdPose</a>。</p>
<p>Track：<a href="https://link.zhihu.com/?target=https://posetrack.net/">PoseTrack</a></p>
<p>3D     :  <a href="https://link.zhihu.com/?target=http://vision.imar.ro/human3.6m/description.php">Human3.6M</a>,  <a href="https://link.zhihu.com/?target=http://densepose.org/">DensePose</a></p>
<h1 id="单人姿态估计"><a href="#单人姿态估计" class="headerlink" title="单人姿态估计"></a>单人姿态估计</h1><h2 id="Flowing-ConvNets"><a href="#Flowing-ConvNets" class="headerlink" title="Flowing ConvNets"></a>Flowing ConvNets</h2><p>对于当前帧t及相邻的前后n帧使用全卷积网络为每帧输出一个预测的heatmap（去掉FC层），再用光流信息将这些heatmap扭曲到当前帧t。之后将warped的heatmap合并到另一个卷积层中，权衡来自附近框架的扭曲的heatmap。最后使用集合热图的<strong>最大值</strong>作为关节点。</p>
<h2 id="CPM"><a href="#CPM" class="headerlink" title="CPM"></a>CPM</h2><p>Convolution pose machine</p>
<p>CPM是CMU Yaser Sheikh组的工作，后续非常有名的openpose也是他们的工作。从CPM开始，神经网络已经可以e2e的把feature representation以及关键点的空间位置关系建模进去（隐式的建模），输入一个图片的patch， 输出带spatial信息的tensor，channel的个数一般就是人体关键点的个数（或者是关键点个数加1）。空间大小往往是原图的等比例缩放图。通过在输出的heatmap上面按channel找最大的响应位置(x,y坐标），就可以找到相应关键点的位置。</p>
<p>这种heatmap的方式被广泛使用在人体骨架的问题里面。这个跟人脸landmark有明显的差异，一般人脸landmark会直接使用回归(fully connected layer for regression)出landmark的坐标位置。这边我做一些解释。首先人脸landmark的问题往往相对比较简单，对速度很敏感，所以直接回归相比heatmap来讲速度会更快，另外直接回归往往可以得到sub-pixel的精度，但是heatmap的坐标进度取决于在spatial图片上面的argmax操作，所以精度往往是pixel级别（同时会受下采样的影响）。 但是heatmap的好处在于空间位置信息的保存，这个非常重要。一方面，这个可以保留multi-modal的信息，比如没有很好的context信息的情况下，是很难区分左右手的，所以图片中左右手同时都可能有比较好的响应，这种heatmap的形式便于后续的cascade的进行refinement优化。另外一个方面，人体姿态估计这个问题本身的自由度很大，直接regression的方式对自由度小的问题比如人脸landmark是比较适合的，但是对于自由度大的姿态估计问题整体的建模能力会比较弱。相反，heatmap是比较中间状态的表示，所以信息的保存会更丰富。</p>
<p>后续2D的人体姿态估计方法几乎都是围绕heatmap这种形式来做的（3D姿态估计将会是另外一条路），通过使用神经网络来获得更好的feature representation，同时把关键点的空间位置关系隐式的encode在heatmap中，进行学习。大部分的方法区别在于网络设计的细节。先从CPM开始说起。</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/v2-96ad2a0de1a4e6530afc09912936ae1a_hd.jpg" alt="img"></p>
<p>整个网络会有多个stage，每个stage设计一个小型网络，用于提取feature，然后在每个stage结束的时候，加上一个监督信号。中间层的信息可以给后续层提供context，后续stage可以认为是基于前面的stage做refinement。这个工作在MPII上面的结果可以达到88.5，在当时是非常好的结果。</p>
<h2 id="Hourglass"><a href="#Hourglass" class="headerlink" title="Hourglass"></a>Hourglass</h2><p>在2016年的7月份，Princeton的Deng Jia组放出了另外一个非常棒的人体姿态估计工作，Hourglass。后续Deng Jia那边基于Hourglass的想法做了Associate Embedding，以及后续的CornerNet都是非常好的工作。</p>
<p>Hourglass相比CPM的最大改进是网络结构更简单，更优美。</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/v2-db15965be3e41da469a60910b5ea2e7a_hd.jpg" alt="img"></p>
<p>上图可以看出，网络是重复的堆叠一个u-shape的structure.</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/v2-3f55e5bd539fd38b8932e2d2386081b1_hd.jpg" alt="img"></p>
<p>pipeline上面跟CPM很类似。只是结构做了修改。从MPII上的结果来看，也有明显的提升，可以达到90.9的PCKh。</p>
<p>这种u-shape的结构其实被广泛应用于现代化的物体检测，分割等算法中，同时结果上面来讲也是有非常好的提升的。另外，Hourglass这种堆多个module的结构，后续也有一些工作follow用在其他任务上面。</p>
<p>但是Hourglass也是存在一些问题的，具体可以看后续讲解的MSPN网络。</p>
<p>在CPM以及Hourglass之后，也有很多不错的工作持续在优化单人姿态估计算法，比如[10] [11]。</p>
<p>2016年的下半年还出现了一个非常重要的数据集: <a href="https://link.zhihu.com/?target=http://cocodataset.org/%23keypoints-eval">COCO</a>。这个时间点也是非常好的时间点。一方面，MPII已经出现两年，同时有很多非常好的工作，比如CPM， Hourglass已经把结果推到90+，数据集已经开始呈现出一定的饱和状态。另外一方面，物体检测&#x2F;行人检测方面，算法提升也特别明显，有了很多很好的工作出现，比如Faster R-CNN和SSD。所以COCO的团队在COCO的数据集上面引入了多人姿态估计的标注，并且加入到了2016年COCO比赛中，当成是一个track。从此，多人姿态估计成为学术界比较active的研究topic。正如前面我在“问题”的部分描述的，多人姿态估计会分成top-down以及bottom-up两种模式。我们这边会先以bottom-up方法开始描述。</p>
<h1 id="多人姿态估计"><a href="#多人姿态估计" class="headerlink" title="多人姿态估计"></a>多人姿态估计</h1><h2 id="DeepCut"><a href="#DeepCut" class="headerlink" title="DeepCut"></a>DeepCut</h2><h2 id="DeeperCut"><a href="#DeeperCut" class="headerlink" title="DeeperCut"></a>DeeperCut</h2><p>基于deepcut的基础，1. 采用residual net进行body part提取。2. 使用Image-Conditioned Pairwish Terms的方法，压缩候选区。</p>
<h2 id="OpenPose"><a href="#OpenPose" class="headerlink" title="OpenPose"></a>OpenPose</h2><p>在2016年COCO比赛中，当时的第一名就是<a href="https://link.zhihu.com/?target=https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a> [12]。 CMU团队基于CPM为组件，先找到图片中的每个joint的位置，然后提出Part Affinity Field （PAF)来做人体的组装。</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/v2-d563e1d28f50e6e04de0a4fa000a3723_hd.jpg" alt="img"></p>
<p>PAF的基本原理是在两个相邻关键点之间，建立一个有向场，比如左手腕，左手肘。我们把CPM找到的所有的左手腕以及左手肘拿出来建立一个二分图，边权就是基于PAF的场来计算的。然后进行匹配，匹配成功就认为是同一个人的关节。依次类别，对所有相邻点做此匹配操作，最后就得到每个人的所有关键点。</p>
<p>在当时来讲，这个工作效果是非常惊艳的，特别是视频的结果图，具体可以参考Openpose的Github官网。在COCO的benchmark test-dev上面的AP结果大概是61.8。</p>
<h3 id="Keypoint-json"><a href="#Keypoint-json" class="headerlink" title="Keypoint-json"></a>Keypoint-json</h3><p>output_overview:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">&#x27;1&#x27;</span> ./build2/examples/openpose/openpose.bin  --image_dir ./image --write_images ./output_image --write_json ./output_json     --display 0</span><br></pre></td></tr></table></figure>

<p>v1.1-25</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">// Result for BODY_25 (25 body parts consisting of COCO + foot)</span><br><span class="line">// const std::map&lt;unsigned int, std::string&gt; POSE_BODY_25_BODY_PARTS &#123;</span><br><span class="line">//     &#123;0,  &quot;Nose&quot;&#125;,</span><br><span class="line">//     &#123;1,  &quot;Neck&quot;&#125;,</span><br><span class="line">//     &#123;2,  &quot;RShoulder&quot;&#125;,</span><br><span class="line">//     &#123;3,  &quot;RElbow&quot;&#125;,</span><br><span class="line">//     &#123;4,  &quot;RWrist&quot;&#125;,</span><br><span class="line">//     &#123;5,  &quot;LShoulder&quot;&#125;,</span><br><span class="line">//     &#123;6,  &quot;LElbow&quot;&#125;,</span><br><span class="line">//     &#123;7,  &quot;LWrist&quot;&#125;,</span><br><span class="line">//     &#123;8,  &quot;MidHip&quot;&#125;,</span><br><span class="line">//     &#123;9,  &quot;RHip&quot;&#125;,</span><br><span class="line">//     &#123;10, &quot;RKnee&quot;&#125;,</span><br><span class="line">//     &#123;11, &quot;RAnkle&quot;&#125;,</span><br><span class="line">//     &#123;12, &quot;LHip&quot;&#125;,</span><br><span class="line">//     &#123;13, &quot;LKnee&quot;&#125;,</span><br><span class="line">//     &#123;14, &quot;LAnkle&quot;&#125;,</span><br><span class="line">//     &#123;15, &quot;REye&quot;&#125;,</span><br><span class="line">//     &#123;16, &quot;LEye&quot;&#125;,</span><br><span class="line">//     &#123;17, &quot;REar&quot;&#125;,</span><br><span class="line">//     &#123;18, &quot;LEar&quot;&#125;,</span><br><span class="line">//     &#123;19, &quot;LBigToe&quot;&#125;,</span><br><span class="line">//     &#123;20, &quot;LSmallToe&quot;&#125;,</span><br><span class="line">//     &#123;21, &quot;LHeel&quot;&#125;,</span><br><span class="line">//     &#123;22, &quot;RBigToe&quot;&#125;,</span><br><span class="line">//     &#123;23, &quot;RSmallToe&quot;&#125;,</span><br><span class="line">//     &#123;24, &quot;RHeel&quot;&#125;,</span><br><span class="line">//     &#123;25, &quot;Background&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>v1.0-18</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">// C++ API call</span><br><span class="line"> #include &lt;openpose/pose/poseParameters.hpp&gt;</span><br><span class="line"> const auto&amp; poseBodyPartMappingCoco = getPoseBodyPartMapping(PoseModel::COCO_18);</span><br><span class="line"> const auto&amp; poseBodyPartMappingMpi = getPoseBodyPartMapping(PoseModel::MPI_15);</span><br><span class="line"></span><br><span class="line"> // Result for COCO (18 body parts)</span><br><span class="line"> // POSE_COCO_BODY_PARTS &#123;</span><br><span class="line"> //     &#123;0,  &quot;Nose&quot;&#125;,</span><br><span class="line"> //     &#123;1,  &quot;Neck&quot;&#125;,</span><br><span class="line"> //     &#123;2,  &quot;RShoulder&quot;&#125;,</span><br><span class="line"> //     &#123;3,  &quot;RElbow&quot;&#125;,</span><br><span class="line"> //     &#123;4,  &quot;RWrist&quot;&#125;,</span><br><span class="line"> //     &#123;5,  &quot;LShoulder&quot;&#125;,</span><br><span class="line"> //     &#123;6,  &quot;LElbow&quot;&#125;,</span><br><span class="line"> //     &#123;7,  &quot;LWrist&quot;&#125;,</span><br><span class="line"> //     &#123;8,  &quot;RHip&quot;&#125;,</span><br><span class="line"> //     &#123;9,  &quot;RKnee&quot;&#125;,</span><br><span class="line"> //     &#123;10, &quot;RAnkle&quot;&#125;,</span><br><span class="line"> //     &#123;11, &quot;LHip&quot;&#125;,</span><br><span class="line"> //     &#123;12, &quot;LKnee&quot;&#125;,</span><br><span class="line"> //     &#123;13, &quot;LAnkle&quot;&#125;,</span><br><span class="line"> //     &#123;14, &quot;REye&quot;&#125;,</span><br><span class="line"> //     &#123;15, &quot;LEye&quot;&#125;,</span><br><span class="line"> //     &#123;16, &quot;REar&quot;&#125;,</span><br><span class="line"> //     &#123;17, &quot;LEar&quot;&#125;,</span><br><span class="line"> //     &#123;18, &quot;Background&quot;&#125;,</span><br><span class="line"> // &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/1574412724600.png" alt="1574412724600"></p>
<h3 id="heatmap-order"><a href="#heatmap-order" class="headerlink" title="heatmap order"></a>heatmap order</h3><p>对于<strong>热图存储格式</strong>，而不是分别保存67个热图(18个主体部分+背景+ 2 x 19个PAFs)，库将它们连接成一个巨大的(宽度x #热图)x(高度)矩阵(即。，由列连接)。例如：列[0，单个热图宽度]包含第一个热图，列[单个热图宽度+ 1, 2 *单个热图宽度]包含第二个热图，等等。注意，一些图像查看器由于尺寸的原因无法显示结果图像。然而，Chrome和Firefox能够正确地打开它们。</p>
<p>PAFs遵循“getposepartpair (const PoseModel PoseModel)”和“getPoseMapIndex(const PoseModel PoseModel)”中指定的顺序。例如，假设COCO(参见下面的示例代码)，COCO中的PAF通道从19开始(‘getPoseMapIndex’中最小的数字，等于#body parts + 1)，到56结束(最高的一个)。</p>
<h2 id="Hourglass-Associative-Embedding"><a href="#Hourglass-Associative-Embedding" class="headerlink" title="Hourglass + Associative Embedding"></a>Hourglass + Associative Embedding</h2><p>A <strong>single-stage</strong>，end-to-end way for joint detection and grouping</p>
<p>关节点检测使用stacked hourglass，在原来的基础上每一次下采样时增加输出通道的个数，同时individual layers的残差模型改为3*3的卷积结构，其他结构不变。</p>
<p>在2016年比赛的榜单上面，还有另外一个很重要的工作就是Deng Jia组的Associative Embedding[13]。文章类似Openpose思路，使用<strong>bottom-up</strong>的方法，寻找part使用了Hourglass的方式来做。关键在于行人的组装上面，提出了Associative Embedding的想法。大概想法是希望对每个关键点输出一个embedding，使得同一个人的embedding尽可能相近，不同人的embedding尽可能不一样。</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/v2-07149694c280fd4c819c97d817f17df1_hd.jpg" alt="img"></p>
<p>在COCO2016比赛后，这个工作持续的在提升，文章发表的时候，COCO test-dev上面的结果在65.5。</p>
<p>除了Openpose以及Associative Embedding之外，bottom-up还有一个工作非常不错，DeepCut[14]以及DeeperCut[15]，他们使用优化问题来直接优化求解人的组合关系。</p>
<h2 id="CPN（18cvpr，stoa）"><a href="#CPN（18cvpr，stoa）" class="headerlink" title="CPN（18cvpr，stoa）"></a>CPN（18cvpr，stoa）</h2><p>topdown思路</p>
<p>第一个stage检测可见的easy keypoint，第二个stage专门解决hard keypoint。</p>
<p>后面一部分章节我会重点围绕COCO数据集，特别是COCO每年的比赛来描述多人姿态估计的进展。虽然2016年bottom-up是一个丰富时间点，但是<strong>从2017年开始，越来的工作开始围绕top-down展开</strong>，一个直接的原因是top-down的效果往往更有潜力。top-down相比bottom-up效果好的原因可以认为有两点。首先是人的recall往往更好。因为top-down是先做人体检测，人体往往会比part更大，所以从检测角度来讲会更简单，相应找到的recall也会更高。其次是关键点的定位精度会更准，这部分原因是基于crop的框，对空间信息有一定的align，同时因为在做single person estimation的时候，可以获得一些中间层的context信息，对于点的定位是很有帮助的。当然，top-down往往会被认为速度比bottom-up会更慢，所以在很多要求实时速度，特别是手机端上的很多算法都是基于openpose来做修改的。不过这个也要例外，我们自己也有做手机端上的多人姿态估计，但是我们是基于top-down来做的，主要原因是我们的人体检测器可以做的非常快。</p>
<p>说完了背景后，在COCO2017年的比赛中，我们的CPN[16]一开始就决定围绕top-down的算法进行尝试。我们当时的想法是一个coarse-to-fine的逻辑，先用一个网络出一个coarse的结果(GlobalNet)，然后再coarse的结果上面做refinement (RefineNet)。具体结果如下：</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/v2-560f07eca67db173ce1c0457d7fbd421_hd.jpg" alt="img"></p>
<p>为了处理处理难的样本，我们在loss上面做了一定的处理，最后的L2 loss我们希望针对难的关键点进行监督，而不是针对所有关键点uniform的进行监督，所以我们提出了一个Hard keypoint mining的loss。这个工作最后在COCO test-dev达到了72.1的结果 （不使用额外数据以及ensemble)，获得了2017年的COCO骨架比赛的第一名。</p>
<p>另外，这个工作的另外一个贡献是比较完备的ablation。我们给出了很多因素的影响。比如top-down的第一步是检测，我们分析了检测性能对最后结果的影响。物体检测结果从30+提升到40+(mmAP)的时候，人体姿态估计能有一定的涨点（1个点左右），但是从40+提升到50+左右，涨点就非常微弱了（0.1-0.2）。另外，我们对data augmentation，网络的具体结构设计都给出了比较完整的实验结果。另外，我们开始引入了传统的ImageNet basemodel (ResNet50)做了backbone，而不是像Openpose或者Hourglass这种非主流的模型设计结构，所以效果上面也有很好的提升。</p>
<h2 id="G-RMI"><a href="#G-RMI" class="headerlink" title="G-RMI"></a>G-RMI</h2><p>CVPR 2017《Towards accurate multi-person pose estimation in the wild》</p>
<p>第一阶段使用faster rcnn做detection，检测出图片中的多个人，并对bounding box进行image crop； 第二阶段采用fully convolutional resnet对每一个bonding box中的人物预测dense heatmap和offset; 最后通过heatmap和offset的融合得到关键点的精确定位 （如下）。</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/1577073795609.png" alt="1577073795609"></p>
<h2 id="RMPE（AlphaPose）"><a href="#RMPE（AlphaPose）" class="headerlink" title="RMPE（AlphaPose）"></a>RMPE（AlphaPose）</h2><p>ICCV 2017《REPE: Regional Multi-person Pose Estimation》</p>
<p>现已更新到新版本AlphaPose，效果拔群</p>
<p>在SPPE结构上添加SSTN，能够在不精准的区域框中提取到高质量的人体区域。并行的SPPE分支（SSTN）来优化自身网络。使用parametric pose NMS来解决冗余检测问题，在该结构中，使用了自创的姿态距离度量方案比较姿态之间的相似度。用数据驱动的方法优化姿态距离参数。最后我们使用PGPG来强化训练数据，通过学习输出结果中不同姿态的描述信息，来模仿人体区域框的生成过程，进一步产生一个更大的训练集。</p>
<p>识别姿态使用Stacked Hourglass. 致力于解决对于imperfect proposal，使得crop的person能够被单人姿态估计方法很好的识别，从而克服检测带来的定位误差。第一步获得human proposal第二步是将proposal输入到两个并行的分支里面，上面的分支是STN+SPPE+SDTN的结构，STN接收的是human proposal，SDTN产生的是pose proposal。下面并行的分支充当额外的正则化矫正器。第三步是对pose proposal做Pose NMS（非最大值抑制），用来消除冗余的pose proposal。</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/1577073834341.png" alt="1577073834341"></p>
<p>参考: <a target="_blank" rel="noopener" href="https://www.cnblogs.com/taoshiqian/p/9593901.html">AlphaPose论文笔记</a></p>
<h2 id="MSPN"><a href="#MSPN" class="headerlink" title="MSPN"></a>MSPN</h2><p>2018年的COCO比赛中，我们继续沿用top-down的思路。当时我们基于CPN做了一些修改，比如把backbone不停的扩大，发现效果提升很不明显。我们做了一些猜测，原来CPN的两个stage可能并没有把context信息利用好，单个stage的模型能力可能已经比较饱和了，增加更多stage来做refinement可能是一个解决当前问题，提升人体姿态估计算法uppper-bound的途径。所以我们在CPN的globalNet基础上面，做了多个stage的堆叠，类似于Hourglass的结构。</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/v2-7017b843c43caf890872d924a9290e67_hd.jpg" alt="img"></p>
<p>相比Hourglass结构，我们提出的MSPN[17]做了如下三个方面的改进。首先是Hourglass的每个stage的网络，使用固定的256 channel，即使中间有下采样，这种结构对信息的提取并不是很有益。所以我们使用了类似ResNet-50这种标准的ImageNet backbone做为每个stage的网络。另外，在两个相邻stage上面，我们也加入了一个连接用于更好的信息传递。最后，我们对于每个stage的中间层监督信号做了不同的处理，前面层的监督信号更侧重分类，找到coarse的位置，后面更侧重精确的定位。从最后效果上面来看，我们在COCO test-dev上面一举跑到了76.1 （单模型不加额外数据）。</p>
<h2 id="HRNet"><a href="#HRNet" class="headerlink" title="HRNet"></a>HRNet</h2><p>之前我们讲的很多人体姿态估计方面的工作，都在围绕context来做工作，如何更好的encode和使用这些context是大家工作的重点。到了2019年， MSRA wang jingdong组出了一个很好的工作，提出了spatial resolution的重要性。在这篇工作之前，我们往往会暴力的放大图片来保留更多信息，同时给出更精准的关键点定位，比如从256x192拉大到384x288。这样对效果提升还是很明显的，但是对于计算量的增加也是非常大的。 HRNet从另外一个角度，抛出了一个新的可能性：</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/v2-43134409349aa3f2d4e964acd6cd4060_hd.jpg" alt="img"></p>
<p>相比传统的下采样的网络结构，这里提出了一种<strong>新的结构</strong>。<strong>分成多个层级，但是始终保留着最精细的spaital那一层的信息，通过fuse下采样然后做上采样的层，来获得更多的context以及语义层面的信息</strong>（比如更大的感受野）。从结果上面来看，在COCO test-dev上面单模型可以达到75.5。</p>
<hr>
<p>到此为止，我们重点讲述了几个多人姿态估计的算法，当然中间穿插了不少我们自己的私货。在多人姿态估计领域还有很多其他很好的工作，因为篇幅问题，这里我们就略过了。</p>
<p>回到2017年，MPI提出了一个新的数据集， <a href="https://link.zhihu.com/?target=https://posetrack.net/">PoseTrack</a>，主要是希望能帮忙解决视频中的人体姿态估计的问题，并且在每年的ICCV或者ECCV上面做challenge比赛。 PoseTrack的数据集主要还是来源于MPII的数据集，标注风格也很相近。围绕PoseTrack这个任务，我们重点讲一个工作, Simple Baselines。</p>
<h1 id="Pose-Track"><a href="#Pose-Track" class="headerlink" title="Pose Track"></a>Pose Track</h1><h2 id="Simple-Baselines"><a href="#Simple-Baselines" class="headerlink" title="Simple Baselines"></a>Simple Baselines</h2><p>Simple Baselines [19]是xiao bin在MSRA的工作。提出了一种非常简洁的结构可以用于多人姿态估计以及人体姿态估计的跟踪问题。这里重点讲一下对于PoseTrack的处理方法：</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/v2-d42d665de14dcdc85938b33a519db196_hd.jpg" alt="img"></p>
<p>这里有两个细节，首先是会利用上一帧的检测结果，merge到新的一帧，避免检测miss的问题。另外，在两帧间，会使用OKS based相似度来做人体的关联，而不是只是简单的使用框的overlap，这样可以更好的利用每个关键点的temporal smooth的性质。从结果上面来看，这个方法也获得了PoseTrack2018比赛的第一名。</p>
<p>到目前位置，我们描述了单人的姿态估计，多人的姿态估计，以及简单讲了一下视频中的人体姿态跟踪的问题。最后，我们讲一下3D人体姿态估计的问题，这个我觉得这个是目前非常active的研究方向，也是未来的重要的方向。</p>
<h1 id="3D-人体姿态估计"><a href="#3D-人体姿态估计" class="headerlink" title="3D 人体姿态估计"></a>3D 人体姿态估计</h1><ul>
<li>3D keypoint （Skeleton）</li>
<li>3D shape (3D surface)<ul>
<li>Densepose</li>
<li>SMPL（参数化人体模型）</li>
</ul>
</li>
</ul>
<p>2017年deva Ramanan组的一个非常有意思的工作【20】开始说起，<strong>3D Human Pose Estimation &#x3D; 2D Pose Estimation + Matching</strong>。从名字可以看出，大致的做法。首先是做2D的人体姿态估计，然后基于Nearest neighbor最近邻的match来从training data中找最像的姿态。2D的姿态估计算法是基于CPM来做的。3D的match方法是先把training data中的人体3d骨架投射到2D空间，然后把test sample的2d骨架跟这些training data进行对比，最后使用最相近的2d骨架对应的3D骨架当成最后test sample点3D骨架。当training数据量非常多的时候，这种方法可能可以保证比较好的精度，但是在大部分时候，这种匹配方法的精度较粗，而且误差很大</p>
<p>也在17年，另外一个非常有意思的工作【21】发表在ICCV2017。同样，从这个工作的名字可以看出，这个工作提出了一个比较simple的baseline，但是效果还是非常明显。方法上面来讲，就是先做一个2d skeleton的姿态估计，方法是基于Hourglass的，文章中的解释是较好的效果以及不错的速度。 基于获得的2d骨架位置，后续接入两个fully connected的操作，直接回归3D坐标点。这个做法非常<strong>粗暴直接</strong>，但是效果还是非常明显的。在回归之前，需要对坐标系统做一些操作。</p>
<p>从2017年的ICCV开始，已经有工作【22】开始把2D以及3d skeleton的估计问题joint一起来做优化。这样的好处其实是非常明显的。因为很多2d数据对于3d来讲是有帮助的，同时3D姿态对于2d位置点估计也能提供额外的信息辅助。2D的MPII， COCO数据可以让算法获得比较强的前背景点分割能力，然后3D的姿态估计数据集只需要关注前景的3D骨架估计。这也是目前学术界数据集的现状。从实际效果上面来讲，<strong>joint training</strong>的方法效果确实也比割裂的train 2d以及3d skeleton效果要好。</p>
<p><strong>2018年开始，3D skeleton开始往3d shape发展。</strong></p>
<p>原先只需要知道joint点的3D坐标位置，但是很多应用，比如人体交互，美体，可能需要更dense的人体姿态估计。这时候就有了一个比较有意思的工作densePose 【23】。这个工作既提出来一个新的问题，也包含新的benchmark以及baseline。相比传统的SMPL模型，这个工作提出了使用UV map来做估计（同时间也有denseBody类似的工作），可以获得非常dense的3d姿态位置，等价于生成了3d shape。当然，从3d shape的角度来讲，有很多非常不错的工作，这里就不做重点展开。</p>
<p>最后讲一下3d人体姿态估计目前存在的问题(训练集小，数据鲁棒性不足，Pose固定)。我<strong>个人认为</strong>主要是benchmark。目前最常使用的human 3.6M实际上很容易被overfit，因为subjects数量太小（实际训练样本只有5－6人，depend on具体的测试方法，测试样本更少）。同时，是在受限的实验室场景录制，跟真实场景差异太大，背景很干净，同时前景的动作pose也比较固定。当然，3d skeleton的数据集的难度非常大，特别是需要采集unconstrained条件下面的数据。目前也有一些工作在尝试用生成的数据来提升结果。</p>
<p><strong>Apply</strong></p>
<p>首先的一个应用是人体的<strong>动作行为估计</strong>，要理解行人，人体的姿态估计其实是一个非常重要的中间层信息。目前有蛮多基于人体姿态估计直接做action recogntion的工作，比如把关键点当成graph的节点，然后是使用graph convolution network来整合各种信息做动作分类。我博士的研究课题是action recognition，我读完四年博士的一个总结是action这个问题，如果需要真正做到落地，人体姿态估计算法是必不可少的组成部分。</p>
<p>第二类应用是偏娱乐类的，比如<strong>人体交互，美体</strong>等。比如可以通过3d姿态估计来虚拟出一个动画人物来做交互，使用真实人体来控制虚拟人物。另外比如前一段时间比较火热的瘦腰，美腿等操作背后都可能依赖于人体姿态估计算法。</p>
<p>第三类应用是可以做为其他算法的辅助环节，比如Person ReID可以基于人体姿态估计来做alignment，姿态估计可以用来辅助行人检测，杀掉检测的FP之类的。</p>
<h2 id="SMPL"><a href="#SMPL" class="headerlink" title="SMPL"></a>SMPL</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/chenguowen21/article/details/82793994">SMPL 介绍</a></p>
<p>SMPL模型是一种参数化人体模型，是马普所提出的一种人体建模方法，该方法可以进行任意的人体建模和动画驱动。这种方法与传统的LBS的最大的不同在于其提出的人体姿态影像体表形貌的方法，这种方法可以模拟人的肌肉在肢体运动过程中的凸起和凹陷。因此可以避免人体在运动过程中的表面失真，可以精准的刻画人的肌肉拉伸以及收缩运动的形貌。 </p>
<p>该方法中β和θ是其中的输入参数，其中β代表是个人体高矮胖瘦、头身比等比例的10个参数，θ是代表人体整体运动位姿和24个关节相对角度的75个参数。</p>
<p>β参数是ShapeBlendPose参数，可以通过10个增量模板控制人体形状变化： 具体而言：每个参数控制人体形态的变化可以通过动图来刻画：<br><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/20190511224026457.gif" width="300" height="300" div><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/20190511224048329.gif" width="300" height="300" div></p>
<h2 id="DensePose"><a href="#DensePose" class="headerlink" title="DensePose"></a>DensePose</h2><p><a target="_blank" rel="noopener" href="https://new.qq.com/omn/20190210/20190210A0ADS1.html">实时人体姿态估计：Dense Pose及其应用展望 -QQnews</a></p>
<p><a target="_blank" rel="noopener" href="http://m.elecfans.com/article/775573.html">基于DensePose的姿势转换系统，仅根据一张输入图像和目标姿势 - 新智元</a></p>
<ul>
<li>「人体姿态估计」(human pose estimation)</li>
</ul>
<p>应用场景:</p>
<ul>
<li>「密集姿态转移」（dense pose transfer）</li>
</ul>
<p>Facebook 和 Inria France 的研究人员分别在 CVPR 2018 和 ECCV 2018 相继发表了两篇有关「人体姿态估计」(human pose estimation) 的文章 [1] [2]，用于介绍他们提出的 Dense Pose 系统以及一个应用场景「密集姿态转移」（dense pose transfer）。</p>
<p>数据集：</p>
<ul>
<li>DensePose COCO 的大型数据集，包含了预先手工标注的 5 万张各种人类动作的图片。</li>
</ul>
<p><strong>三点：</strong></p>
<p>（1）利用 Dense Pose 实现单张 2D 人体图像到 3D 表面模型的原理 ；</p>
<p>（2）如何将 Dense Pose 系统应用在「姿态转移」（pose transfer）这一问题上；</p>
<p>（3）粗略展望 Dense Pose 的一些潜在应用。</p>
<p><strong>&lt;1&gt; 2D 图片中描述人体的像素，映射到一个 3D 表面模型</strong></p>
<ul>
<li>系统架构</li>
<li>工作流程</li>
</ul>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/8iJ92H1549778415090A7Dfq.jpeg" alt="img"></p>
<p>图 1：密集姿态估计的目标是将 2D 图片中描述人体的像素，映射到一个 3D 表面模型。左：输入的原始图像，以及利用 [1] 中提出的 Dense Pose-RCNN，获得人体各区域的 UV 坐标。UV 坐标又称纹理坐标 (texture coordinates), 用于控制 3D 表面的纹理映射； 中：DensePose COCO 数据集中的原始标注；右：人体表面的分割以及 UV 参数化示意图。</p>
<p>密集姿态估计的核心任务是，训练一个深度网络，用于预测 2D 图片像素 (image pixels）与 3D 表面模型点 (surface points) 之间的密集联系 (dense correspondences)。这个任务最近已经通过基于全连接卷积网络 [4] 的 Dense Regression (DenseReg) 系统 [3] 得到了解决。</p>
<p>而 Dense Pose-RCNN 系统 [1]，正是结合了 DenseReg 系统以及 Mask-RCNN 架构 [5]。</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/1000.jpg" alt="图2"></p>
<p>图 2 展示了 Dense Pose-RCNN 的级连 (cascade) 架构：这是一个全卷积网络 (fully-convolutional network)，并连接着 ROIAlign 池化层 (ROIAlign pooling)，用于处理两个核心任务，分别是：<strong>（1）分类</strong>。判断图片的某一像素来自于「背景」，还是「人体部位」；<strong>（2）回归</strong>。预测该像素在「人体部位」的具体坐标。</p>
<p>图 2 中的 ResNet50 FPN (feature pyramid networks) 将输出 feature map，然后通过 ROIAlign 模块对每一个 ROI 生成固定尺寸的 feature map。</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/1000-1576824372301.jpg" alt="图3"></p>
<p>图 3 展示了 ROIAlign 模块的「跨级连」(cross-cascading) 结构，这种结构利用两个辅助任务 (keypoint estimation &amp; mask) 提供的信息，帮助提高 Dense Pose 系统的姿态估计效果。作为 Dense Pose-RCNN 基础之一的 Mask-RCNN [5] 结构，正是借助两个相关任务（即 keypoint estimation 和 instance segmentation）提供的信息，用于提高分割效果。</p>
<p>图 3：Dense Pose-RCNN[1] 中的 ROIAlign 模块采用了「跨级连」(cross-cascading) 架构。</p>
<p><strong>&lt;2&gt;Dense Pose 到「姿态转移」（pose transfer)</strong></p>
<p>应用：「纹理转移」(texture transfer)</p>
<p> ECCV 2018 上，论文 [1] 的三名作者发表了 Dense Pose 的一个后续应用，即「密集姿态转移」(dense pose transfer，以下简称为 DPT) [2]。与纹理转换不同的是，DPT 这一任务的目标是，根据输入的 2D 人体图像和目标姿态 (target dense pose)，将输入图像中的人体姿态转换成目标姿态，并且不改变人体表面纹理。</p>
<p>如图 5 所示，DPT 系统以 Dense Pose[1] 为基础，并且由两个互补的模块组成，分别是（1）推测模块 (predictive module)，用于根据输入图像，预测出具有目标姿态的人体图像；（2）变形模块 (warping module)，负责从输入图像中提取纹理，并「补全」(inpainting) 具有目标姿态的人体表面纹理。此外，系统中还有一个合成模块 (blending module)，通过端对端、可训练的单一框架，将推测和变形模块的输出进行合成，并产生最终的图像。</p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/1000-1576824579481.jpg" alt="图5"></p>
<p>图 5：密集姿态转移（DPT) 系统的流程图。该系统包括推测模块、变形模块，以及合成模块。</p>
<p><strong>&lt;3&gt;展望 Dense Pose</strong></p>
<p>一个应用方向是，利用单一图片进行服装的虚拟试穿</p>
<p>另一个应用方向则是，远程视频诊断背部痛疾。</p>
<h2 id="AMA-net"><a href="#AMA-net" class="headerlink" title="AMA-net"></a>AMA-net</h2><p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/3-Figure2-1.png" alt="Figure 2: An illustration of feature extraction on ROIs using different methods. The top and middle demonstrate the existing methods: DensePose R-CNN and Parsing R-CNN. The bottom is our proposed AMA-net."></p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/4-Figure3-1.png" alt="Figure 3: The framework of our AMA-net. In the added dense pose estimation branch, we propose the Multi-Path Representation learning modular and the Adaptive Path Aggregation modular for exploring the multi-level features. B, BP, SP, U, and V losses denote the losses on body masks, body part masks, surface patches, U coordinates, and V coordinates respectively."></p>
<p><img src="/2019/11/22/Paper/Paper-CV-PoseEstimation/8-Figure5-1.png" alt="Figure 5: DensePose R-CNN vs AMA-net. Left: input image; middle: DensePose R-CNN; right: AMA-net. The red circles spot the difference between the DensePose R-CNN and AMA-net estimation. The yellow circles mark the positions where both methods fail to estimate UV coordinates."></p>
<p>Figure 5: DensePose R-CNN vs AMA-net. Left: input image; middle: DensePose R-CNN; right: AMA-net. The red circles spot the difference between the DensePose R-CNN and AMA-net estimation. The yellow circles mark the positions where both methods fail to estimate UV coordinates.</p>
<ul>
<li>从论文的结果看，效果比DensePose的结果更好。</li>
</ul>
<h1 id="参考Blog"><a href="#参考Blog" class="headerlink" title="参考Blog"></a>参考Blog</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/BockSong/article/details/81037059">人体姿态估计(Human Pose Estimation)文献综述</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41665360/article/details/91432434">论文笔记：Towards accurate multi-person pose estimation in the wild（G-RMI）</a></p>
<h1 id="refrence-1"><a href="#refrence-1" class="headerlink" title="refrence 1:"></a>refrence 1:</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[1] Alp Güler, Riza, Natalia Neverova, and Iasonas Kokkinos. &quot;Densepose: Dense human pose estimation in the wild.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</span><br><span class="line"></span><br><span class="line">[2] Neverova, Natalia, Riza Alp Guler, and Iasonas Kokkinos. &quot;Dense pose transfer.&quot; Proceedings of the European Conference on Computer Vision (ECCV). 2018.</span><br><span class="line"></span><br><span class="line">[3] Alp Guler, Riza, et al. &quot;Densereg: Fully convolutional dense shape regression in-the-wild.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.</span><br><span class="line"></span><br><span class="line">[4] Chen, Liang-Chieh, et al. &quot;Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.&quot; IEEE transactions on pattern analysis and machine intelligence 40.4 (2017): 834-848.</span><br><span class="line"></span><br><span class="line">[5] He, Kaiming, et al. &quot;Mask r-cnn.&quot; Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017.</span><br><span class="line"></span><br><span class="line">[6] Liu, Ziwei, et al. &quot;Deepfashion: Powering robust clothes recognition and retrieval with rich annotations.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</span><br><span class="line"></span><br><span class="line">[7] Siarohin, Aliaksandr, et al. &quot;Deformable gans for pose-based human image generation.&quot; CVPR 2018-Computer Vision and Pattern Recognition. 2018.</span><br></pre></td></tr></table></figure>

<h1 id="refrence-2"><a href="#refrence-2" class="headerlink" title="refrence 2:"></a>refrence 2:</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">Reference</span><br><span class="line"></span><br><span class="line">[1] Randomized Trees for Human Pose Detection, Rogez etc, CVPR 2018</span><br><span class="line"></span><br><span class="line">[2] Local probabilistic regression for activity-independent human pose inference, Urtasun etc, ICCV 2009</span><br><span class="line"></span><br><span class="line">[3] Strong Appearance and Expressive Spatial Models for Human Pose Estimation, Pishchulin etc, ICCV 2013</span><br><span class="line"></span><br><span class="line">[4] Pictorial Structures Revisited: People Detection and Articulated Pose Estimation, Andriluka etc, CVPR 2009</span><br><span class="line"></span><br><span class="line">[5] Latent Structured Models for Human Pose Estimation, Ionescu etc, ICCV 2011</span><br><span class="line"></span><br><span class="line">[6] Poselet Conditioned Pictorial Structures, Pishchulin etc, CVPR 2013</span><br><span class="line"></span><br><span class="line">[7] Learning Human Pose Estimation Features with Convolutional Networks, Jain etc, ICLR 2014</span><br><span class="line"></span><br><span class="line">[8] Convolutional Pose Machines, Wei etc, CVPR 2016</span><br><span class="line"></span><br><span class="line">[9] Stacked Hourglass Networks for Human Pose Estimation, Newell etc, ECCV 2016</span><br><span class="line"></span><br><span class="line">[10] Multi-Context Attention for Human Pose Estimation, Chu etc, CVPR 2017</span><br><span class="line"></span><br><span class="line">[11] Deeply Learned Compositional Models for Human Pose Estimation, ECCV 2018</span><br><span class="line"></span><br><span class="line">[12] Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields, Cao etc, CVPR 2017</span><br><span class="line"></span><br><span class="line">[13] Associative Embedding: End-to-End Learning for Joint Detection and Grouping, Newell etc, NIPS 2017</span><br><span class="line"></span><br><span class="line">[14] DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation, Pishchulin etc, CVPR 2016</span><br><span class="line"></span><br><span class="line">[15] DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model, Insafutdinov, ECCV 2016</span><br><span class="line"></span><br><span class="line">[16] Cascaded Pyramid Network for Multi-Person Pose Estimation, Chen etc, CVPR 2017</span><br><span class="line"></span><br><span class="line">[17] Rethinking on Multi-Stage Networks for Human Pose Estimation, Li etc, Arxiv 2018</span><br><span class="line"></span><br><span class="line">[18] Deep High-Resolution Representation Learning for Human Pose Estimation, Sun etc, CVPR 2019</span><br><span class="line"></span><br><span class="line">[19] Simple Baselines for Human Pose Estimation and Tracking, Xiao etc, ECCV 2018</span><br><span class="line"></span><br><span class="line">[20] 3D Human Pose Estimation = 2D Pose Estimation + Matching, Chen etc, CVPR 2017</span><br><span class="line"></span><br><span class="line">[21] A simple yet effective baseline for 3d human pose estimation, Martinez, ICCV 2017</span><br><span class="line"></span><br><span class="line">[22] Compositional Human Pose Regression, Sun etc, ICCV 2017</span><br><span class="line"></span><br><span class="line">[23] Densepose: Dense Human Pose Estimation in the Wild, Guler etc, CVPR 2018</span><br><span class="line"></span><br><span class="line">[24] ThunderNet: Toward Real-time Generic Object Detection, Qin etc, ICCV 2019</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/11/13/CV/CV_VTON/Paper-CV+WUTON/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/13/CV/CV_VTON/Paper-CV+WUTON/" class="post-title-link" itemprop="url">Paper-CV+WUTON</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-13 15:46:46" itemprop="dateCreated datePublished" datetime="2019-11-13T15:46:46+00:00">2019-11-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/" itemprop="url" rel="index"><span itemprop="name">CV_Apply</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/VTON/" itemprop="url" rel="index"><span itemprop="name">VTON</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/11/11/Paper/Paper-CV-Dataset/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/11/Paper/Paper-CV-Dataset/" class="post-title-link" itemprop="url">Paper-CV_Human Parsing Dataset</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-11 17:42:38" itemprop="dateCreated datePublished" datetime="2019-11-11T17:42:38+00:00">2019-11-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:40" itemprop="dateModified" datetime="2025-08-06T08:16:40+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Datasets/" itemprop="url" rel="index"><span itemprop="name">Datasets</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[toc]</p>
<h1 id="人体解析数据集-human-parsing"><a href="#人体解析数据集-human-parsing" class="headerlink" title="人体解析数据集 -human parsing"></a>人体解析数据集 -human parsing</h1><table>
<thead>
<tr>
<th></th>
<th>开源</th>
<th></th>
<th>项目</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><strong>LIP</strong></td>
<td>[open]</td>
<td>2017</td>
<td>SS-NAN</td>
<td>Single Person <a target="_blank" rel="noopener" href="http://hcp.sysu.edu.cn/lip">http://hcp.sysu.edu.cn/lip</a> <br><a target="_blank" rel="noopener" href="https://github.com/Engineering-Course/LIP_SSL">https://github.com/Engineering-Course/LIP_SSL</a></td>
</tr>
<tr>
<td><strong>CIHP</strong></td>
<td>[open]</td>
<td>2018</td>
<td></td>
<td><a target="_blank" rel="noopener" href="http://sysu-hcp.net/lip/overview.php">http://sysu-hcp.net/lip/overview.php</a></td>
</tr>
<tr>
<td><strong>ATR</strong></td>
<td>[open]</td>
<td>2015</td>
<td></td>
<td></td>
</tr>
<tr>
<td><em>Chictopia10k</em></td>
<td></td>
<td></td>
<td>Co-CNN</td>
<td>human parsing  <a target="_blank" rel="noopener" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Liang_Human_Parsing_With_ICCV_2015_paper.html">“Human parsing with contextualized convolutional neural network.” ICCV’15</a>,</td>
</tr>
<tr>
<td><strong>VIP</strong></td>
<td>[open]</td>
<td></td>
<td></td>
<td>Video Multi-Person Human Parsing</td>
</tr>
<tr>
<td><strong>MHP</strong></td>
<td>[open]</td>
<td>2017</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://lv-mhp.github.io/dataset">https://lv-mhp.github.io/dataset</a></td>
</tr>
<tr>
<td><strong>Person-Part</strong></td>
<td>[open]</td>
<td>2014</td>
<td></td>
<td><a target="_blank" rel="noopener" href="http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html">http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html</a></td>
</tr>
<tr>
<td>Fashionista</td>
<td></td>
<td>2012</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>DeepFashion</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>VITON</td>
<td></td>
<td>2017</td>
<td>VITON</td>
<td>闭源；16,253 pairs</td>
</tr>
<tr>
<td><em>MPV</em></td>
<td></td>
<td>2019</td>
<td>MG-VTON</td>
<td>已经闭源</td>
</tr>
</tbody></table>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wxf19940618/article/details/83661891">refer</a> </p>
<h1 id="引文1-人体解析-human-parsing"><a href="#引文1-人体解析-human-parsing" class="headerlink" title="[引文1]人体解析-human parsing"></a>[引文1]人体解析-human parsing</h1><p>人体解析-human parsing</p>
<p>研究目标与意义<br>人体解析是指将在图像中捕获的人分割成多个语义上一致的区域，例如， 身体部位和衣物。作为一种细粒度的语义分割任务，它比仅是寻找人体轮廓的人物分割更具挑战性。 人体解析对于以人为中心的分析非常重要，并且具有许多工业上的应用，例如，虚拟现实，视频监控和人类行为分析等等。</p>
<p>Fashionista 数据集</p>
<p>论文：Parsing Clothing in Fashion Photographs</p>
<p>论文地址：<a target="_blank" rel="noopener" href="http://www.tamaraberg.com/papers/parsingclothing.pdf">http://www.tamaraberg.com/papers/parsingclothing.pdf</a></p>
<p><img src="/2019/11/11/Paper/Paper-CV-Dataset/20181102213456284.png" alt="Fashionista"></p>
<p>Person-part 数据集</p>
<p>论文：Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts</p>
<p>论文地址：<a target="_blank" rel="noopener" href="http://www.stat.ucla.edu/~xianjie.chen/paper/Chen14cvpr.pdf">http://www.stat.ucla.edu/~xianjie.chen/paper/Chen14cvpr.pdf</a></p>
<p>数据集地址：<a target="_blank" rel="noopener" href="http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html">http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html</a></p>
<p><img src="/2019/11/11/Paper/Paper-CV-Dataset/20181102213456243.png" alt="Person Part"></p>
<p>ATR 数据集</p>
<p>论文：Deep Human Parsing with Active Template Regression</p>
<p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1503.02391.pdf">https://arxiv.org/pdf/1503.02391.pdf</a></p>
<p><img src="/2019/11/11/Paper/Paper-CV-Dataset/20181102213456344.png" alt="ATR img"></p>
<p>LIP数据集</p>
<p>论文：Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing</p>
<p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.05446.pdf">https://arxiv.org/pdf/1703.05446.pdf</a></p>
<p>数据集地址：<a target="_blank" rel="noopener" href="http://hcp.sysu.edu.cn/lip">http://hcp.sysu.edu.cn/lip</a></p>
<p>代码地址：<a target="_blank" rel="noopener" href="https://github.com/Engineering-Course/LIP_SSL">https://github.com/Engineering-Course/LIP_SSL</a></p>
<p><img src="/2019/11/11/Paper/Paper-CV-Dataset/20181102213456375.png" alt="img"></p>
<p>MHP数据集</p>
<p>论文：Multi-Human Parsing in the Wild</p>
<p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.07206.pdf">https://arxiv.org/pdf/1705.07206.pdf</a></p>
<p>数据集地址：<a target="_blank" rel="noopener" href="https://lv-mhp.github.io/dataset">https://lv-mhp.github.io/dataset</a></p>
<p><img src="/2019/11/11/Paper/Paper-CV-Dataset/20181102213456383.png" alt="img"></p>
<p>CIHP 数据集</p>
<p>论文：Instance-level Human Parsing via Part Grouping Network</p>
<p>论文地址：<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ke">http://openaccess.thecvf.com/content_ECCV_2018/papers/Ke</a>_</p>
<p>Gong_Instance-level_Human_Parsing_ECCV_2018_paper.pdf</p>
<p>数据集地址：<a target="_blank" rel="noopener" href="http://sysu-hcp.net/lip/overview.php">http://sysu-hcp.net/lip/overview.php</a></p>
<p>代码地址：<a target="_blank" rel="noopener" href="http://sysu-hcp.net/lip/">http://sysu-hcp.net/lip/</a></p>
<p><img src="/2019/11/11/Paper/Paper-CV-Dataset/20181102213456303.png" alt="img"></p>
<p><strong>DeepFashion:</strong></p>
<p><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html">HomePage -Download</a></p>
<p>​	DeepFashion [38] only have the pairs of the same person in different poses but do not have the image of clothes. </p>
<h3 id="人体解析近年论文"><a href="#人体解析近年论文" class="headerlink" title="人体解析近年论文:"></a>人体解析近年论文:</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">2018-ECCV-Mutual Learning to Adapt for Joint Human Parsing and Pose Estimation</span><br><span class="line">【MuLA】用于联合人体解析和姿态估计，通过反复利用其并行任务中的指导信息来预测动态任务特定的模型参数</span><br><span class="line"></span><br><span class="line">2018-ECCV-Instance-level Human Parsing via Part Grouping Network</span><br><span class="line">【PGN】在一次传递中解析图像中的多个人，将实例级人类解析重新定义为两个可以通过统一网络共同学习和相互提炼的双关联子任务：1）用于将每个像素指定为人体部位（例如，面部，手臂）的语义分割; 2）实例感知边缘检测，以将语义部位分组为不同的人物实例</span><br><span class="line"></span><br><span class="line">2018-ECCV-Macro-Micro Adversarial Network for Human Parsing</span><br><span class="line">【MMAN】有两个判别器，一个Macro D用于低分辨率标签图并对语义不一致进行惩罚，例如错位的身体部位。另一个Micro D专注于高分辨率标签图的多个patch，以解决局部不一致性，例如模糊和孔洞</span><br><span class="line"></span><br><span class="line">2018-CVPR-Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer</span><br><span class="line">提出了一种获得训练数据的新方法，可以使用人体关键点的数据来生成人体部位解析数据。主要想法是利用人类之间的形态相似性，将一个人的部位解析结果传递给具有相似姿势的另一个人</span><br><span class="line"></span><br><span class="line">2017-CVPR-Look Into Person: Self-Supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing</span><br><span class="line">不引入额外信息，可以用自生成的人类关节的信息反过来指导人体解析</span><br><span class="line"></span><br><span class="line">2017-CVPR-Learning Adaptive Receptive Fields for Deep Image Parsing Network</span><br><span class="line">一种自动调节深度图像解析网络中感受野的新方法，在网络主干中使用两个仿射变换层并在特征映射上运行</span><br><span class="line"></span><br><span class="line">2017-CVPR- Multiple-Human Parsing in the Wild</span><br><span class="line">MH-Parser借助新的Graph-GAN模型以自下而上的方式同时生成全局解析和人物解析</span><br><span class="line"></span><br><span class="line">2015-ICCV-Human Parsing with Contextualized Convolutional Neural Network</span><br><span class="line">【Co-CNN】架构，将多级图像上下文集成到一个统一的网络中</span><br></pre></td></tr></table></figure>

<h1 id="引文2-Dataset-Overview-中山大"><a href="#引文2-Dataset-Overview-中山大" class="headerlink" title="[引文2] Dataset Overview [中山大]"></a>[引文2] Dataset Overview [中山大]</h1><h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><p>Look into Person (LIP) is a new large-scale dataset, focus on semantic understanding of person. Following are the detailed descriptions.</p>
<h3 id="1-1-Volume"><a href="#1-1-Volume" class="headerlink" title="1.1 Volume"></a>1.1 Volume</h3><p>The dataset contains 50,000 images with elaborated pixel-wise annotations with 19 semantic human part labels and 2D human poses with 16 key points.</p>
<h3 id="1-2-Diversity"><a href="#1-2-Diversity" class="headerlink" title="1.2 Diversity"></a>1.2 Diversity</h3><p>The annotated 50,000 images are cropped person instances from <a target="_blank" rel="noopener" href="http://mscoco.org/home/">COCO dataset</a> with size larger than 50 * 50.The images collected from the real-world scenarios contain human appearing with challenging poses and views, heavily occlusions, various appearances and low-resolutions. We are working on collecting and annotating more images to increase diversity.</p>
<h2 id="2-Download"><a href="#2-Download" class="headerlink" title="2. Download"></a>2. Download</h2><h3 id="2-1-Single-Person"><a href="#2-1-Single-Person" class="headerlink" title="2.1 Single Person"></a>2.1 Single Person</h3><p>We have divided images into three sets. 30462 images for training set, 10000 images for validation set and 10000 for testing set.The dataset is available at <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/0BzvH3bSnp3E9ZW9paE9kdkJtM3M?usp=sharing">Google Drive</a> and <a target="_blank" rel="noopener" href="http://pan.baidu.com/s/1nvqmZBN">Baidu Drive</a>.</p>
<p>Besides we have another large dataset mentioned in <a target="_blank" rel="noopener" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Liang_Human_Parsing_With_ICCV_2015_paper.html">“Human parsing with contextualized convolutional neural network.” ICCV’15</a>, which focuses on fashion images. You can download the dataset including 17000 images as extra training data.</p>
<h3 id="2-2-Multi-Person"><a href="#2-2-Multi-Person" class="headerlink" title="2.2 Multi-Person"></a>2.2 Multi-Person</h3><p>To stimulate the multiple-human parsing research, we collect the images with multiple person instances to establish the first standard and comprehensive benchmark for instance-level human parsing. Our Crowd Instance-level Human Parsing Dataset (CIHP) contains 28280 training, 5000 validation and 5000 test images, in which there are 38280 multiple-person images in total.</p>
<p>You can also downlod this dataset at <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/0BzvH3bSnp3E9ZW9paE9kdkJtM3M?usp=sharing">Google Drive</a> and <a target="_blank" rel="noopener" href="http://pan.baidu.com/s/1nvqmZBN">Baidu Drive</a>.</p>
<h3 id="2-3-Video-Multi-Person-Human-Parsing"><a href="#2-3-Video-Multi-Person-Human-Parsing" class="headerlink" title="2.3 Video Multi-Person Human Parsing"></a>2.3 Video Multi-Person Human Parsing</h3><p>VIP(Video instance-level Parsing) dataset, the first video multi-person human parsing benchmark, consists of 404 videos covering various scenarios. For every 25 consecutive frames in each video, one frame is annotated densely with pixel-wise semantic part categories and instance-level identification. There are 21247 densely annotated images in total. We divide these 404 sequences into 304 train sequences, 50 validation sequences and 50 test sequences.</p>
<p>You can also downlod this dataset at <a target="_blank" rel="noopener" href="https://1drv.ms/f/s!ArFSFaZzVErwgSHRpiJNJTzgMR8j">OneDrive</a> and <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/18_PVNy7FCh4T74nVzRXbtA">Baidu Drive</a>.</p>
<ul>
<li>VIP_Fine: All annotated images and fine annotations for train and val sets.</li>
<li>VIP_Sequence: 20-frame surrounding each VIP_Fine image (-10 | +10).</li>
<li>VIP_Videos: 404 video sequences of VIP dataset.</li>
</ul>
<h3 id="2-4-Image-based-Multi-pose-Virtual-Try-On"><a href="#2-4-Image-based-Multi-pose-Virtual-Try-On" class="headerlink" title="2.4 Image-based Multi-pose Virtual Try On"></a>2.4 Image-based Multi-pose Virtual Try On</h3><p>MPV (Multi-Pose Virtual try on) dataset, which consists of 35,687&#x2F;13,524 person&#x2F;clothes images, with the resolution of 256x192. Each person has different poses. We split them into the train&#x2F;test set 52,236&#x2F;10,544 three-tuples, respectively.</p>
<p>You can also downlod this dataset at <a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1dCqrNCp9zac7vikYkBRZAPl-RFFsmZdR">Google Drive</a> or <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1ym9o9ScPRwGlfGjsPBlDkw">Baidu Drive</a>.<br>Baidu Drive extract password:<br>		f6i2</p>
<h1 id="引文"><a href="#引文" class="headerlink" title="引文"></a>引文</h1><p>引文1：<a target="_blank" rel="noopener" href="https://blog.csdn.net/wxf19940618/article/details/83661891">https://blog.csdn.net/wxf19940618/article/details/83661891</a></p>
<p>引文2:  <a target="_blank" rel="noopener" href="http://www.sysu-hcp.net/lip/overview.php">http://www.sysu-hcp.net/lip/overview.php</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/11/11/CV/CV_VTON/Paper-CV+CP-VTON/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/11/CV/CV_VTON/Paper-CV+CP-VTON/" class="post-title-link" itemprop="url">Paper-CV+CP-VTON</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-11 15:40:47" itemprop="dateCreated datePublished" datetime="2019-11-11T15:40:47+00:00">2019-11-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/" itemprop="url" rel="index"><span itemprop="name">CV_Apply</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/VTON/" itemprop="url" rel="index"><span itemprop="name">VTON</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[toc]</p>
<h2 id="CP-VTON"><a href="#CP-VTON" class="headerlink" title="CP-VTON"></a>CP-VTON</h2><p>1807.07688</p>
<p><a target="_blank" rel="noopener" href="https://github.com/sergeywong/cp-vton">https://github.com/sergeywong/cp-vton</a></p>
<ul>
<li>解决了在真实的虚拟试穿情况下面临的在大空间变形时的服装细节的保留问题。</li>
<li>通过GMM模块整合了全学习的TPS，用来获得更健壮和更强大的对齐图像。</li>
<li>在给定对齐图像的基础上，通过Try-On模块来动态合并渲染结果与变形结果。</li>
<li>CP-VTON网络的性能已经在Han等人收集的数据集上进行了证明。</li>
</ul>
<h2 id="Motivation："><a href="#Motivation：" class="headerlink" title="Motivation："></a>Motivation：</h2><p>“在保留目标服装细节的情况下将服装转换为适合目标人物的体型”</p>
<ul>
<li><p>几何匹配模块GMM，将目标服装转换为适合目标人物体型的形状</p>
</li>
<li><p>Try-On模块将变形后的服装与人物整合并渲染整合后的图像</p>
</li>
</ul>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>alignment Network 对齐网络</p>
<p>a single pass generative framework 单通道生成框架</p>
<p><img src="/2019/11/11/CV/CV_VTON/Paper-CV+CP-VTON/1573474568644.png" alt="1573474568644"></p>
<h3 id="GMM模型-几何匹配模块"><a href="#GMM模型-几何匹配模块" class="headerlink" title="GMM模型 几何匹配模块"></a>GMM模型 几何匹配模块</h3><p>Geometric Matching Moduel</p>
<p><strong>extracting network:</strong></p>
<ul>
<li><p><input checked disabled type="checkbox"> 
IN_cloth-agnostic_person [h, w, 22]  (64, 128, 256, 512, 512 )</p>
</li>
<li><p><input checked disabled type="checkbox"> 
IN_in_shop_cloth [h, w, 3]  (64, 128, 256, 512, 512 )</p>
</li>
</ul>
<p><strong>correlation Matching:</strong></p>
<ul>
<li><input disabled type="checkbox"> 矩阵变换</li>
</ul>
<p><strong>regression network:</strong></p>
<ul>
<li><input checked disabled type="checkbox"> CNN (512, 256, 128, 64 )</li>
</ul>
<p><strong>TPS transformation:</strong></p>
<ul>
<li><input disabled type="checkbox"> TPS 理解</li>
</ul>
<p><strong>LOSS</strong></p>
<ul>
<li><input checked disabled type="checkbox"> l1_loss</li>
</ul>
<p><strong>生成文件</strong></p>
<ul>
<li>warped_cloth</li>
<li>warped_mask</li>
</ul>
<p><img src="/2019/11/11/CV/CV_VTON/Paper-CV+CP-VTON/1573810852348.png" alt="1573810852348"></p>
<h3 id="Tron-ON-Model"><a href="#Tron-ON-Model" class="headerlink" title="Tron-ON Model"></a>Tron-ON Model</h3><p><strong>GAN:</strong></p>
<ul>
<li><input checked disabled type="checkbox"> 12-layer UNet（输出Render 和 composite）</li>
</ul>
<p><strong>MaskComposition:</strong></p>
<ul>
<li><input checked disabled type="checkbox"> Mask Composition 实现：直接使用 cloth * m_composite + p_rendered * (1 - m_composite)</li>
</ul>
<p><strong>LOSS</strong></p>
<ul>
<li><input checked disabled type="checkbox"> L1loss_tryon_im</li>
<li><input checked disabled type="checkbox"> Lvgg_tryon_im</li>
<li><input checked disabled type="checkbox"> L1loss_composition</li>
</ul>
<p>训练时的Vision</p>
<p><img src="/2019/11/11/CV/CV_VTON/Paper-CV+CP-VTON/1573810017024.png" alt="1573810017024"></p>
<h2 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h2><h4 id="原始数据及格式："><a href="#原始数据及格式：" class="headerlink" title="原始数据及格式："></a>原始数据及格式：</h4><p>image</p>
<p>image_parse</p>
<p>warped_img 【处理image_parse数据二次处理后得到】</p>
<p>Pose【通过OpenPose对refer image处理得到 keypoint文件，二次处理】</p>
<p>Cloth</p>
<p>Cloth-Mask</p>
<p><img src="/2019/11/11/CV/CV_VTON/Paper-CV+CP-VTON/1573784884416.png" alt="1573784884416"></p>
<h4 id="转换后数据格式："><a href="#转换后数据格式：" class="headerlink" title="转换后数据格式："></a>转换后数据格式：</h4><p>cloth-<strong>agnostic</strong>_person（22通道）：</p>
<ul>
<li>Pose heatmap: an 18-channel 每个通道对应一个人体姿势关键点（绘制为11×1111×11的白色矩形）。</li>
<li>Body shape: a 1-channel  一个1通道的 blurred binary mask 特征图，能够粗糙地包括人体的不同部位。</li>
<li>Reserved regions: an RGB image 一个包括面部和头发的RGB图像，用来维持人物身份（保证生成的是同一个人）。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shape 	 &lt;- image-parse</span><br><span class="line">im_h 	 &lt;- [Regions Part]</span><br><span class="line">pose_map &lt;- [_keypoints.json]</span><br><span class="line"></span><br><span class="line">agnostic =  [shape,  im_h, pose_map]</span><br></pre></td></tr></table></figure>



<h2 id="数据转换-难点理解："><a href="#数据转换-难点理解：" class="headerlink" title="数据转换-难点理解："></a>数据转换-难点理解：</h2><h3 id="0、VITON"><a href="#0、VITON" class="headerlink" title="0、VITON"></a>0、VITON</h3><p>解析VITON数据集合得到的数据有：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x27;viton_train_pairs.txt&#x27;: imname, cloth_name</span><br><span class="line">women_top/             ===&gt; cloth</span><br><span class="line">women_top/ +double + imfill + medfilt2 ===&gt; cloth-mask</span><br><span class="line">women_top/			   ===&gt; image</span><br><span class="line">/VITON/segment/ + cmap ===&gt; image-parse</span><br><span class="line">/VITON/pose/pkl 		   ===&gt; pose</span><br></pre></td></tr></table></figure>

<p>human_colormap.mat 数据集</p>
<h3 id="1、keypoints文件生成"><a href="#1、keypoints文件生成" class="headerlink" title="1、keypoints文件生成"></a>1、keypoints文件生成</h3><p>首先原始数据取自VITON</p>
<p>CP-VTON use the json format for pose info as generated by <a target="_blank" rel="noopener" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a>.</p>
<p>我们使用[OpenPose](<a target="_blank" rel="noopener" href="https://github.com/cmu-computing">https://github.com/cmu-computing</a> - lab&#x2F;openpose)生成json格式的pose信息。</p>
<h3 id="2、Image-Parse文件生成"><a href="#2、Image-Parse文件生成" class="headerlink" title="2、Image-Parse文件生成"></a>2、Image-Parse文件生成</h3><ul>
<li>LIP-SSL得到.mat文件， 保存到&#x2F;VITON&#x2F;segment&#x2F;</li>
<li>转换VITON的<strong>human_colormap.mat</strong>文件得到</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">image_parse</span><span class="params">()</span></span></span><br><span class="line">        source_root_dir = <span class="string">&#x27;F:/BaiduNetdiskDownload/viton_resize/train&#x27;</span>;</span><br><span class="line">        iname = <span class="string">&#x27;000003_0.jpg&#x27;</span>;</span><br><span class="line">        cname = <span class="string">&#x27;000003_1.jpg&#x27;</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">% generate parsing result</span></span><br><span class="line">		im = imread([source_root_dir <span class="string">&#x27;/&#x27;</span> <span class="string">&#x27;women_top/&#x27;</span> imname]);</span><br><span class="line">		h = <span class="built_in">size</span>(im,<span class="number">1</span>);</span><br><span class="line">		w = <span class="built_in">size</span>(im,<span class="number">2</span>);</span><br><span class="line">        s_name = strrep(imname,<span class="string">&#x27;.jpg&#x27;</span>,<span class="string">&#x27;.mat&#x27;</span>);</span><br><span class="line">        segment = importdata([source_root_dir <span class="string">&#x27;/&#x27;</span> <span class="string">&#x27;segment/&#x27;</span> s_name]);</span><br><span class="line">		segment = segment&#x27;;</span><br><span class="line">		</span><br><span class="line">		# 图片裁剪</span><br><span class="line">	    <span class="keyword">if</span> h &gt; w</span><br><span class="line">	        segment = segment(:,<span class="number">1</span>:int32(<span class="number">641.0</span>*w/h));</span><br><span class="line">	    <span class="keyword">else</span></span><br><span class="line">	        segment = segment(<span class="number">1</span>:int32(<span class="number">641.8</span>*h/w),:);</span><br><span class="line">	    <span class="keyword">end</span></span><br><span class="line">	    segment = imresize(segment, [h,w], <span class="string">&#x27;nearest&#x27;</span>);</span><br><span class="line">        </span><br><span class="line">        segment = imresize(segment, [fine_height, fine_width], <span class="string">&#x27;nearest&#x27;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">% save parsing result</span></span><br><span class="line">	    segment = uint8(segment);</span><br><span class="line">	    pname = strrep(imname, <span class="string">&#x27;.jpg&#x27;</span>, <span class="string">&#x27;.png&#x27;</span>);</span><br><span class="line">	    imwrite(segment,cmap,[target_root_dir <span class="string">&#x27;/&#x27;</span> mode <span class="string">&#x27;/image-parse/&#x27;</span> pname]);</span><br><span class="line"><span class="keyword">end</span>        </span><br></pre></td></tr></table></figure>



<h3 id="3、Cloth-Mask文件生成"><a href="#3、Cloth-Mask文件生成" class="headerlink" title="3、Cloth-Mask文件生成"></a>3、Cloth-Mask文件生成</h3><p>cloth 文件生成</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">conver_data</span><span class="params">()</span></span></span><br><span class="line">    source_root_dir = <span class="string">&#x27;F:/BaiduNetdiskDownload/viton_resize/train&#x27;</span>;</span><br><span class="line">    cname = <span class="string">&#x27;000003_1.jpg&#x27;</span>;</span><br><span class="line">    target_root_dir = <span class="string">&#x27;F:/BaiduNetdiskDownload/viton_resize_qs/train&#x27;</span>;</span><br><span class="line">    im_c = imread([source_root_dir <span class="string">&#x27;/&#x27;</span> <span class="string">&#x27;cloth/&#x27;</span> cname]);</span><br><span class="line">    <span class="comment">% save cloth mask</span></span><br><span class="line">    mask = double((im_c(:,:,<span class="number">1</span>) &lt;= <span class="number">250</span>) &amp; (im_c(:,:,<span class="number">2</span>) &lt;= <span class="number">250</span>) &amp; (im_c(:,:,<span class="number">3</span>) &lt;= <span class="number">250</span>));</span><br><span class="line">    mask = imfill(mask);</span><br><span class="line">    mask = medfilt2(mask);</span><br><span class="line">    imwrite(mask, [target_root_dir <span class="string">&#x27;/cloth-mask/&#x27;</span> cname]);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>

<h3 id="4、Body-shape裁剪-训练时转换"><a href="#4、Body-shape裁剪-训练时转换" class="headerlink" title="4、Body shape裁剪 [训练时转换]"></a>4、Body shape裁剪 [训练时转换]</h3><h3 id="5、保留区域的转换-训练时转换"><a href="#5、保留区域的转换-训练时转换" class="headerlink" title="5、保留区域的转换 [训练时转换]"></a>5、保留区域的转换 [训练时转换]</h3><h3 id="6、Pose-heatmap-生成"><a href="#6、Pose-heatmap-生成" class="headerlink" title="6、Pose heatmap 生成"></a>6、Pose heatmap 生成</h3><p>ViTON-  采用OpenPose开源库，直接从源文件img生成。</p>
<h2 id="Reference："><a href="#Reference：" class="headerlink" title="Reference："></a>Reference：</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/aldy56/p/9956160.html">论文笔记MG-VTON</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/11/11/CV/CV_VTON/Paper-CV+VTON/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/11/CV/CV_VTON/Paper-CV+VTON/" class="post-title-link" itemprop="url">Paper-CV+VTON</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-11 15:40:25" itemprop="dateCreated datePublished" datetime="2019-11-11T15:40:25+00:00">2019-11-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/" itemprop="url" rel="index"><span itemprop="name">CV_Apply</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/VTON/" itemprop="url" rel="index"><span itemprop="name">VTON</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p><a target="_blank" rel="noopener" href="https://github.com/xthan/VITON">VTON code</a></p>
<p>1711.08447 [VITON] An Image-based Virtual Try-on Network.pdf</p>
<h2 id="VITON：基于映像的虚拟试穿网络"><a href="#VITON：基于映像的虚拟试穿网络" class="headerlink" title="VITON：基于映像的虚拟试穿网络"></a>VITON：基于映像的虚拟试穿网络</h2><p>CVPR 2018论文“ VITON：基于图像的虚拟试穿网络”的代码和数据集</p>
<h3 id="人像提取"><a href="#人像提取" class="headerlink" title="人像提取"></a>人像提取</h3><p>本文中使用的人物表示由2D姿态估计器和人工解析器提取：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation">实时多人姿势估计</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/Engineering-Course/LIP_SSL">自我监督的结构敏感学习</a></li>
</ul>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>由于版权问题，该数据集不再公开可用。对于已经下载数据集的用户，请注意，使用或分发数据集是非法的！</p>
<p>This dataset is crawled from women’s tops on <a target="_blank" rel="noopener" href="https://www.zalando.co.uk/womens-clothing-tops/">Zalando</a>. </p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h4><p>在<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1qFU4KmvnEr4CwEFXQZS_6Ebw5dPJAE21?usp=sharing">Google云端硬盘</a>上下载经过预训练的模型。将它们放在<code>model/</code>文件夹下。</p>
<p>运行<code>test_stage1.sh</code>以进行推断。结果在中<code>results/stage1/images/</code>。<code>results/stage1/index.html</code>可视化结果。</p>
<h4 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h4><p>运行matlab脚本<code>shape_context_warp.m</code>以提取TPS转换控制点。</p>
<p>然后<code>test_stage2.sh</code>进行优化并生成最终结果，该结果位于中<code>results/stage2/images/</code>。<code>results/stage2/index.html</code>可视化结果。</p>
<h3 id="培养"><a href="#培养" class="headerlink" title="培养"></a>培养</h3><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><p>往里走<code>prepare_data</code>。</p>
<p>首先运行<code>extract_tps.m</code>。这将需要一些时间，您可以尝试并行运行它，也可以直接通过Google云端硬盘下载预先计算的TPS控制点，然后将其放入<code>data/tps/</code>。</p>
<p>然后运行<code>./preprocess_viton.sh</code>，生成的TF记录将位于中<code>prepare_data/tfrecord</code>。</p>
<h4 id="第一阶段-1"><a href="#第一阶段-1" class="headerlink" title="第一阶段"></a>第一阶段</h4><p>跑 <code>train_stage1.sh</code></p>
<h4 id="第二阶段-1"><a href="#第二阶段-1" class="headerlink" title="第二阶段"></a>第二阶段</h4><p>跑 <code>train_stage2.sh</code></p>
<h3 id="引文"><a href="#引文" class="headerlink" title="引文"></a>引文</h3><p>如果此代码或数据集有助于您的研究，请引用我们的论文：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@inproceedings&#123;han2017viton,</span><br><span class="line">  title = &#123;VITON: An Image-based Virtual Try-on Network&#125;,</span><br><span class="line">  author = &#123;Han, Xintong and Wu, Zuxuan and Wu, Zhe and Yu, Ruichi and Davis, Larry S&#125;,</span><br><span class="line">  booktitle = &#123;CVPR&#125;,</span><br><span class="line">  year  = &#123;2018&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://github.com/xthan/VITON">https://github.com/xthan/VITON</a></p>
<h2 id="Paper-Viton-札记"><a href="#Paper-Viton-札记" class="headerlink" title="Paper Viton[札记]"></a>Paper Viton[札记]</h2><p>VITON用一个由粗到细的框架解决了这个问题，并期望通过TPS变形来捕捉布料的变形。</p>
<p>An overview of VITON.  </p>
<p><img src="/2019/11/11/CV/CV_VTON/Paper-CV+VTON/1573543133654.png" alt="1573543133654"></p>
<h2 id="VITON-Paper"><a href="#VITON-Paper" class="headerlink" title="VITON Paper:"></a>VITON Paper:</h2><p>VITON 的目标在于，对给定的参考图像（模特）I和目标衣服c，生成合成图像I霸；I霸中c被自然地“穿”到参考图像I中模特对应的区域上，而模特的姿势和其他身体部位特征被保留。最直接的方法是用这样的训练集：同一个模特（姿势和人体特征相同）穿着不同衣服的多张图片和对应的这些衣服的产品图。但是这种数据集是很难大规模得到的。<br>在实际虚拟试衣场景中，在测试时只有参考图像和目标产品的图像是可以得到的。因此，我们把这种设定同样运用到训练集中来，所以输入的参考图像I中模特穿的衣服就是目标衣服c，这样的数据集是易得的（就像某宝上卖衣服，不仅给产品图还要给卖家秀)。那么现在的重点就是，给定c和模特人体信息，怎么训练一个网络-&gt;不仅可以生成合成图像，更重要的是要在测试中能够泛化，用任意所需的服装项目合成感知上令人信服的图像。</p>
<h3 id="3-Person-Representation-人体特征表示"><a href="#3-Person-Representation-人体特征表示" class="headerlink" title="3. Person Representation 人体特征表示"></a>3. Person Representation 人体特征表示</h3><h5 id="3-1-Pose-heatmap-姿势热图"><a href="#3-1-Pose-heatmap-姿势热图" class="headerlink" title="3.1 Pose heatmap 姿势热图"></a>3.1 Pose heatmap 姿势热图</h5><p>人体姿势的多样性导致了衣服不同程度的变形，因此我们使用最先进的<strong>姿势估计器</strong>明确地<strong>建模姿势信息</strong>。（用的是CVPR2017的人体姿势估计Realtime Multi-Person Pose Estimation）人体姿势估计包含了18个关键点，为了使Represent表示的各个部分可以空间堆叠，每个关键点被转换成热图heatmap，在关键点附近有一个11×11的邻居填充了1, 在其他地方填充0，然后把这些热图堆叠成一个18通道的姿势热图。</p>
<ul>
<li>Realtime Multi-Person Pose Estimation –》姿态信息， 18个关键点</li>
</ul>
<h5 id="3-2-Human-body-representation-人体身体部位表示"><a href="#3-2-Human-body-representation-人体身体部位表示" class="headerlink" title="3.2 Human body representation 人体身体部位表示"></a>3.2 Human body representation 人体身体部位表示</h5><p>衣服的外形很大程度上取决于人的形状（高矮胖瘦），因此如何将目标衣服进行变形取决于不同的身体部位（如手臂，躯干）和身体形态。一个先进的人体解析的方法（LIP-SSL，个人感觉LIP-SSP比较难跑，caffe环境配置复杂，建议跑它的另一个版本JPPNet，不过要是想得到.mat要自己改一下输出）输出人体分割图（.mat格式，不同分割区域标记了不同编号）。然后我们把这个分割图转换成一个单通道的二进制掩模，其中1代表身体部分，0表示其他部位。这个二进制掩模呗下采样成一个低分辨率的图（16*12），当身体形状和目标衣服混淆时，避免伪影。</p>
<ul>
<li>LIP-SSL 人体解析算法，生成人体分割图.mat文件</li>
<li>JPPNET(option)</li>
</ul>
<h5 id="3-3-Face-and-hair-segment-脸部、头发的分割"><a href="#3-3-Face-and-hair-segment-脸部、头发的分割" class="headerlink" title="3.3 Face and hair segment 脸部、头发的分割"></a>3.3 Face and hair segment 脸部、头发的分割</h5><p>为了维持人本身的特点，我们在人体表示中融入了身体特征，如脸、肤色、头发之类。我们用Human body representation里一样的人体解析器分离出人脸和头发部分的RGB图，作为新合成图像的身份信息。</p>
<ul>
<li>LIP-SLL 分离人脸和头发部分的RGB图</li>
</ul>
<p>Pose + Body + Face &#x3D;&#x3D; (18 + 1 +  3)</p>
<p>最后，把以上得到的三个部分转换到统一分辨率，然后堆叠形成与衣物无关的人体表示P（256，192，22），示意图如下。P包含了丰富的人体信息，convNet就是基于这些信息学习它们之间的关系，比以往的person Presentation都精细。<br><img src="/2019/11/11/CV/CV_VTON/Paper-CV+VTON/20190525153625478.png" alt="在这里插入图片描述"></p>
<h3 id="Multi-task-Encoder-Decoder-Generator-Gc"><a href="#Multi-task-Encoder-Decoder-Generator-Gc" class="headerlink" title="Multi-task Encoder-Decoder Generator -&gt; Gc"></a>Multi-task Encoder-Decoder Generator -&gt; Gc</h3><p>输入：Person Representation P和目标衣服Target Clothing C<br>        输出：粗合成I’和对应区域掩模M<br>        Gc：这里用到的编码解码框架是一种包含连接跳跃skip connections的U-net网络结构。<br>设Gc为编码解码器估计的方程，它将P和C组合作为输入并输出四通道（I‘和M）结果。我们希望得到一个生成器，使得粗合成图I’尽可能接近参考图I，掩模M尽可能接近从参考图I模特上预测得到的伪ground truth掩模M0。一个简单的方法就是用L1损失，使当输出目标是M0这样的二进制掩模时，它会产生不错的结果。但是如果输出要是RGB图像的话，L1 Loss就会使输出图像变模糊。我们还引入了a perceptual loss感知损失。下式是感知损失的和以及LI Loss加和的结果。<br><img src="/2019/11/11/CV/CV_VTON/Paper-CV+VTON/20190525163224192.png" alt="在这里插入图片描述"><br>网络用ImageNet数据集训练的VGG19权重初始化。超参数lamda_i控制第i层的贡献。第一个Stage的Gc没有学习细节（L1 Loss导致），所以得到的是粗合成图。</p>
<h3 id="Refinement-Network-GR"><a href="#Refinement-Network-GR" class="headerlink" title="Refinement Network -&gt; GR"></a>Refinement Network -&gt; GR</h3><p>是为了从目标衣服产品图中学习对应模糊图像区域的细节并恢复。</p>
<h4 id="Warped-clothing-item-衣服变形"><a href="#Warped-clothing-item-衣服变形" class="headerlink" title="Warped clothing item 衣服变形"></a>Warped clothing item 衣服变形</h4><p>为了运用目标衣服产品图中的纹理细节，要先对衣服进行变形。</p>
<p>a thin plate spline (TPS) transformation</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41971682/article/details/90549389"> copy from </a></p>
<h2 id="VITON-realtime"><a href="#VITON-realtime" class="headerlink" title="VITON_realtime"></a>VITON_realtime</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>The dataset can be downloaded on <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1-RIcmjQKTqsf3PZsoHT4hivNngx_3386?usp=sharing">Google Drive</a>.</p>
<p><strong>This dataset is crawled from women’s tops on</strong> <a target="_blank" rel="noopener" href="https://www.zalando.co.uk/womens-clothing-tops/">Zalando</a>. These images can be downloaded on Google Drive. The results of pose estimation and human parsing are also included. Note that number of the images&#x2F;poses&#x2F;segmentation maps are more than that reported in the paper, since the ones with bad pose estimations (too few keypoints are detected) or parsing results (parsed upper clothes regions only occupy a small portion of the image).</p>
<p>Put all folder and labels in the <code>data</code> folder:</p>
<p><code>data/women_top</code>: reference images (image name is ID_0.jpg) and clothing images (image name is ID_1.jpg). For example, the clothing image on reference image 000001_0.jpg is 000001_1.jpg. The resolution of these images is 1100x762.</p>
<p><code>data/pose.pkl</code>: a pickle file containing a dictionary of the pose keypoints of each reference image. Please refer to <a target="_blank" rel="noopener" href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation/blob/master/testing/python/demo.ipynb">this demo</a> for how to parse the stored results, and <a target="_blank" rel="noopener" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md">OpenPose output</a> to understand the output format. (字典文件保存：（pose keypoints， image）pose keypoints of each reference image)</p>
<p>包含每个参考图像的位姿关键点字典的pickle文件。请参考<a target="_blank" rel="noopener" href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation/blob/master/testing/python/demo.ipynb">这个演示</a>如何解析存储的结果，和<a target="_blank" rel="noopener" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md">OpenPose输出</a>来理解输出格式。</p>
<p><code>data/pose/</code>: folder containing the pose keypoints of each reference image.</p>
<p><code>data/segment</code>: folder containing the segmentation map of each reference image. In a segmentation map, label 5 corresponds to the regions of tops (used as pseudo ground truth of clothing region). label 1 (hat), 2 (hair), 4 (sunglasses), and 13 (face) are merged as the face&#x2F;hair representation. All other non-background regions are merged for extracting the body representation. The colormap for visualization can be downloaded <a target="_blank" rel="noopener" href="https://github.com/Engineering-Course/LIP_SSL/blob/master/human_colormap.mat">here</a>. Due to padding operations of the parser, these segmentation maps are 641x641, you need to crop them based on the aspect ratio of the original reference images.</p>
<p><code>data/tps/</code>: TPS control points between product image and its corresponding reference image.</p>
<p><code>data/viton_train_images.txt</code>: training image list.</p>
<p><code>data/viton_train_pairs.txt</code>: 14,221 reference image and clothing training image pairs.</p>
<p><code>data/viton_test_pairs.txt</code>: 2,032 reference image and target clothing testing image pairs. Note that these pairs are only used for the evaluation in our paper, one can choose any reference image and target clothing to generate the virtual try-on results.</p>
<p>问题1：（人体解析）segment的数据来源</p>
<p>​    LIP-SSL 人体解析，分离人脸，发等特征</p>
<p>问题2：（姿态评估）pose.pkl 文件格式，如何生成</p>
<p>​    see ##3.1. Person Representation</p>
<p>​    reference images的pose keypoints</p>
<p>​    Realtime_Multi-Person_Pose_Estimation 模型得到的人体姿态评估 18通道</p>
<p>问题3：（人体分割）pose结果的格式</p>
<p>​    LIP_SSL得到的人体Shape .mat 文件</p>
<p>问题4：pose keypoints文件的来源</p>
<p>​    见问题3</p>
<p>问题5：tps的数据格式</p>
<h3 id="替代技术选型："><a href="#替代技术选型：" class="headerlink" title="替代技术选型："></a>替代技术选型：</h3><p>Human Parse:      <a target="_blank" rel="noopener" href="https://github.com/llltttppp/SS-NAN">SS-NAN</a></p>
<p>Pose Estimator:   <a target="_blank" rel="noopener" href="https://github.com/ildoonet/tf-pose-estimation">TF-pose-estimation</a></p>
<h3 id="RUN-（How-to-run-）"><a href="#RUN-（How-to-run-）" class="headerlink" title="RUN （How to run?）"></a>RUN （How to run?）</h3><ol>
<li>Download related models</li>
</ol>
<ul>
<li><p>Download pretrained SS-NAN model <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1nvMMl0P">here</a>. Put AttResnet101FCN_lip_0023.h5 under SS-NAN&#x2F; folder.</p>
</li>
<li><p>Model of tf-pose-estimation is already in the repo since it could use mobile-net.</p>
</li>
<li><p>Download pretrained VITON models on <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1qFU4KmvnEr4CwEFXQZS_6Ebw5dPJAE21">Google Drive</a>. Put them under model&#x2F; folder.</p>
</li>
</ul>
<ol start="2">
<li><p>For remote server with GPU support, run the below for API server to deal with pose and segmentation inferrence:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yml</span><br><span class="line">source activate MakeNTU</span><br><span class="line">bash run_API_server.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>For local server, run the below to do VITON inferrence and avoid tensorflow session problem for concurrency:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yml</span><br><span class="line">source activate MakeNTU</span><br><span class="line">export FLASK_APP=VITON_local_server.py</span><br><span class="line">flask run</span><br></pre></td></tr></table></figure>
</li>
<li><p>Change settings in VITON_Demo_post:<br>Set VIDEO_SOURCE to your webcam number or video path.</p>
</li>
<li><p>Finally, run the main app:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export SEG_SERVER=&lt;IP address ofthe remote server, like http://192.168.0.123:8000&gt;</span><br><span class="line">export POSE_SERVER=&lt;IP address ofthe remote server, like http://192.168.0.123:8000&gt;</span><br><span class="line">export VITON_SERVER=&#x27;http://localhost:5000&#x27;</span><br><span class="line">source activate MakeNTU</span><br><span class="line">python VITON_Demo_post.py</span><br></pre></td></tr></table></figure></li>
</ol>
<p>Keyboard controls</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">q: to exit</span><br><span class="line">c: to capture image and do virtual try-on</span><br><span class="line">a/s/d/f: change clothes to try on</span><br></pre></td></tr></table></figure>

<p>Other files are for running all things locally or without concurrency.</p>
<p>One could also run <code>python post_viton.py</code> to run without local VITON server.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/11/06/CV/CV_VTON/Paper-CV+MG-VTON/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/06/CV/CV_VTON/Paper-CV+MG-VTON/" class="post-title-link" itemprop="url">Paper_CV+ MG-VTON</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-06 20:10:57" itemprop="dateCreated datePublished" datetime="2019-11-06T20:10:57+00:00">2019-11-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:38" itemprop="dateModified" datetime="2025-08-06T08:16:38+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/" itemprop="url" rel="index"><span itemprop="name">CV_Apply</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV-Apply/VTON/" itemprop="url" rel="index"><span itemprop="name">VTON</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h2 id="MG-VTON-《Towards-Multi-pose-Guided-Virtual-Try-on-Network-》"><a href="#MG-VTON-《Towards-Multi-pose-Guided-Virtual-Try-on-Network-》" class="headerlink" title="MG-VTON  《Towards Multi-pose Guided Virtual Try-on Network 》"></a>MG-VTON  《Towards Multi-pose Guided Virtual Try-on Network 》</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.11026">https://arxiv.org/abs/1902.11026</a></p>
<p>Virtual try-on system 虚拟试穿系统。</p>
<p>—现有的方法，只能从单个固定的人体姿态上换装。往往失去纹理细节，缺少姿态多样性。</p>
<p>three Stages:</p>
<ol>
<li><p>a desired human parsing map of the target image is synthesized to match both the desired pose and the desired clothes shape; 合成目标图像的所需人工解析映射，以匹配所需的姿态和所需的衣服形状</p>
</li>
<li><p>a deep Warping Generative Adversarial Network (Warp-GAN) warps the desired clothes appearance into the synthesized human parsing map and alleviates the misalignment problem between the input human pose and desired human pose; 深度扭曲生成对抗网络(Warp-GAN)将期望的服装外观扭曲到合成的人体解析图中，解决了输入人体姿态与期望人体姿态之间的错位问题</p>
</li>
<li><p>a refinement render utilizing multi-pose composition masks recovers the texture details of clothes and removes some artifacts. Extensive experiments on well-known datasets and our newly collected largest virtual try-on benchmark demonstrate that our MGVTON significantly outperforms all state-of-the-art  methods both qualitatively and quantitatively with promising multipose virtual try-on performances. 使用多姿态合成蒙版的细化渲染恢复衣服的纹理细节，并删除一些人工制品.在知名数据集上的大量实验和我们新收集的最大的虚拟试测基准表明，我们的MGVTON在定性和定量上都显著优于所有最先进的方法，具有很有前途的多姿态虚拟试测性能。</p>
</li>
</ol>
<p><strong>Key:</strong></p>
<p>数据(人工标记)：姿态，衣服形状</p>
<p>数据集: well-know【DeepFashion】 &amp; self conllected largest</p>
<p>网络架构：Warp-GAN</p>
<p>技术储备：多姿态合成蒙版（一种利用多姿态合成掩模来恢复纹理细节和减少伪影的细化网络）</p>
<p><strong>Apply:</strong> 虚拟试穿、虚拟现实和人机交互</p>
<p>Other exiting works [14, 20, 35] usually leverage 3D measurements  to solve those issues since the 3D information have abundant details of the shape of the body that can help to generate the realistic results. However, it needs expert knowledge and huge labor cost to build the 3D models, which requires collecting the 3D annotated data and massive computation. These costs and complexity would limit the applications in the practical virtual try-on simulation. 3D建模，这些费用和复杂性限制了虚拟试验仿真的实际应用。</p>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset:"></a>Dataset:</h2><p><strong>MPV:</strong></p>
<p>​    collect from the internet, named MPV ;</p>
<p>​    contains 35,687 person images and 13,524 clothes images.  </p>
<p>​    The image is in the resolution of 256 × 192. We extract the 62,780 three-tuples of the same person in the same clothes but with diverse poses .</p>
<p>​    图片大小265x192，提取了62780组图片，每组含同一人的不同姿势的三张图片。</p>
<h2 id="MG-VTON"><a href="#MG-VTON" class="headerlink" title="MG_VTON"></a>MG_VTON</h2><p>Picture  + clothes + pose -&gt; 穿新衣的人物Pose照片</p>
<p>MG-VTON 四个组成部分：</p>
<p>1- a pose-clothes-guided human parsing network is designed to guide the image synthesis;  设计了一种基于服装姿态引导的人工解析网络来指导图像合成;</p>
<p>2- a Warp-GAN learns to synthesized realistic image by using a warping features strategy。GAN学习合成真实图像。</p>
<p>3- a refinement network learns to recover the texture details </p>
<p>4- a mask-based geometric matching network is presented to warp clothes that enhances the visual quality of the generated image 提出了一种基于掩模的几何匹配网络，通过对衣服的变形来提高图像的视觉质量。</p>
<p>我们采用了一个“粗-精”策略，将这个任务分成三个子任务，three subtasks ：</p>
<ul>
<li><p><input disabled type="checkbox"> 
conditional parsing learning,(条件解析学习)</p>
</li>
<li><p><input disabled type="checkbox"> 
the Warp-GAN, </p>
</li>
<li><p><input disabled type="checkbox"> 
the refinement render.  （细化纹理）</p>
</li>
</ul>
<p><img src="/2019/11/06/CV/CV_VTON/Paper-CV+MG-VTON/1573111435377.png" alt="OverView MG-VTON"></p>
<p>姿态编码：使用pose estimator [4]  ，我们将姿态编码为18个热图，其中填充一个半径为4像素的圆，其他地方为0。</p>
<p>使用人解析器[6]来预测由20个标签组成的人类分割地图，从中提取面部、头发和身体形状的二进制掩码</p>
<p>根据VITON[8]，我们将身体形状的采样降低到一个较低的分辨率(16×12)，并直接将其调整到原始分辨率(256×192)，这减轻了由于身体形状的变化所造成的人为影响</p>
<p><img src="/2019/11/06/CV/CV_VTON/Paper-CV+MG-VTON/1573117982297.png" alt="1573117982297"></p>
<p>Architecture </p>
<h3 id="3-1-Conditional-Parsing-Learning-【-人体解析-语义分割】"><a href="#3-1-Conditional-Parsing-Learning-【-人体解析-语义分割】" class="headerlink" title="3.1. Conditional Parsing Learning 【~人体解析&#x2F;语义分割】"></a>3.1. Conditional Parsing Learning 【~人体解析&#x2F;语义分割】</h3><p><img src="/2019/11/06/CV/CV_VTON/Paper-CV+MG-VTON/1573203694348.png" alt="1573203694348"></p>
<p>L1-loss 产生更平滑的结果</p>
<p>softmax_loss 合成高质量的人工Parsing Map</p>
<p>IN: (ImageOfClothes, postHeatMap, body shape, mask Hair, mask Face)</p>
<p>p(St’|(Mh, Mf, Mb, C, P))</p>
<p>G: 该阶段基于条件生成对抗网络(CGAN)</p>
<p>D: We adopt the discriminator D directly from the pix2pixHD  </p>
<p>OUT: Parsing</p>
<h3 id="3-2-Warp-GAN-【-Fake】"><a href="#3-2-Warp-GAN-【-Fake】" class="headerlink" title="3.2. Warp-GAN 【~Fake】"></a>3.2. Warp-GAN 【~Fake】</h3><p><img src="/2019/11/06/CV/CV_VTON/Paper-CV+MG-VTON/1573203679896.png" alt="1573203679896"></p>
<p>[geometric matching](《Convolutional neural network architecture for geometric matching》) module to warp clothes image<a href="###3.4">3.4</a> </p>
<h3 id="3-3-Refinement-render-【-高像素修复】"><a href="#3-3-Refinement-render-【-高像素修复】" class="headerlink" title="3.3. Refinement render 【~高像素修复】"></a>3.3. Refinement render 【~高像素修复】</h3><p><img src="/2019/11/06/CV/CV_VTON/Paper-CV+MG-VTON/1573203710267.png" alt="1573203710267"></p>
<h3 id="3-4-Geometric-matching-learning-【-】"><a href="#3-4-Geometric-matching-learning-【-】" class="headerlink" title="3.4. Geometric matching learning 【~】"></a>3.4. Geometric matching learning 【~】</h3><p><img src="/2019/11/06/CV/CV_VTON/Paper-CV+MG-VTON/1573203724114.png" alt="1573203724114"></p>
<h2 id="Implementation-Detail"><a href="#Implementation-Detail" class="headerlink" title="Implementation Detail:"></a>Implementation Detail:</h2><p><strong>Setting.</strong> We train the conditional parsing network, WarpGAN, refinement render, and geometric matching network for 200, 15, 5, 35 epochs, respectively, using ADAM optimizer [13], with the batch size of 40, learning rate of 0.0002, β1 &#x3D; 0:5, β2 &#x3D; 0:999. We use two NVIDIA Titan XP GPUs and Pytorch platform on Ubuntu 14.04. </p>
<p><strong>Architecture.</strong> </p>
<p>each <strong>generator</strong> of MG-VTON is a ResNet-like network, which consists of three  downsample layers, three upsample layers, and nine residual blocks, each block has three convolutional layers with 3x3 filter kernels followed by the bath-norm layer and Relu activation function.  </p>
<p>64, 128, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 256, 128, 64. </p>
<p><strong>discriminator</strong>  we apply the same architecture as <strong>pix2pixHD</strong> [30], which can handle the feature map<br>in different scale with different layers. Each discriminator contains four downsample layers which include 4x4 kernels, InstanceNorm, and LeakyReLU activation function. </p>
<h2 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h2><h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><h3 id="Person-image-synthesis-（人像合成）"><a href="#Person-image-synthesis-（人像合成）" class="headerlink" title="Person image synthesis （人像合成）"></a>Person image synthesis （人像合成）</h3><p>骨架辅助[32]提出了一种骨架导向的人体图像生成方法，该方法以人体图像和目标骨骼为条件。</p>
<p>PG2[17]应用了由粗到细的框架，包括粗级和精级。novel model [18] to further improve the quality of result by using a decomposition strategy（分解策略）</p>
<p>deformableGANs[27]和[1]分别在粗糙矩形区域使用仿射变换和在pixellevel上对零件进行翘曲，试图缓解不同位姿之间的错位问题</p>
<p>V-UNET[5]引入了一个变化的U-Net[24]，通过用stickman标签重组形状来合成人的图像</p>
<p>[21]直接应用CycleGAN[36]进行位姿操作</p>
<p>However, all those works fail to preserve the texture details consistency corresponding with the pose. 然而，所有这些作品都未能保持与姿态相对应的纹理细节的一致性。</p>
<p>The reason behind that is they ignore to consider the interplay between the human parsing map and the pose in the person image synthesis。</p>
<p>这背后的原因是他们忽略了人解析映射和人图像合成中的姿态之间的相互作用。</p>
<p>人体解析图可以指导生成器在精确的区域级合成图像，保证了人体结构的一致性</p>
<h3 id="Virtual-try-on"><a href="#Virtual-try-on" class="headerlink" title="Virtual try-on."></a>Virtual try-on.</h3><p><strong>fiexd pose</strong>:</p>
<p>VITON[8]  computed the transformation mapping by the shape context TPS warps [2]  </p>
<p>CP-VTON[29] 估计变换参数的学习方法。</p>
<p>FanshionGAN[37] 学会了在输入图像的基础上生成新衣服，这个人以描述不同衣服的句子为条件</p>
<p>ClothNet[15]提出了一种基于图像的生成模型，根据颜色生成新衣服。</p>
<p>CAGAN[10]提出了一个条件类比网络来合成以衣服配对为条件的人的图像，这限制了实际的虚拟试穿场景。</p>
<p>ClothCap[20]利用3D扫描仪自动捕捉衣服和身体的形状</p>
<p>[26]提出了一种需要三维人体形态的虚拟试衣系统，对注释的采集十分繁琐。</p>
<p>—-learning to synthesize image with the new outfit on the person through adversarial learning , which can manipulate the pose simultaneously. </p>
<p>—-使用对抗学习用新衣服合成图像, 同时控制姿势</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/10/29/Games/Paper-Game-Bridge/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/29/Games/Paper-Game-Bridge/" class="post-title-link" itemprop="url">Paper_game_Bridge</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-29 16:26:46" itemprop="dateCreated datePublished" datetime="2019-10-29T16:26:46+00:00">2019-10-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:39" itemprop="dateModified" datetime="2025-08-06T08:16:39+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Game/" itemprop="url" rel="index"><span itemprop="name">Game</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Game/Imperfect-Information-Game/" itemprop="url" rel="index"><span itemprop="name">Imperfect Information Game</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Game/Imperfect-Information-Game/Bridge/" itemprop="url" rel="index"><span itemprop="name">Bridge</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[toc]</p>
<h2 id="Contract-Bridge"><a href="#Contract-Bridge" class="headerlink" title="Contract Bridge"></a>Contract Bridge</h2><p>trick-taking card game .</p>
<p>由一个叫牌阶段和一个打牌阶段组成</p>
<p>opponent(s) and partner </p>
<hr>
<p>规则：同Skat类似分两个阶段，叫分和出牌阶段</p>
<p>4人，两两一组，没人13张牌，共计52张</p>
<p>打13轮，每轮一张（与Skat的出牌规则一致），赢一轮则称之为赢一墩。基本局要赢6墩。</p>
<p>无将（最高）、黑桃、红桃、方片、梅花（最低）</p>
<p>叫分：NT； 1H(赢7墩), 2C（赢8墩）…</p>
<p>出牌：同花色比大小，（有将牌，奖牌大）；最大队赢一墩。</p>
<h3 id="GIB"><a href="#GIB" class="headerlink" title="GIB"></a>GIB</h3><p>Not Plan or mimic human</p>
<p>PIMC, a brute force approach</p>
<h3 id="PIMC"><a href="#PIMC" class="headerlink" title="PIMC:"></a>PIMC:</h3><p>当PIMC玩家move时，我们先创建一个虚拟的世界，对所有未观察到的变量赋值（当前游戏状态下）。</p>
<p>然后当成完全信息游戏PK。repeat 多次，选择平均效用最高的move。</p>
<hr>
<p>strategy fusion</p>
<p>non-locality</p>
<h2 id="Paper-GIB"><a href="#Paper-GIB" class="headerlink" title="Paper : GIB"></a>Paper : GIB</h2><h2 id="Paper-The-State-of-Automated-Bridge-Play"><a href="#Paper-The-State-of-Automated-Bridge-Play" class="headerlink" title="Paper :The State of Automated Bridge Play"></a>Paper :The State of Automated Bridge Play</h2><p>Paul M Bethe, NYU 2010 </p>
<h2 id="Paper-Automatic-Bridge-Bidding-Using-Deep-Reinforcement-Learning"><a href="#Paper-Automatic-Bridge-Bidding-Using-Deep-Reinforcement-Learning" class="headerlink" title="Paper : Automatic Bridge Bidding Using Deep Reinforcement Learning"></a>Paper : Automatic Bridge Bidding Using Deep Reinforcement Learning</h2><p>使用Q-Learning进行叫牌。</p>
<p><img src="/2019/10/29/Games/Paper-Game-Bridge/1572512512141.png" alt="1572512512141"></p>
<p>算法： 不考虑竞争？</p>
<p><img src="/2019/10/29/Games/Paper-Game-Bridge/1572509442777.png" alt="1572509442777"></p>
<h2 id="Paper-Competitive-Bridge-Bidding-with-Deep-Neural-Networks"><a href="#Paper-Competitive-Bridge-Bidding-with-Deep-Neural-Networks" class="headerlink" title="Paper : Competitive Bridge Bidding with Deep Neural Networks"></a>Paper : Competitive Bridge Bidding with Deep Neural Networks</h2><p>基于<strong>神经网络</strong>构建叫牌系统的方法。</p>
<p>桥牌游戏分为叫牌和打牌两个阶段。对计算机程序来说，虽然打牌相对容易，但叫牌是非常具有挑战性的。在叫牌阶段，每个玩家只知道自己牌，但同时，他需要在对手的干扰下与搭档交换信息。现有的解决完全信息博弈的方法不能直接应用于叫牌中。大多数桥牌程序都是基于人工设计的规则，但是，这些规则并不能覆盖所有的情况，而且，它们通常模棱两可甚至相互矛盾。本文首次提出了一种基于深度学习技术的叫牌系统，在文中，我们展示了两个创新点。首先，我们设计了一个紧凑的表示，对私人和公共信息进行编码，供玩家投标。第二，在分析其他玩家的未知牌对最终结果的影响的基础上，设计了<strong>两个神经网络</strong>来处理不完全信息，<strong>第一个神经网络推断出搭档的牌</strong>，第二个神经网络将第一个神经网络的输出作为其输入的一部分来<strong>选择叫牌</strong>。实验结果表明，我们的叫牌系统优于基于规则的最优方案。</p>
<p>思路：</p>
<p>​	预测队友的牌来缩小信息集，从而降低结果的不确定性。</p>
<p>​	SL + RL(Self-Play); 同一个团队的 2 个智能体会通过不断修正对队友牌面的预测准确性来保持信息交换和协作，同时与自己的历史版本对抗，在不断的学习中提升自己的胜率。</p>
<p>项目设计：</p>
<p>​	PI: 自己的牌，局况，叫牌序列</p>
<p>​	in-PI: 队友牌的预测</p>
<p>​	ENN: ($c_p,v, h, c_p+$); label $c_p+$ contains 13 ones and 39 zeros; sigmoid(52); 推断出搭档的牌</p>
<p>​	PNN: ($c_p, v, h, ϕ_w, (c_p, v, h), b_p$) ; softmax(39); 选择叫牌</p>
<p>算法：</p>
<p><img src="/2019/10/29/Games/Paper-Game-Bridge/1572429108140.png" alt="1572429108140"></p>
<p>成果:</p>
<p>​	在叫牌阶段超越了当前版本的 Wbridge5</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>《Search, Inference and Opponent Modelling in an Expert-Caliber <strong>Skat</strong> Player》</p>
<p>《GIB: Imperfect Information in a Computationally Challenging Game.》 M. Ginsberg.  Journal<br>of Artificial Intelligence Research, pages 303–358, 2001. </p>
<p>《The State of Automated Bridge Play 》Paul M Bethe, NYU 2010 </p>
<p>《Automatic Bridge Bidding Using Deep Reinforcement Learning 》Chih-Kuan Yeh 2016</p>
<p>《Competitive Bridge Bidding with Deep Neural Networks∗》Jiang Rong  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.00900v2">arxiv</a> AAMAS 2019 的 140 篇入选论文; <a target="_blank" rel="noopener" href="https://www.yanxishe.com/blogDetail/13392">论文采访解读</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/10/24/Games/Paper-Game-DeepSkack/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/24/Games/Paper-Game-DeepSkack/" class="post-title-link" itemprop="url">DeepStack to Texas Hold'em</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-24 17:32:31" itemprop="dateCreated datePublished" datetime="2019-10-24T17:32:31+00:00">2019-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:39" itemprop="dateModified" datetime="2025-08-06T08:16:39+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Game/" itemprop="url" rel="index"><span itemprop="name">Game</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Game/Imperfect-Information-Game/" itemprop="url" rel="index"><span itemprop="name">Imperfect Information Game</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Game/Imperfect-Information-Game/Texas/" itemprop="url" rel="index"><span itemprop="name">Texas</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[toc]</p>
<h2 id="DeepStack"><a href="#DeepStack" class="headerlink" title="DeepStack"></a>DeepStack</h2><p><img src="/2019/10/24/Games/Paper-Game-DeepSkack/1571971584952.png" alt="1571971584952"></p>
<p>图 1：HUNL 中公开树的一部分。</p>
<p><img src="/2019/10/24/Games/Paper-Game-DeepSkack/1571971689058.png" alt="1571971689058"></p>
<p>图2：DeepStack架构： </p>
<p>A: 公共树的推理，action probabilities for all cards</p>
<p>B: evaluation function: 使用NN，该网络以当前迭代的公开状态和范围作为输入，然后输出两个玩家的反事实价值。</p>
<p>C: 随机生成poker situations(pot size, board cards, ranges)，生成训练样本，供NN训练。</p>
<h4 id="Continual-re-solving-持续解决"><a href="#Continual-re-solving-持续解决" class="headerlink" title="Continual re-solving 持续解决"></a>Continual re-solving 持续解决</h4><h4 id="Limited-depth-lookahead-via-intuition-通过直觉实现有限深度前瞻"><a href="#Limited-depth-lookahead-via-intuition-通过直觉实现有限深度前瞻" class="headerlink" title="Limited depth lookahead via intuition 通过直觉实现有限深度前瞻"></a>Limited depth lookahead via intuition 通过直觉实现有限深度前瞻</h4><h4 id="Sound-reasoning-合理推理"><a href="#Sound-reasoning-合理推理" class="headerlink" title="Sound reasoning 合理推理"></a>Sound reasoning 合理推理</h4><p>DeepStack的深度有限的连续重算是可靠的。如果DeepStack的直觉是“好的”, 并且在每一个重新求解的步骤中都使用了“足够的”计算，那么DeepStack就扮演了一个任意接近于纳什均衡的近似。</p>
<h4 id="Sparse-lookahead-trees-稀疏前瞻树"><a href="#Sparse-lookahead-trees-稀疏前瞻树" class="headerlink" title="Sparse lookahead trees 稀疏前瞻树"></a>Sparse lookahead trees 稀疏前瞻树</h4><p>reduction in the number of actions.</p>
<h4 id="Relationship-to-heuristic-search-in-prefect-information-games"><a href="#Relationship-to-heuristic-search-in-prefect-information-games" class="headerlink" title="Relationship to heuristic search in prefect information games"></a>Relationship to heuristic search in prefect information games</h4><h4 id="Relationship-to-abstraction-based-approaches"><a href="#Relationship-to-abstraction-based-approaches" class="headerlink" title="Relationship to abstraction-based approaches"></a>Relationship to abstraction-based approaches</h4><h2 id="Deep-Counterfactual-Value-Networks"><a href="#Deep-Counterfactual-Value-Networks" class="headerlink" title="Deep Counterfactual Value Networks"></a>Deep Counterfactual Value Networks</h2><h4 id="Architecture-Train"><a href="#Architecture-Train" class="headerlink" title="Architecture &amp; Train:"></a>Architecture &amp; Train:</h4><p>两个NN；</p>
<p>Flop Network: 1 million randomly generated flop games. </p>
<p>Turn Network: 10 million randomly generated poker turn games. </p>
<p>一个辅助网络；在处理任何公共卡之前，使用一个辅助的值网络来加速早期动作的重新求解</p>
<p><img src="/2019/10/24/Games/Paper-Game-DeepSkack/1571911214323.png" alt="1571911214323"></p>
<p>输入：池底大小，公共牌，手牌范围（Player Card ranges）&#x3D;&gt; hand clusters。</p>
<p>输出：Zero-sum; Conterfactual values。</p>
<h2 id="Evaluating-DeepStack"><a href="#Evaluating-DeepStack" class="headerlink" title="Evaluating DeepStack"></a>Evaluating DeepStack</h2><p><img src="/2019/10/24/Games/Paper-Game-DeepSkack/1571992324228.png" alt="1571992324228"></p>
<h2 id="Online"><a href="#Online" class="headerlink" title="Online"></a>Online</h2><p>DeepStack 会在游戏的每一个节点重新计算一小段可能性的树，而不是提前算出整个博弈树。</p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p><img src="/2019/10/24/Games/Paper-Game-DeepSkack/1571971177717.png" alt="1571971177717"></p>
<p><img src="/2019/10/24/Games/Paper-Game-DeepSkack/1571971190822.png" alt="1571971190822"></p>
<p><img src="/2019/10/24/Games/Paper-Game-DeepSkack/1571971204916.png" alt="1571971204916"></p>
<p>reference</p>
<p><a target="_blank" rel="noopener" href="http://www.sohu.com/a/127773829_465975">重磅 | Science论文详解击败德扑职业玩家的DeepStack，Nature探讨其与Libratus的优劣</a></p>
<p><a target="_blank" rel="noopener" href="http://www.sohu.com/a/345097139_500659"><em>Pluribus</em> Science论文解读：打牌一时爽，一直打牌一直爽</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/10/15/Games/RL-NFSP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/15/Games/RL-NFSP/" class="post-title-link" itemprop="url">RL_NFSP</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-15 15:56:06" itemprop="dateCreated datePublished" datetime="2019-10-15T15:56:06+00:00">2019-10-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:39" itemprop="dateModified" datetime="2025-08-06T08:16:39+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/RL/" itemprop="url" rel="index"><span itemprop="name">RL</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="Papers"><a href="#Papers" class="headerlink" title="Papers:"></a>Papers:</h1><p>2015 <a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v37/heinrich15.pdf">SFP</a>《Fictitious self-play in extensive-form games》 Heinrich et al. (2015)   UCL&amp;&amp;DeepMind</p>
<p>2016 [NFSP]《Deep Reinforcement Learning from Self-Play in Imperfect-Information Games 》 UCL: Johannes Heinrich </p>
<p>2019 [MC-NFSP]《Monte Carlo Neural Fictitious Self-Play: Approach to Approximate Nash Equilibrium of Imperfect-Information Games? 》 ZJU: Li Zhang </p>
<h2 id="Background："><a href="#Background：" class="headerlink" title="Background："></a>Background：</h2><p>Extensive-form Game:</p>
<p>​    扩展形式游戏是一种涉及多个代理的顺序交互模型</p>
<p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%8D%9A%E5%BC%88%E8%AE%BA">博弈论</a>中，与<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%89%87%E5%BD%A2%E5%BC%8F%E7%9A%84%E5%8D%9A%E5%BC%88">正则形式</a>相应，<strong>扩展形式</strong>（英语：Extensive-form game）通过<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%A0%91_(%E5%9B%BE%E8%AE%BA)">树</a>来描述博弈。每个<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E9%A1%B6%E7%82%B9_(%E5%9B%BE%E8%AE%BA)">节点</a>（称作<strong>决策节点</strong>）表示博弈进行中的每一个可能的状态。博弈从唯一的<strong>初始节点</strong>开始，通过由参与者决定的路径到达<strong>终端节点</strong>，此时<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/w/index.php?title=%E7%BB%93%E6%9E%9C_(%E5%8D%9A%E5%BC%88%E8%AE%BA)&action=edit&redlink=1">博弈结束</a>，参与者得到相应的收益。每个非终端节点只属于一个参与者；参与者在该节点选择其可能的行动，每个可能的行动通过<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%BE%B9_(%E5%9B%BE%E8%AE%BA)">边</a>从该节点到达另一个节点。<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%89%A9%E5%B1%95%E5%BD%A2%E5%BC%8F%E7%9A%84%E5%8D%9A%E5%BC%88">wiki</a></p>
<p>Normal-form Game</p>
<p>在<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%8D%9A%E5%BC%88%E8%AE%BA">博弈论</a>中，<strong>正则形式</strong>（Normal-form game）是描述博弈的一种方式。与<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%89%A9%E5%B1%95%E5%BD%A2%E5%BC%8F%E7%9A%84%E5%8D%9A%E5%BC%88">延展形式</a>不同，正则形式不用图形来描述博弈，而是用<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%98%B5">矩阵</a>来陈述博弈。与延展形式的表述方式相比，这种方式在识别出<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/w/index.php?title=%E4%B8%A5%E6%A0%BC%E4%BC%98%E5%8A%BF%E7%AD%96%E7%95%A5&action=edit&redlink=1">严格优势策略</a>和<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%BA%B3%E4%BB%80%E5%9D%87%E8%A1%A1">纳什均衡</a>上更有用，但会丢失某些信息。博弈的正则形式的表述方式包括如下部分：每个参与者所有显然的和可能的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/w/index.php?title=%E7%AD%96%E7%95%A5(%E5%8D%9A%E5%BC%88%E8%AE%BA)&action=edit&redlink=1">策略</a>，以及和与其相对应的收益。</p>
<p>FP (Fictitious play )</p>
<p><img src="/2019/10/15/Games/RL-NFSP/1571629669702.png" alt="1571629669702"></p>
<p>​    双玩家零和和潜在游戏</p>
<h2 id="RL-分类"><a href="#RL-分类" class="headerlink" title="RL-分类"></a>RL-分类</h2><p><strong>on-policy</strong>: An agent is learning on-policy if it gathers these transition tuples by following its own policy.（从自己的策略中学习）</p>
<p><strong>off-policy</strong>: In the off-policy setting an agent is learning from experience of another agent or another policy. (从其它agent的经验中学习)</p>
<p>​    eg: Q-learning</p>
<h3 id="3-Extensive-Form-Fictitious-Play"><a href="#3-Extensive-Form-Fictitious-Play" class="headerlink" title="3. Extensive-Form Fictitious Play"></a>3. Extensive-Form Fictitious Play</h3><p>In this section, we derive a process in behavioural strategies that is realization equivalent to normal-form fictitious play （我们推导出行为策略的一个过程，即实现等同于正常形式的虚拟游戏）</p>
<p>下面的引理(Theorem6)显示了如何通过一个加权组合的实现等价的行为策略来实现标准形式策略的混合。</p>
<h2 id="RL-算法pseudo-code"><a href="#RL-算法pseudo-code" class="headerlink" title="RL-算法pseudo-code"></a>RL-算法pseudo-code</h2><h3 id="XFP"><a href="#XFP" class="headerlink" title="XFP"></a>XFP</h3><p>( full-width extensive-form fictitious play) ： <strong>通过Theorem7更新Behavioural Strategy，可以收敛到纳什均衡。</strong></p>
<p>1、计算best Respose</p>
<p>2、更新策略，使用<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v37/heinrich15.pdf">theorem 7</a></p>
<p>repeat</p>
<p><img src="/2019/10/15/Games/RL-NFSP/1571128998813.png" alt="1571128998813"></p>
<h3 id="FSP-Fictitious-Self-Play"><a href="#FSP-Fictitious-Self-Play" class="headerlink" title="FSP(Fictitious Self-Play):"></a>FSP(Fictitious Self-Play):</h3><ul>
<li>使用强化学习计算BestResponse</li>
<li>使用监督学习更新策略</li>
</ul>
<p><img src="/2019/10/15/Games/RL-NFSP/1571129034224.png" alt="1571129034224"></p>
<h3 id="NFSP"><a href="#NFSP" class="headerlink" title="NFSP:"></a>NFSP:</h3><ul>
<li>引入Neural Network近似； 替代传统算法：强化学习和监督学习都使用神经网络拟合。</li>
<li>Policy Network Π</li>
<li>Action-Value Network Q</li>
</ul>
<p><img src="/2019/10/15/Games/RL-NFSP/1571126251481.png" alt="1571126251481"></p>
<h3 id="MC-NFSP"><a href="#MC-NFSP" class="headerlink" title="MC-NFSP:"></a>MC-NFSP:</h3><p>训练时，采用 $\eta$-greed 的概率进行policy的选择</p>
<ul>
<li>1-$\eta$  时，Policy Network $\Pi$ </li>
<li>$\eta$ 时，Policy-Value Network B 结合MCTS，选择最佳策略</li>
</ul>
<p><img src="/2019/10/15/Games/RL-NFSP/1571126325514.png" alt="1571126325514"></p>
<p><img src="/2019/10/15/Games/RL-NFSP/1571126340205.png" alt="1571126340205"></p>
<h2 id="引申："><a href="#引申：" class="headerlink" title="引申："></a>引申：</h2><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1150412">Neural Fictitious Self Play——从博弈论到深度强化学习 腾讯云</a></p>
<p>[从Fictitious Play 到 NFSP](<a target="_blank" rel="noopener" href="https://gyh75520.github.io/2017/07/27/%E4%BB%8EFictitious">https://gyh75520.github.io/2017/07/27/从Fictitious</a> Play 到 NFSP&#x2F;)</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://novav.github.io/2019/10/12/AI/ML/ML-loss-func/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Simon Shi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon Shi的小站">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/12/AI/ML/ML-loss-func/" class="post-title-link" itemprop="url">Summary of loss function in Machine Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-12 11:17:17" itemprop="dateCreated datePublished" datetime="2019-10-12T11:17:17+00:00">2019-10-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-08-06 08:16:37" itemprop="dateModified" datetime="2025-08-06T08:16:37+00:00">2025-08-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[toc]</p>
<h2 id="ML损失函数"><a href="#ML损失函数" class="headerlink" title="ML损失函数"></a>ML损失函数</h2><h4 id="0-1损失函数"><a href="#0-1损失函数" class="headerlink" title="0-1损失函数"></a>0-1损失函数</h4><p>$$<br>L(Y, f(X)) &#x3D; \begin{cases}<br>1, Y \neq f(X) \<br>0, Y &#x3D; f(X)<br>\end{cases}<br>$$</p>
<p>$$<br>L(Y, f(X)) &#x3D; \begin{cases}<br>1 , |Y - f(X)| \geq T \<br>0 , |Y &#x3D; f(X)| &lt; T<br>\end{cases}<br>$$</p>
<h4 id="绝对值损失函数"><a href="#绝对值损失函数" class="headerlink" title="绝对值损失函数"></a>绝对值损失函数</h4><p>$$<br>L(Y, f(X)) &#x3D; |Y - f(X)|<br>$$</p>
<h4 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h4><p>实际结果和观测结果之间差距的平方和，一般用在线性回归中，(与最小二乘法应用场景类似)<br>$$<br>L(Y, f(X)) &#x3D; \sum_{i&#x3D;1}^{N} (y_i-f(x_i))^2<br>$$</p>
<h4 id="对数损失函数"><a href="#对数损失函数" class="headerlink" title="对数损失函数"></a>对数损失函数</h4><pre><code>主要在逻辑回归中使用，样本预测值和实际值的误差符合高斯分布，使用极大似然估计的方法，取对数得到损失函数：
</code></pre>
<p>$$<br>L(Y, P(Y|X)) &#x3D; -logP(Y|X)<br>$$<br>对数损失函数包括entropy和softmax，一般在做分类问题的时候使用（而回归时多用绝对值损失（拉普拉斯分布时，μ值为中位数）和平方损失（高斯分布时，μ值为均值））</p>
<h4 id="指数损失函数"><a href="#指数损失函数" class="headerlink" title="指数损失函数"></a>指数损失函数</h4><p>$$<br>L(Y|f(X))  &#x3D; \exp(-yf(x))<br>$$</p>
<h4 id="铰链损失函数"><a href="#铰链损失函数" class="headerlink" title="铰链损失函数"></a>铰链损失函数</h4><pre><code>主要用在SVM中，Hinge Loss的标准形式为：
</code></pre>
<p>$$<br>L(y) &#x3D; max(0, 1-ty)<br>$$</p>
<h2 id="Keras-TensorFlow-中常用-Cost-Function-总结"><a href="#Keras-TensorFlow-中常用-Cost-Function-总结" class="headerlink" title="Keras &#x2F; TensorFlow 中常用 Cost Function 总结"></a>Keras &#x2F; TensorFlow 中常用 Cost Function 总结</h2><ul>
<li>mean_squared_error或mse</li>
<li>mean_absolute_error或mae</li>
<li>mean_absolute_percentage_error或mape</li>
<li>mean_squared_logarithmic_error或msle</li>
<li>squared_hinge</li>
<li>hinge</li>
<li>categorical_hinge</li>
<li>binary_crossentropy（亦称作对数损失，logloss）</li>
<li>sigmoid_cross_entropy</li>
<li>softmax_cross_entropy</li>
<li>sparse_softmax_cross_entropy</li>
<li>logcosh</li>
<li>categorical_crossentropy：亦称作多类的对数损失，注意使用该目标函数时，需要将标签转化为形如<code>(nb_samples, nb_classes)</code>的二值序列</li>
<li>sparse_categorical_crossentrop：如上，但接受稀疏标签。注意，使用该函数时仍然需要你的标签与输出值的维度相同，你可能需要在标签数据上增加一个维度：<code>np.expand_dims(y,-1)</code></li>
<li>kullback_leibler_divergence:从预测值概率分布Q到真值概率分布P的信息增益,用以度量两个分布的差异.</li>
<li>poisson：即<code>(predictions - targets * log(predictions))</code>的均值</li>
<li>cosine_proximity：即预测值与真实标签的余弦距离平均值的相反数</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/28/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><span class="page-number current">29</span><a class="page-number" href="/page/30/">30</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="extend next" rel="next" href="/page/30/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Simon Shi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">322</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">142</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">269</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Simon Shi</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
